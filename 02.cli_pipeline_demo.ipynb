{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IceNet: Pipeline usage\n",
    "\n",
    "## Context\n",
    "\n",
    "### Purpose\n",
    "The first notebook demonstrated the use of high level command-line interfaces (CLI) of the IceNet library to download, process, train and predict from end to end.\n",
    "\n",
    "Now that you have gone through the basic steps of running the IceNet model via the CLI, you may wish to establish a framework to run the model automatically for end-to-end runs. This is often called a Pipeline. A Pipeline can schedule ongoing model runs or run multiple model variations simultaneously.\n",
    "\n",
    "This notebook illustrates the use of helper scripts from the IceNet pipeline repository for testing and producing operational forecasts.\n",
    "\n",
    "Please do go through the first notebook before proceeding with this, as the data download exists outside of the pipeline, and this is covered in detail in the first notebook. However, even so, this notebook has been designed to be run independent of other notebooks in this repository.\n",
    "\n",
    "This demonstrator notebook has been run on the British Antarctic Survey in-house HPC, however, the pipeline is by no means limited to running solely on HPCs.\n",
    "\n",
    "### Highlights\n",
    "The key features of an end to end run are:\n",
    "\n",
    " * [1. Introduction](#0-Introduction)\n",
    " * [2. Setup](#1-Setup)\n",
    " * [3. Process](#2-Process)\n",
    " * [4. Train](#3-Train)\n",
    " * [5. Predict](#4-Predict)\n",
    " * [6. Visualisation](#5-Visualisation)\n",
    "\n",
    "**Note:** Steps 3, 4 and 5 are within the IceNet pipeline.\n",
    "\n",
    "### Contributions\n",
    "#### Notebook\n",
    "\n",
    "James Byrne (author)\n",
    "\n",
    "Matthew Gascoyne\n",
    "\n",
    "Bryn Noel Ubald\n",
    "\n",
    "__Please raise issues [in this repository](https://github.com/icenet-ai/icenet-notebooks/issues) to suggest updates to this notebook!__ \n",
    "\n",
    "Contact me at _jambyr \\<at\\> bas.ac.uk_ for anything else...\n",
    "\n",
    "#### Modelling codebase\n",
    "James Byrne (code author), Tom Andersson (science author)\n",
    "\n",
    "#### Modelling publications\n",
    "Andersson, T.R., Hosking, J.S., Pérez-Ortiz, M. et al. Seasonal Arctic sea ice forecasting with probabilistic deep learning. Nat Commun 12, 5124 (2021). https://doi.org/10.1038/s41467-021-25257-4\n",
    "\n",
    "#### Involved organisations\n",
    "The Alan Turing Institute and British Antarctic Survey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# 1. Introduction\n",
    "\n",
    "## CLI vs Library vs Pipeline usage\n",
    "\n",
    "The IceNet package is designed to support automated runs from end to end by exposing the CLI operations demonstrated in the first notebook. These are simple wrappers around the library itself, and __any__ step of this can be undertaken manually or programmatically by inspecting the relevant endpoints. \n",
    "\n",
    "IceNet can be run in a number of ways: from the command line, the python interface, or as a pipeline.\n",
    "\n",
    "The rule of thumb to follow: \n",
    "\n",
    "* Use the [pipeline repository](https://github.com/icenet-ai/icenet-pipeline) if you want to run the end to end IceNet processing out of the box.\n",
    "* Adapt or customise this process using `icenet_*` commands described in this notebook and in the scripts contained in [the pipeline repo](https://github.com/icenet-ai/icenet-pipeline).\n",
    "* For ultimate customisation, you can interact with the IceNet repository programmatically (which is how the CLI commands operate.) For more information look at the [IceNet CLI implementations](https://github.com/JimCircadian/icenet2/blob/main/setup.py#L32) and the [library notebook](03.library_usage.ipynb), along with the [library documentation](#TODO). \n",
    "\n",
    "## Using the Pipeline\n",
    "\n",
    "Now that you have gone through the basic steps of running the IceNet model via the high-level CLI commands, you may wish to establish a framework to run the model automatically for end-to-end runs. This is often called a Pipeline. A Pipeline can schedule ongoing model runs or run multiple model variations simultaneously. The pipeline is driven by a series of bash scripts, and an environmental `ENVS` configuration file.\n",
    "\n",
    "![Diagram of Icenet and it's pipeline](./pipeline_diagram3.png \"Icenet pipeline diagram displaying process blocks and data being processed from input on the left to output on the right, through the pipeline\")\n",
    "\n",
    "To automatically produce daily IceNet forecasts we train multiple variations of the model, each with different starting conditions. We call this ensemble training. Then we run predictions for each model variation, producing a mean and error across the whole model ensemble. This captures some of the model uncertainty.\n",
    "\n",
    "### Data\n",
    "\n",
    "This assumes that you have a data store in a `data/` folder (This can be the same as the `data/` directory generated when running through the first notebook). Since the data is common across pipelines, you do not need to redownload data that you have previously downloaded. It is recommended to symbolically link to a data store such that data is only downloaded when has not been downloaded previously.\n",
    "\n",
    "### Ensemble Running\n",
    "\n",
    "To do this, an [icenet-pipeline](https://www.github.com/icenet-ai/icenet-pipeline) repository is available. The icenet-pipeline offers the `run_train_ensemble.sh` and `run_predict_ensemble.sh` script which operates similarly to the `icenet_train` and `icenet_predict` CLI commands demonstrated in the first notebook from the IceNet library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# 2. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the IceNet Pipeline\n",
    "\n",
    "Before progressing you will need to clone the icenet-pipeline repository. Assuming you have followed the directory structure from the first notebook:\n",
    "\n",
    "\n",
    "```bash\n",
    "git clone https://www.github.com/icenet-ai/icenet-pipeline.git green\n",
    "ln -s green notebook-pipeline\n",
    "cd icenet-notebooks\n",
    "```\n",
    "\n",
    "We clone a 'fresh' pipeline repository into a directory called 'green' (as an arbitrary way of identifying the fresh pipeline) and then symbolically link to it. This allows us to symbolically swap to another pipeline later if we want to.\n",
    "\n",
    "```bash\n",
    "my-icenet-project/       <--- we're in here!\n",
    "├── data/\n",
    "├── icenet-notebooks/\n",
    "├── green/               <--- Clone of icenet-pipeline\n",
    "└── notebook-pipeline@   <--- Symlink to the green/ `icenet-pipeline` repo we've just cloned into\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Viewing symbolically linked files.\n",
    "!find .. -maxdepth 1 -type l -ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move into the `notebook-pipeline` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../notebook-pipeline\")\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline is driven by environmental variables that are defined within an `ENVS` file.\n",
    "\n",
    "There is an example ENVS file (`ENVS.example`) in the `../notebook-pipeline` directory which is what ENVS is symbolically linked to by default.\n",
    "\n",
    "You can copy the `ENVS.example` file and create many variations to cover your usage scenario. Then, update the `ENVS` file symbolic link to the run you would like to go through."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a demonstrator, we will change the existing `my-icenet-project/notebook-pipeline/ENVS` link that points to `my-icenet-project/notebook-pipeline/ENVS.example`.\n",
    "\n",
    "We will instead point it to the example in this notebook repository after copying it over `my-icenet-project/icenet-notebooks/ENVS.notebook_tutorial`.\n",
    "\n",
    "The ENVS files are typically collated within the `notebook-pipeline` repo, hence why we are copying the file across, this is not mandatory, but is being done for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unlink the existing symoblic link (under `my-icenet-project/notebook-pipeline/ENVS`)\n",
    "!unlink ENVS\n",
    "\n",
    "# Point to the ENVS file from the icenet-notebooks repository (where this notebook is)\n",
    "!ln -s ../icenet-notebooks/ENVS.notebook_tutorial ENVS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running through this notebook, please update the following variables in the ENVS file to point to your icenet conda environment (if different to the default):\n",
    "\n",
    "<pre>\n",
    "export ICENET_HOME=${ICENET_HOME:-${HOME}/icenet/${ICENET_ENVIRONMENT}}\n",
    "export ICENET_CONDA=${ICENET_CONDA:-${HOME}/conda-envs/icenet}\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the symlinked files in the `notebook-pipeline` directory\n",
    "!find . -maxdepth 1 -type l -ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data before initiating pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the pipeline image at the top, the source data download is external to the pipeline since it is common across pipelines.\n",
    "\n",
    "Hence, the same commands from the first notebook can be used to download the required data into a data store (if not previously downloaded) and symbolically linked into in the working directory before using the pipeline. Please check the first notebook for details regarding the usage of these commands.\n",
    "\n",
    "**Please note that you do not need to redownload data you have already downloaded previously** (i.e., for date ranges you have previously downloaded into your data store)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!icenet_data_masks south"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!icenet_data_era5 south --vars uas,vas,tas,zg --levels ',,,500|250' 2020-1-1 2020-4-30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** We also make sure to also download sea-ice concentration data for the time period we're predicting for (in addition to the training range).\n",
    "\n",
    "In this case, the ENVS file defines the latest train date as being `2020-3-31`, and the latest test date being `2020-4-2`. Since we would like to forecast for `7` days (Also defined within the `ENVS` file under `export FORECAST_DAYS=7`), we should download up to 7 days after the end dates of train/validation/test.\n",
    "\n",
    "This will also be of use when comparing the prediction data.\n",
    "\n",
    "These do not have to be downloaded in separate date ranges, you can cover the entire period in one go (`2019-12-29 2020-4-30`), or use (e.g. `2019-12-29,2020-4-3 2020-3-31,2020-4-23`) syntax. The download is split into multiple sections to also demonstrate that previously downloaded data will be skipped over. This is the same for the above ERA5 download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date range for training (Adding 7 days forecast period to end date)\n",
    "!icenet_data_sic south -d 2020-1-1 2020-4-7\n",
    "\n",
    "# Date range for validation (Adding 7 days forecast period to end date)\n",
    "!icenet_data_sic south -d 2020-4-3 2020-4-30\n",
    "\n",
    "# Date range for test  (Adding 7 days forecast period to end date)\n",
    "# Note: Above date range already covers this, so this data will not be re-downloaded.\n",
    "!icenet_data_sic south -d 2020-4-1 2020-4-9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# 3. Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following command processes the downloaded data for the dates defined in the ENVS file.\n",
    "\n",
    "This is equivalent to running `icenet_process_era5`, `icenet_process_ora5`, `icenet_process_sic`, `icenet_process_metadata` commands from the IceNet library (as demonstrated in the first notebook).\n",
    "\n",
    "The arguments passed to these commands are obtained from the `PROC_ARGS_*` variables in the ENVS file.\n",
    "\n",
    "And, the dates that are processed are defined by the following variables in the ENVS file:\n",
    "* `TRAIN_START_*`\n",
    "* `TRAIN_END_*`\n",
    "* `VAL_START_*`\n",
    "* `VAL_END_*`\n",
    "* `TEST_START_*`\n",
    "* `TEST_END_*`\n",
    "\n",
    "This only needs to be run once unless the above variables need to be changed. Hence, it can be run as a precursor to the pipeline if the processed data does not need to change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./run_data.sh south"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# 4. Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For producing forecasts in the described pipeline we actually run a set of models using the [model-ensembler](https://github.com/JimCircadian/model-ensembler) tool and as such there are convenience scripts for doing this as part of the end to end run.\n",
    "\n",
    "This requires the [model-ensembler](https://pypi.org/project/model-ensembler/) (`pip install model-ensembler`) module to be installed.\n",
    "\n",
    "Note that the model-ensembler will submit jobs and to configure the job scripts, you can access the templates that are used to generate them in the `.yaml` (in particular [`train.tmpl.yaml`](https://github.com/icenet-ai/icenet-pipeline/blob/main/ensemble/train.tmpl.yaml) for the training ensemble jobs) files in the `ensemble/` folder of the clone of the `icenet-pipeline` repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the arguments for the following command are equivalent to the `icenet_train` command. However, the `-n` filters factor is actually `-f` in this example and we have additional arguments `-n` for the node to run on, `-p` for the pre_run script to use and `-j` for the number of simultaneous runs to execute on the SLURM cluster we use at BAS. However, these arguments are not necessarily required for other clusters, nor is the model-ensembler limited to running on SLURM (it can, at present, also run locally.)\n",
    "\n",
    "The pipeline repository shell scripts that provide this functionality are easily adaptable, as well as the ensemble itself which is stored in the pipeline repository under `/ensemble/`.\n",
    "\n",
    "_Please review the `-h` help option for the script to gain further insight the options available._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./run_train_ensemble.sh --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The optional arguments (Some are not defined in this example):\n",
    "| argument | description                                                                                                  | value |\n",
    "|     ---: |:---                                                                                                          | :---  |\n",
    "|*-b*      | Batch size                                                                                                   | -     |\n",
    "|*-d*      | Run locally instead of submitting SLURM jobs                                                                 | -     |\n",
    "|*-e*      | Number of epochs to train for                                                                                | 10    |\n",
    "|*-f*      | Scale the neural network channel sizes by this factor (reduces network size, priority over ENVS definition)  | 0.6   |\n",
    "|*-m*      | Memory required                                                                                              | 64gb  |\n",
    "|*-n*      | Node to run on                                                                                               | -     |\n",
    "|*-p*      | pre_run script to use                                                                                        | -     |\n",
    "|*-q*      | Maximum queue size                                                                                           | 4     |\n",
    "|*-r*      | Seed values for ensemble members (determines no. of ensemble members, overrides values in ENVS if specified) | -     |\n",
    "|*-j*      | No. of simultaneous runs to execute on the SLURM cluster                                                     | 5     |\n",
    "\n",
    "\n",
    "The positional arguments:\n",
    "| argument | description                                   | value                   |\n",
    "|     ---: |:---                                           | :---                    |\n",
    "|*LOADER*  | Name of loader: loader.{LOADER}.json          | tutorial_pipeline_south |\n",
    "|*DATASET* | Name of dataset: dataset_config.{LOADER}.json | tutorial_pipeline_south |\n",
    "|*NAME*    | Neural network output name                    | tutorial_south_ensemble |\n",
    "\n",
    "The loader and dataset names are defined by the prefix in the `ENVS` file. The hemisphere is appended to the defined string, so the following in the `ENVS.notebook_tutorial` file becomes \"tutorial_pipeline_south\".\n",
    "\n",
    "```bash\n",
    "PREFIX=\"TUTORIAL_PIPELINE\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Arguments\n",
    "# argument 1: The loader json file:          loader.tutorial_pipeline_south.json\n",
    "# argument 2: The dataset json file:         dataset_config.tutorial_pipeline_south.json\n",
    "# argument 3: The trained network name:      tutorial_south_ensemble\n",
    "!./run_train_ensemble.sh -e 10 -f 0.6 -m 64gb -q 4 -j 5 tutorial_pipeline_south tutorial_pipeline_south tutorial_south_ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This trains based on the processed data, and creates a sub-directory under `ensemble/` with the network name that contains each of the ensemble runs. This includes log files for debugging in case of any errors/issues in the training process.\n",
    "\n",
    "```bash\n",
    "ensemble/\n",
    "└── tutorial_pipeline_south_ensemble/\n",
    "    ├── tutorial_pipeline_south_ensemble-0/\n",
    "    │   ├── *.err    <-- Error file\n",
    "    │   └── *.out    <-- Log file\n",
    "    └── tutorial_pipeline_south_ensemble-1/\n",
    "        └── ...\n",
    "```\n",
    "\n",
    "The output from the trained network can be found in `results/networks`. The specifics of what is contained in here is out of scope of this notebook (please see [03.data_and_forecasts.ipynb](03.data_and_forecasts.ipynb) after running through this notebook), but in general it stores the trained model, and a history of the losses and other metrics.\n",
    "\n",
    "```bash\n",
    "results/\n",
    "└── networks/\n",
    "    └── tutorial_pipeline_south_ensemble/\n",
    "        ├── *.h5\n",
    "        ├── *.json\n",
    "        └── ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# 5. Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a similar manner to the training script, the `run_predict_ensemble` script will submit jobs to the HPC. The template corresponding to the prediction run is [`predict.tmpl.yaml`](https://github.com/icenet-ai/icenet-pipeline/blob/main/ensemble/predict.tmpl.yaml) found in the `icenet-pipeline` repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the ensemble prediction, we define the dates we want to predict for in a csv file. This can be automatically generated from the dataset as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./loader_test_dates.sh tutorial_pipeline_south | tee testdates.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First look at the required input arguments for running the prediction ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./run_predict_ensemble.sh --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the command line arguments are the same as with `run_train_ensemble` listed above.\n",
    "\n",
    "So to to predict from an ensemble training run, we use:  \n",
    "\n",
    "| argument  | description                                          | value                            |\n",
    "|     ---:  |:---                                                  | :---                             |\n",
    "|*NETWORK*  | Name of trained neural network to use for prediction | tutorial_south_ensemble          |\n",
    "|*DATASET*  | Name of dataset: dataset_config.{LOADER}.json        | tutorial_pipeline_south          |\n",
    "|*NAME*     | Name of output prediction                            | tutorial_south_ensemble_forecast |\n",
    "|*DATEFILE* | Dates to predict for                                 | testdates.csv                    |\n",
    "|*LOADER*   | Name of loader: loader.{LOADER}.json (optional)      | -                                |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -f: n_filters_factor (matching the value used for training)\n",
    "# -p: prep bash script (A bash script to run before running the prediction)\n",
    "!./run_predict_ensemble.sh -f 0.6 -p bashpc.sh tutorial_south_ensemble tutorial_pipeline_south tutorial_south_ensemble_forecast testdates.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the previous example, the individual numpy outputs, samples and sample weights are deposited into `/results/predict` for each ensemble member. However, the ensemble also runs `icenet_output` to generate __a CF-compliant NetCDF containing the forecasts requested__ which can then be post-processed or [deposited to an external location](#Uploading-to-Azure) (which is the platform for the [wider IceNet forecasting infrastructure](https://github.com/alan-turing-institute/IceNet-Project)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy files location (under each ensemble directory listed in the output of this cell)\n",
    "!ls ./results/predict/tutorial_south_ensemble_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined netCDF file location\n",
    "!ls ./results/predict/tutorial_south_ensemble_forecast.nc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# 6. Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the forecast output from the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a prediction, we can visualise the binary sea ice concentration using some of the built-in tools in IceNet that utilise `cartopy` and `matplotlib`.\n",
    "\n",
    "(Note: There are also some scripts in the [icenet-pipeline](https://github.com/icenet-ai/icenet-pipeline) repository that enable plotting common results such as `produce_op_assets.sh`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are loading the prediction netCDF file we've just created in the previous step.\n",
    "\n",
    "We are also using the `Masks` class from IceNet to create a land mask region that will mask out the land regions in the forecast plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icenet.plotting.video import xarray_to_video as xvid\n",
    "from icenet.data.sic.mask import Masks\n",
    "from IPython.display import HTML\n",
    "import xarray as xr, pandas as pd, datetime as dt\n",
    "\n",
    "# Load our output prediction file\n",
    "ds = xr.open_dataset(\"results/predict/tutorial_south_ensemble_forecast.nc\")\n",
    "land_mask = Masks(south=True, north=False).get_land_mask()\n",
    "ds.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell obtains the start date of the forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the forecast start date\n",
    "forecast_date = ds.time.values[0]\n",
    "print(forecast_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, here, we plot the forecast across the range of days we've defined within the `ENVS` file (7 days in this case).\n",
    "\n",
    "Since this is a demonstrator notebook, we have not trained our network for a prolonged period of time or for a large date range, but the plot below shows indicative results of what the output would look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = ds.sic_mean.isel(time=0).drop_vars(\"time\").rename(dict(leadtime=\"time\"))\n",
    "fc['time'] = [pd.to_datetime(forecast_date) \\\n",
    "              + dt.timedelta(days=int(e)) for e in fc.time.values]\n",
    "\n",
    "anim = xvid(fc, 15, figsize=4, mask=land_mask)\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Pipeline Considerations\n",
    "\n",
    "### A bit more information on ensemble runs\n",
    "\n",
    "#### Cleaning up runs\n",
    "\n",
    "Ensemble runs take place under `/ensemble/` in the pipeline folder and ARE NOT deleted after they've happened, to allow for debugging. Commonly, the ensemble configurations will contain a delete task to remove the extraneous run folders. __In the meantime this should be done manually__ after running `run_train_ensemble` or `run_predict_ensemble`.\n",
    "\n",
    "The only exception to this is the use of `run_daily.sh` (see below) which does clean up prior to rerunning. \n",
    "\n",
    "### Daily execution\n",
    "\n",
    "Daily execution is facilitated in the pipeline by using [`run_daily.sh`](https://github.com/antarctica/IceNet-Pipeline/blob/main/run_daily.sh). This wraps all the necessary steps to perform the following sequence for producing forecasts from yesterday for the next 93 days, for both northern and southern hemispheres. \n",
    "\n",
    "* Removes any old ensemble runs\n",
    "* Downloads [HRES forecast data from the ECMWF MARS API](https://www.ecmwf.int/en/forecasts/datasets/catalogue-ecmwf-real-time-products)\n",
    "* Processes the HRES and necessary training metadata to produce a data loader\n",
    "* Creates a dataset configuration for it\n",
    "* Runs a [prediction ensemble](#4-Predict) to produce a NetCDF\n",
    "* Uploads to the necessary endpoint\n",
    "\n",
    "#### Automation\n",
    "\n",
    "With the above shell script it's trivial to automate using cron. Of course this is simply for demonstration, with more complex workflow managers offering far great flexibility especially when considering analysis of the produced forecasts.\n",
    "\n",
    "```bash\n",
    "# We assume your environment is configured appropriately to run conda from cron files, for example by adding...\n",
    "#\n",
    "# SHELL=/bin/bash\n",
    "# BASH_ENV=~/.bashrc_env\n",
    "#\n",
    "# With conda initialisation in bashrc_env at the top of your crontab\n",
    "25 9 * * * conda activate icenet; cd $HOME/hpc/icenet/pipeline && bash run_daily.sh >$HOME/daily.log 2>&1; conda deactivate\n",
    "```\n",
    "\n",
    "TODO: more information on the usage of this command.\n",
    "\n",
    "## Summary\n",
    "\n",
    "Within this notebook we've attempted to give a full crash course on the IceNet pipeline and how to utilise it for a generalised run using the __pipeline helper scripts__. This is the second of six (currently) notebooks contained within the pipeline repository, covering further information: \n",
    "\n",
    "* [Data structure and analysis](03.data_and_forecasts.ipynb): understand the structure of the data stores and products created by these workflows and what tools currently exist in IceNet to looks over them.\n",
    "* [Library usage](04.library_usage.ipynb): understand how to programmatically perform an end to end run.\n",
    "* [Library extension](05.library_extension.ipynb): understand why and how to extend the IceNet library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import icenet\n",
    "icenet.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version\n",
    "- IceNet Codebase: v0.2.7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icenet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
