{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in /data/hpcdata/users/rychan/notebooks/notebook-pipeline\n"
     ]
    }
   ],
   "source": [
    "# Quick hack to put us in the icenet-pipeline folder,\n",
    "# assuming it was created as per 01.cli_demonstration.ipynb\n",
    "import os\n",
    "if os.path.exists(\"pytorch_example.ipynb\"):\n",
    "    os.chdir(\"../../notebook-pipeline\")\n",
    "print(\"Running in {}\".format(os.getcwd()))\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-30 17:07:17.448588: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-08-30 17:07:17.511983: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import logging\n",
    "\n",
    "from icenet.data.loaders import IceNetDataLoaderFactory\n",
    "from icenet.data.dataset import IceNetDataSet\n",
    "from icenet_pytorch_dataset import IceNetDataSetPyTorch\n",
    "\n",
    "from train_icenet_unet import train_icenet_unet\n",
    "from test_icenet_unet import test_icenet_unet\n",
    "\n",
    "# We also set the logging level so that we get some feedback from the API\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A 2.0.1+cu117\n",
      "B True\n",
      "C True\n",
      "D _CudaDeviceProperties(name='NVIDIA A2', major=8, minor=6, total_memory=14938MB, multi_processor_count=10)\n"
     ]
    }
   ],
   "source": [
    "print('A', torch.__version__)\n",
    "print('B', torch.cuda.is_available())\n",
    "print('C', torch.backends.cudnn.enabled)\n",
    "device = torch.device('cuda')\n",
    "print('D', torch.cuda.get_device_properties(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Aug 30 17:08:13 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A2           On   | 00000000:98:00.0 Off |                    0 |\n",
      "|  0%   32C    P8     5W /  60W |      2MiB / 15356MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset creation\n",
    "\n",
    "Assuming we have ran [03.library_usage](03.library_usage.ipynb) `loader.notebook_api_data.json` file existing in the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading configuration loader.notebook_api_data.json\n"
     ]
    }
   ],
   "source": [
    "implementation = \"dask\"\n",
    "loader_config = \"loader.notebook_api_data.json\"\n",
    "dataset_name = \"pytorch_notebook\"\n",
    "lag = 1\n",
    "\n",
    "dl = IceNetDataLoaderFactory().create_data_loader(\n",
    "    implementation,\n",
    "    loader_config,\n",
    "    dataset_name,\n",
    "    lag,\n",
    "    n_forecast_days=7,\n",
    "    north=False,\n",
    "    south=True,\n",
    "    output_batch_size=4,\n",
    "    generate_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl._n_forecast_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sources': {'era5': {'name': 'notebook_api_data',\n",
       "   'implementation': 'IceNetERA5PreProcessor',\n",
       "   'anom': ['tas', 'zg500', 'zg250'],\n",
       "   'abs': ['uas', 'vas'],\n",
       "   'dates': {'train': ['2020_01_01',\n",
       "     '2020_01_02',\n",
       "     '2020_01_03',\n",
       "     '2020_01_04',\n",
       "     '2020_01_05',\n",
       "     '2020_01_06',\n",
       "     '2020_01_07',\n",
       "     '2020_01_08',\n",
       "     '2020_01_09',\n",
       "     '2020_01_10',\n",
       "     '2020_01_11',\n",
       "     '2020_01_12',\n",
       "     '2020_01_13',\n",
       "     '2020_01_14',\n",
       "     '2020_01_15',\n",
       "     '2020_01_16',\n",
       "     '2020_01_17',\n",
       "     '2020_01_18',\n",
       "     '2020_01_19',\n",
       "     '2020_01_20',\n",
       "     '2020_01_21',\n",
       "     '2020_01_22',\n",
       "     '2020_01_23',\n",
       "     '2020_01_24',\n",
       "     '2020_01_25',\n",
       "     '2020_01_26',\n",
       "     '2020_01_27',\n",
       "     '2020_01_28',\n",
       "     '2020_01_29',\n",
       "     '2020_01_30',\n",
       "     '2020_01_31',\n",
       "     '2020_02_01',\n",
       "     '2020_02_02',\n",
       "     '2020_02_03',\n",
       "     '2020_02_04',\n",
       "     '2020_02_05',\n",
       "     '2020_02_06',\n",
       "     '2020_02_07',\n",
       "     '2020_02_08',\n",
       "     '2020_02_09',\n",
       "     '2020_02_10',\n",
       "     '2020_02_11',\n",
       "     '2020_02_12',\n",
       "     '2020_02_13',\n",
       "     '2020_02_14',\n",
       "     '2020_02_15',\n",
       "     '2020_02_16',\n",
       "     '2020_02_17',\n",
       "     '2020_02_18',\n",
       "     '2020_02_19',\n",
       "     '2020_02_20',\n",
       "     '2020_02_21',\n",
       "     '2020_02_22',\n",
       "     '2020_02_23',\n",
       "     '2020_02_24',\n",
       "     '2020_02_25',\n",
       "     '2020_02_26',\n",
       "     '2020_02_27',\n",
       "     '2020_02_28',\n",
       "     '2020_02_29',\n",
       "     '2020_03_01',\n",
       "     '2020_03_02',\n",
       "     '2020_03_03',\n",
       "     '2020_03_04',\n",
       "     '2020_03_05',\n",
       "     '2020_03_06',\n",
       "     '2020_03_07',\n",
       "     '2020_03_08',\n",
       "     '2020_03_09',\n",
       "     '2020_03_10',\n",
       "     '2020_03_11',\n",
       "     '2020_03_12',\n",
       "     '2020_03_13',\n",
       "     '2020_03_14',\n",
       "     '2020_03_15',\n",
       "     '2020_03_16',\n",
       "     '2020_03_17',\n",
       "     '2020_03_18',\n",
       "     '2020_03_19',\n",
       "     '2020_03_20',\n",
       "     '2020_03_21',\n",
       "     '2020_03_22',\n",
       "     '2020_03_23',\n",
       "     '2020_03_24',\n",
       "     '2020_03_25',\n",
       "     '2020_03_26',\n",
       "     '2020_03_27',\n",
       "     '2020_03_28',\n",
       "     '2020_03_29',\n",
       "     '2020_03_30',\n",
       "     '2020_03_31'],\n",
       "    'val': ['2020_04_03',\n",
       "     '2020_04_04',\n",
       "     '2020_04_05',\n",
       "     '2020_04_06',\n",
       "     '2020_04_07',\n",
       "     '2020_04_08',\n",
       "     '2020_04_09',\n",
       "     '2020_04_10',\n",
       "     '2020_04_11',\n",
       "     '2020_04_12',\n",
       "     '2020_04_13',\n",
       "     '2020_04_14',\n",
       "     '2020_04_15',\n",
       "     '2020_04_16',\n",
       "     '2020_04_17',\n",
       "     '2020_04_18',\n",
       "     '2020_04_19',\n",
       "     '2020_04_20',\n",
       "     '2020_04_21',\n",
       "     '2020_04_22',\n",
       "     '2020_04_23'],\n",
       "    'test': ['2020_04_01', '2020_04_02']},\n",
       "   'linear_trends': [],\n",
       "   'linear_trend_steps': [1, 2, 3, 4, 5, 6, 7],\n",
       "   'meta': [],\n",
       "   'var_files': {'uas': ['./processed/notebook_api_data/era5/south/uas/uas_abs.nc'],\n",
       "    'vas': ['./processed/notebook_api_data/era5/south/vas/vas_abs.nc'],\n",
       "    'tas': ['./processed/notebook_api_data/era5/south/tas/tas_anom.nc'],\n",
       "    'zg500': ['./processed/notebook_api_data/era5/south/zg500/zg500_anom.nc'],\n",
       "    'zg250': ['./processed/notebook_api_data/era5/south/zg250/zg250_anom.nc']}},\n",
       "  'osisaf': {'name': 'notebook_api_data',\n",
       "   'implementation': 'IceNetOSIPreProcessor',\n",
       "   'anom': [],\n",
       "   'abs': ['siconca'],\n",
       "   'dates': {'train': ['2020_01_01',\n",
       "     '2020_01_02',\n",
       "     '2020_01_03',\n",
       "     '2020_01_04',\n",
       "     '2020_01_05',\n",
       "     '2020_01_06',\n",
       "     '2020_01_07',\n",
       "     '2020_01_08',\n",
       "     '2020_01_09',\n",
       "     '2020_01_10',\n",
       "     '2020_01_11',\n",
       "     '2020_01_12',\n",
       "     '2020_01_13',\n",
       "     '2020_01_14',\n",
       "     '2020_01_15',\n",
       "     '2020_01_16',\n",
       "     '2020_01_17',\n",
       "     '2020_01_18',\n",
       "     '2020_01_19',\n",
       "     '2020_01_20',\n",
       "     '2020_01_21',\n",
       "     '2020_01_22',\n",
       "     '2020_01_23',\n",
       "     '2020_01_24',\n",
       "     '2020_01_25',\n",
       "     '2020_01_26',\n",
       "     '2020_01_27',\n",
       "     '2020_01_28',\n",
       "     '2020_01_29',\n",
       "     '2020_01_30',\n",
       "     '2020_01_31',\n",
       "     '2020_02_01',\n",
       "     '2020_02_02',\n",
       "     '2020_02_03',\n",
       "     '2020_02_04',\n",
       "     '2020_02_05',\n",
       "     '2020_02_06',\n",
       "     '2020_02_07',\n",
       "     '2020_02_08',\n",
       "     '2020_02_09',\n",
       "     '2020_02_10',\n",
       "     '2020_02_11',\n",
       "     '2020_02_12',\n",
       "     '2020_02_13',\n",
       "     '2020_02_14',\n",
       "     '2020_02_15',\n",
       "     '2020_02_16',\n",
       "     '2020_02_17',\n",
       "     '2020_02_18',\n",
       "     '2020_02_19',\n",
       "     '2020_02_20',\n",
       "     '2020_02_21',\n",
       "     '2020_02_22',\n",
       "     '2020_02_23',\n",
       "     '2020_02_24',\n",
       "     '2020_02_25',\n",
       "     '2020_02_26',\n",
       "     '2020_02_27',\n",
       "     '2020_02_28',\n",
       "     '2020_02_29',\n",
       "     '2020_03_01',\n",
       "     '2020_03_02',\n",
       "     '2020_03_03',\n",
       "     '2020_03_04',\n",
       "     '2020_03_05',\n",
       "     '2020_03_06',\n",
       "     '2020_03_07',\n",
       "     '2020_03_08',\n",
       "     '2020_03_09',\n",
       "     '2020_03_10',\n",
       "     '2020_03_11',\n",
       "     '2020_03_12',\n",
       "     '2020_03_13',\n",
       "     '2020_03_14',\n",
       "     '2020_03_15',\n",
       "     '2020_03_16',\n",
       "     '2020_03_17',\n",
       "     '2020_03_18',\n",
       "     '2020_03_19',\n",
       "     '2020_03_20',\n",
       "     '2020_03_21',\n",
       "     '2020_03_22',\n",
       "     '2020_03_23',\n",
       "     '2020_03_24',\n",
       "     '2020_03_25',\n",
       "     '2020_03_26',\n",
       "     '2020_03_27',\n",
       "     '2020_03_28',\n",
       "     '2020_03_29',\n",
       "     '2020_03_30',\n",
       "     '2020_03_31'],\n",
       "    'val': ['2020_04_03',\n",
       "     '2020_04_04',\n",
       "     '2020_04_05',\n",
       "     '2020_04_06',\n",
       "     '2020_04_07',\n",
       "     '2020_04_08',\n",
       "     '2020_04_09',\n",
       "     '2020_04_10',\n",
       "     '2020_04_11',\n",
       "     '2020_04_12',\n",
       "     '2020_04_13',\n",
       "     '2020_04_14',\n",
       "     '2020_04_15',\n",
       "     '2020_04_16',\n",
       "     '2020_04_17',\n",
       "     '2020_04_18',\n",
       "     '2020_04_19',\n",
       "     '2020_04_20',\n",
       "     '2020_04_21',\n",
       "     '2020_04_22',\n",
       "     '2020_04_23'],\n",
       "    'test': ['2020_04_01', '2020_04_02']},\n",
       "   'linear_trends': [],\n",
       "   'linear_trend_steps': [1, 2, 3, 4, 5, 6, 7],\n",
       "   'meta': [],\n",
       "   'var_files': {'siconca': ['./processed/notebook_api_data/osisaf/south/siconca/siconca_abs.nc']}},\n",
       "  'meta': {'name': 'notebook_api_data',\n",
       "   'implementation': 'IceNetMetaPreProcessor',\n",
       "   'anom': [],\n",
       "   'abs': [],\n",
       "   'dates': {'train': [], 'val': [], 'test': []},\n",
       "   'linear_trends': [],\n",
       "   'linear_trend_steps': [1, 2, 3, 4, 5, 6, 7],\n",
       "   'meta': ['sin', 'cos', 'land'],\n",
       "   'var_files': {'sin': ['./processed/notebook_api_data/meta/south/sin/sin.nc'],\n",
       "    'cos': ['./processed/notebook_api_data/meta/south/cos/cos.nc'],\n",
       "    'land': ['./processed/notebook_api_data/meta/south/land/land.nc']}}},\n",
       " 'dtype': 'float32',\n",
       " 'shape': [432, 432],\n",
       " 'missing_dates': []}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl._config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate a config only dataset, which will get saved in `dataset_config.pytorch_notebook.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Writing dataset configuration without data generation\n",
      "INFO:root:91 train dates in total, NOT generating cache data.\n",
      "INFO:root:21 val dates in total, NOT generating cache data.\n",
      "INFO:root:2 test dates in total, NOT generating cache data.\n",
      "INFO:root:Writing configuration to ./dataset_config.pytorch_notebook.json\n"
     ]
    }
   ],
   "source": [
    "dl.write_dataset_config_only()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create the IceNetDataSet object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_config = \"dataset_config.pytorch_notebook.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading configuration dataset_config.pytorch_notebook.json\n",
      "WARNING:root:Running in configuration only mode, tfrecords were not generated for this dataset\n"
     ]
    }
   ],
   "source": [
    "dataset = IceNetDataSet(dataset_config, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'identifier': 'pytorch_notebook',\n",
       " 'implementation': 'DaskMultiWorkerLoader',\n",
       " 'channels': ['uas_abs_1',\n",
       "  'vas_abs_1',\n",
       "  'siconca_abs_1',\n",
       "  'tas_anom_1',\n",
       "  'zg250_anom_1',\n",
       "  'zg500_anom_1',\n",
       "  'cos_1',\n",
       "  'land_1',\n",
       "  'sin_1'],\n",
       " 'counts': {'train': 91, 'val': 21, 'test': 2},\n",
       " 'dtype': 'float32',\n",
       " 'loader_config': '/data/hpcdata/users/rychan/notebooks/notebook-pipeline/loader.notebook_api_data.json',\n",
       " 'missing_dates': [],\n",
       " 'n_forecast_days': 7,\n",
       " 'north': False,\n",
       " 'num_channels': 9,\n",
       " 'shape': [432, 432],\n",
       " 'south': True,\n",
       " 'dataset_path': False,\n",
       " 'loss_weight_days': True,\n",
       " 'output_batch_size': 4,\n",
       " 'var_lag': 1,\n",
       " 'var_lag_override': {}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset._config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset._config[\"n_forecast_days\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/hpcdata/users/rychan/notebooks/notebook-pipeline/loader.notebook_api_data.json'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.loader_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading configuration /data/hpcdata/users/rychan/notebooks/notebook-pipeline/loader.notebook_api_data.json\n"
     ]
    }
   ],
   "source": [
    "dataloader_from_dataset = dataset.get_data_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['sources', 'dtype', 'shape', 'missing_dates'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader_from_dataset._config.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader_from_dataset._n_forecast_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<icenet.data.loaders.dask.DaskMultiWorkerLoader at 0x7f4c65ee6f70>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader_from_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading configuration dataset_config.pytorch_notebook.json\n",
      "WARNING:root:Running in configuration only mode, tfrecords were not generated for this dataset\n",
      "INFO:root:Loading configuration /data/hpcdata/users/rychan/notebooks/notebook-pipeline/loader.notebook_api_data.json\n"
     ]
    }
   ],
   "source": [
    "ds_torch = IceNetDataSetPyTorch(configuration_path=dataset_config, mode=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_torch.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2020-01-01'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_torch._dates[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_forecast_days: 7\n"
     ]
    }
   ],
   "source": [
    "first_item = ds_torch.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first_item is a <class 'tuple'> of length 3\n"
     ]
    }
   ],
   "source": [
    "print(f\"first_item is a {type(first_item)} of length {len(first_item)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first_item[i] is a <class 'numpy.ndarray'> of shape (432, 432, 9)\n",
      "first_item[i] is a <class 'numpy.ndarray'> of shape (432, 432, 7, 1)\n",
      "first_item[i] is a <class 'numpy.ndarray'> of shape (432, 432, 7, 1)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(first_item)):\n",
    "    print(f\"first_item[i] is a {type(first_item[i])} of shape {first_item[i].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'identifier': 'pytorch_notebook',\n",
       " 'implementation': 'DaskMultiWorkerLoader',\n",
       " 'channels': ['uas_abs_1',\n",
       "  'vas_abs_1',\n",
       "  'siconca_abs_1',\n",
       "  'tas_anom_1',\n",
       "  'zg250_anom_1',\n",
       "  'zg500_anom_1',\n",
       "  'cos_1',\n",
       "  'land_1',\n",
       "  'sin_1'],\n",
       " 'counts': {'train': 91, 'val': 21, 'test': 2},\n",
       " 'dtype': 'float32',\n",
       " 'loader_config': '/data/hpcdata/users/rychan/notebooks/notebook-pipeline/loader.notebook_api_data.json',\n",
       " 'missing_dates': [],\n",
       " 'n_forecast_days': 7,\n",
       " 'north': False,\n",
       " 'num_channels': 9,\n",
       " 'shape': [432, 432],\n",
       " 'south': True,\n",
       " 'dataset_path': False,\n",
       " 'loss_weight_days': True,\n",
       " 'output_batch_size': 4,\n",
       " 'var_lag': 1,\n",
       " 'var_lag_override': {}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_torch._ds._config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_torch._ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_forecast_days: 7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[[ 0.5070619 ,  0.507128  ,  0.        , ..., -0.18561055,\n",
       "           1.        , -0.9826234 ],\n",
       "         [ 0.5067984 ,  0.5119969 ,  0.        , ..., -0.18561055,\n",
       "           1.        , -0.9826234 ],\n",
       "         [ 0.50709283,  0.5130819 ,  0.        , ..., -0.18561055,\n",
       "           1.        , -0.9826234 ],\n",
       "         ...,\n",
       "         [ 0.49436113,  0.51067567,  0.        , ..., -0.18561055,\n",
       "           1.        , -0.9826234 ],\n",
       "         [ 0.49760532,  0.5123943 ,  0.        , ..., -0.18561055,\n",
       "           1.        , -0.9826234 ],\n",
       "         [ 0.5012724 ,  0.5121377 ,  0.        , ..., -0.18561055,\n",
       "           1.        , -0.9826234 ]],\n",
       " \n",
       "        [[ 0.5080182 ,  0.5084777 ,  0.        , ..., -0.18561055,\n",
       "           1.        , -0.9826234 ],\n",
       "         [ 0.50926554,  0.51501405,  0.        , ..., -0.18561055,\n",
       "           1.        , -0.9826234 ],\n",
       "         [ 0.50827605,  0.5093369 ,  0.        , ..., -0.18561055,\n",
       "           1.        , -0.9826234 ],\n",
       "         ...,\n",
       "         [ 0.4894217 ,  0.5110176 ,  0.        , ..., -0.18561055,\n",
       "           1.        , -0.9826234 ],\n",
       "         [ 0.49363175,  0.51387256,  0.        , ..., -0.18561055,\n",
       "           1.        , -0.9826234 ],\n",
       "         [ 0.49781784,  0.5137171 ,  0.        , ..., -0.18561055,\n",
       "           1.        , -0.9826234 ]],\n",
       " \n",
       "        [[ 0.5070832 ,  0.5101056 ,  0.        , ..., -0.18561055,\n",
       "           1.        , -0.9826234 ],\n",
       "         [ 0.5071541 ,  0.5147865 ,  0.        , ..., -0.18561055,\n",
       "           1.        , -0.9826234 ],\n",
       "         [ 0.51002264,  0.5079214 ,  0.        , ..., -0.18561055,\n",
       "           1.        , -0.9826234 ],\n",
       "         ...,\n",
       "         [ 0.48842257,  0.5124112 ,  0.        , ..., -0.18561055,\n",
       "           1.        , -0.9826234 ],\n",
       "         [ 0.49018812,  0.5134328 ,  0.        , ..., -0.18561055,\n",
       "           1.        , -0.9826234 ],\n",
       "         [ 0.4937481 ,  0.5152318 ,  0.        , ..., -0.18561055,\n",
       "           1.        , -0.9826234 ]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 0.5201201 ,  0.5939807 ,  0.        , ..., -0.18561055,\n",
       "          -1.        , -0.9826234 ],\n",
       "         [ 0.5196981 ,  0.5929644 ,  0.        , ..., -0.18561055,\n",
       "          -1.        , -0.9826234 ],\n",
       "         [ 0.5230958 ,  0.5898119 ,  0.        , ..., -0.18561055,\n",
       "          -1.        , -0.9826234 ],\n",
       "         ...,\n",
       "         [ 0.53455853,  0.4542398 ,  0.        , ..., -0.18561055,\n",
       "           1.        , -0.9826234 ],\n",
       "         [ 0.5359504 ,  0.45726374,  0.        , ..., -0.18561055,\n",
       "           1.        , -0.9826234 ],\n",
       "         [ 0.53147364,  0.462161  ,  0.        , ..., -0.18561055,\n",
       "           1.        , -0.9826234 ]],\n",
       " \n",
       "        [[ 0.52010256,  0.5904959 ,  0.        , ..., -0.18561055,\n",
       "          -1.        , -0.9826234 ],\n",
       "         [ 0.5158349 ,  0.59004027,  0.        , ..., -0.18561055,\n",
       "          -1.        , -0.9826234 ],\n",
       "         [ 0.5168654 ,  0.58813155,  0.        , ..., -0.18561055,\n",
       "          -1.        , -0.9826234 ],\n",
       "         ...,\n",
       "         [ 0.5326147 ,  0.46137762,  0.        , ..., -0.18561055,\n",
       "           1.        , -0.9826234 ],\n",
       "         [ 0.53047955,  0.46351588,  0.        , ..., -0.18561055,\n",
       "           1.        , -0.9826234 ],\n",
       "         [ 0.5268573 ,  0.46342507,  0.        , ..., -0.18561055,\n",
       "           1.        , -0.9826234 ]],\n",
       " \n",
       "        [[ 0.5224085 ,  0.5875143 ,  0.        , ..., -0.18561055,\n",
       "          -1.        , -0.9826234 ],\n",
       "         [ 0.51823205,  0.58804107,  0.        , ..., -0.18561055,\n",
       "          -1.        , -0.9826234 ],\n",
       "         [ 0.5156312 ,  0.5871129 ,  0.        , ..., -0.18561055,\n",
       "          -1.        , -0.9826234 ],\n",
       "         ...,\n",
       "         [ 0.52451056,  0.4664941 ,  0.        , ..., -0.18561055,\n",
       "           1.        , -0.9826234 ],\n",
       "         [ 0.5236765 ,  0.4643196 ,  0.        , ..., -0.18561055,\n",
       "           1.        , -0.9826234 ],\n",
       "         [ 0.5230731 ,  0.46157268,  0.        , ..., -0.18561055,\n",
       "           1.        , -0.9826234 ]]], dtype=float32),\n",
       " array([[[[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]]],\n",
       " \n",
       " \n",
       "        [[[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]]],\n",
       " \n",
       " \n",
       "        [[[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]]],\n",
       " \n",
       " \n",
       "        ...,\n",
       " \n",
       " \n",
       "        [[[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]]],\n",
       " \n",
       " \n",
       "        [[[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]]],\n",
       " \n",
       " \n",
       "        [[[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]]]], dtype=float32),\n",
       " array([[[[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]]],\n",
       " \n",
       " \n",
       "        [[[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]]],\n",
       " \n",
       " \n",
       "        [[[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]]],\n",
       " \n",
       " \n",
       "        ...,\n",
       " \n",
       " \n",
       "        [[[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]]],\n",
       " \n",
       " \n",
       "        [[[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]]],\n",
       " \n",
       " \n",
       "        [[[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]]]], dtype=float32))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_torch.__getitem__(80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating PyTorch DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading configuration dataset_config.pytorch_notebook.json\n",
      "WARNING:root:Running in configuration only mode, tfrecords were not generated for this dataset\n",
      "INFO:root:Loading configuration /data/hpcdata/users/rychan/notebooks/notebook-pipeline/loader.notebook_api_data.json\n",
      "INFO:root:Loading configuration dataset_config.pytorch_notebook.json\n",
      "WARNING:root:Running in configuration only mode, tfrecords were not generated for this dataset\n",
      "INFO:root:Loading configuration /data/hpcdata/users/rychan/notebooks/notebook-pipeline/loader.notebook_api_data.json\n",
      "INFO:root:Loading configuration dataset_config.pytorch_notebook.json\n",
      "WARNING:root:Running in configuration only mode, tfrecords were not generated for this dataset\n",
      "INFO:root:Loading configuration /data/hpcdata/users/rychan/notebooks/notebook-pipeline/loader.notebook_api_data.json\n"
     ]
    }
   ],
   "source": [
    "train_dataset = IceNetDataSetPyTorch(configuration_path=dataset_config, mode=\"train\")\n",
    "val_dataset = IceNetDataSetPyTorch(configuration_path=dataset_config, mode=\"val\")\n",
    "test_dataset = IceNetDataSetPyTorch(configuration_path=dataset_config, mode=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 4\n",
    "shuffle = False # set to False for now\n",
    "persistent_workers = False\n",
    "num_workers = 0\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=shuffle,\n",
    "                              persistent_workers=persistent_workers,\n",
    "                              num_workers=num_workers)\n",
    "val_dataloader = DataLoader(val_dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=shuffle,\n",
    "                            persistent_workers=persistent_workers,\n",
    "                            num_workers=num_workers)\n",
    "test_dataloader = DataLoader(test_dataset,\n",
    "                             batch_size=batch_size,\n",
    "                             shuffle=shuffle,\n",
    "                             persistent_workers=persistent_workers,\n",
    "                             num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterating through DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_forecast_days: 7\n",
      "n_forecast_days: 7\n",
      "n_forecast_days: 7\n",
      "n_forecast_days: 7\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels, sample_weights = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 432, 432, 9])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 432, 432, 7, 1])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 432, 432, 7, 1])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icenet_unet_small import UNet\n",
    "\n",
    "unet = UNet(input_channels=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/hpcdata/users/rychan/miniconda3/envs/icenet_pytorch/lib/python3.8/site-packages/torch/nn/functional.py:3737: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
     ]
    }
   ],
   "source": [
    "y_hat = unet(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 432, 432, 3, 6])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IceNet UNet model\n",
    "\n",
    "As a first attempt to implement a PyTorch example, we adapt code from https://github.com/ampersandmcd/icenet-gan/.\n",
    "\n",
    "Below is a PyTorch implementation of the UNet architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "    An implementation of a UNet for pixelwise classification.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 input_channels, \n",
    "                 filter_size=3, \n",
    "                 n_filters_factor=1, \n",
    "                 n_forecast_days=6, \n",
    "                 n_output_classes=3,\n",
    "                **kwargs):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        self.input_channels = input_channels\n",
    "        self.filter_size = filter_size\n",
    "        self.n_filters_factor = n_filters_factor\n",
    "        self.n_forecast_days = n_forecast_days\n",
    "        self.n_output_classes = n_output_classes\n",
    "\n",
    "        self.conv1a = nn.Conv2d(in_channels=input_channels, \n",
    "                                out_channels=int(128*n_filters_factor),\n",
    "                                kernel_size=filter_size,\n",
    "                                padding=\"same\")\n",
    "        self.conv1b = nn.Conv2d(in_channels=int(128*n_filters_factor),\n",
    "                                out_channels=int(128*n_filters_factor),\n",
    "                                kernel_size=filter_size,\n",
    "                                padding=\"same\")\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=int(128*n_filters_factor))\n",
    "\n",
    "        self.conv2a = nn.Conv2d(in_channels=int(128*n_filters_factor),\n",
    "                                out_channels=int(256*n_filters_factor),\n",
    "                                kernel_size=filter_size,\n",
    "                                padding=\"same\")\n",
    "        self.conv2b = nn.Conv2d(in_channels=int(256*n_filters_factor),\n",
    "                                out_channels=int(256*n_filters_factor),\n",
    "                                kernel_size=filter_size,\n",
    "                                padding=\"same\")\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=int(256*n_filters_factor))\n",
    "\n",
    "        # self.conv3a = nn.Conv2d(in_channels=int(256*n_filters_factor),\n",
    "        #                         out_channels=int(512*n_filters_factor),\n",
    "        #                         kernel_size=filter_size,\n",
    "        #                         padding=\"same\")\n",
    "        # self.conv3b = nn.Conv2d(in_channels=int(512*n_filters_factor),\n",
    "        #                         out_channels=int(512*n_filters_factor),\n",
    "        #                         kernel_size=filter_size,\n",
    "        #                         padding=\"same\")\n",
    "        # self.bn3 = nn.BatchNorm2d(num_features=int(512*n_filters_factor))\n",
    "\n",
    "        # self.conv4a = nn.Conv2d(in_channels=int(512*n_filters_factor),\n",
    "        #                         out_channels=int(512*n_filters_factor),\n",
    "        #                         kernel_size=filter_size,\n",
    "        #                         padding=\"same\")\n",
    "        # self.conv4b = nn.Conv2d(in_channels=int(512*n_filters_factor),\n",
    "        #                         out_channels=int(512*n_filters_factor),\n",
    "        #                         kernel_size=filter_size,\n",
    "        #                         padding=\"same\")\n",
    "        # self.bn4 = nn.BatchNorm2d(num_features=int(512*n_filters_factor))\n",
    "\n",
    "        # self.conv5a = nn.Conv2d(in_channels=int(512*n_filters_factor),\n",
    "        #                         out_channels=int(1024*n_filters_factor),\n",
    "        #                         kernel_size=filter_size,\n",
    "        #                         padding=\"same\")\n",
    "        # self.conv5b = nn.Conv2d(in_channels=int(1024*n_filters_factor),\n",
    "        #                         out_channels=int(1024*n_filters_factor),\n",
    "        #                         kernel_size=filter_size,\n",
    "        #                         padding=\"same\")\n",
    "        # self.bn5 = nn.BatchNorm2d(num_features=int(1024*n_filters_factor))\n",
    "\n",
    "        # self.conv6a = nn.Conv2d(in_channels=int(1024*n_filters_factor),\n",
    "        #                         out_channels=int(512*n_filters_factor),\n",
    "        #                         kernel_size=filter_size,\n",
    "        #                         padding=\"same\")\n",
    "        # self.conv6b = nn.Conv2d(in_channels=int(1024*n_filters_factor),\n",
    "        #                         out_channels=int(512*n_filters_factor),\n",
    "        #                         kernel_size=filter_size,\n",
    "        #                         padding=\"same\")\n",
    "        # self.conv6c = nn.Conv2d(in_channels=int(512*n_filters_factor),\n",
    "        #                         out_channels=int(512*n_filters_factor),\n",
    "        #                         kernel_size=filter_size,\n",
    "        #                         padding=\"same\")\n",
    "        # self.bn6 = nn.BatchNorm2d(num_features=int(512*n_filters_factor))\n",
    "\n",
    "        # self.conv7a = nn.Conv2d(in_channels=int(512*n_filters_factor),\n",
    "        #                         out_channels=int(512*n_filters_factor),\n",
    "        #                         kernel_size=filter_size,\n",
    "        #                         padding=\"same\")\n",
    "        # self.conv7b = nn.Conv2d(in_channels=int(1024*n_filters_factor),\n",
    "        #                         out_channels=int(512*n_filters_factor),\n",
    "        #                         kernel_size=filter_size,\n",
    "        #                         padding=\"same\")\n",
    "        # self.conv7c = nn.Conv2d(in_channels=int(512*n_filters_factor),\n",
    "        #                         out_channels=int(512*n_filters_factor),\n",
    "        #                         kernel_size=filter_size,\n",
    "        #                         padding=\"same\")\n",
    "        # self.bn7 = nn.BatchNorm2d(num_features=int(512*n_filters_factor))\n",
    "\n",
    "        # self.conv8a = nn.Conv2d(in_channels=int(512*n_filters_factor),\n",
    "        #                         out_channels=int(256*n_filters_factor),\n",
    "        #                         kernel_size=filter_size,\n",
    "        #                         padding=\"same\")\n",
    "        # self.conv8b = nn.Conv2d(in_channels=int(512*n_filters_factor),\n",
    "        #                         out_channels=int(256*n_filters_factor),\n",
    "        #                         kernel_size=filter_size,\n",
    "        #                         padding=\"same\")\n",
    "        # self.conv8c = nn.Conv2d(in_channels=int(256*n_filters_factor),\n",
    "        #                         out_channels=int(256*n_filters_factor),\n",
    "        #                         kernel_size=filter_size,\n",
    "        #                         padding=\"same\")\n",
    "        # self.bn8 = nn.BatchNorm2d(num_features=int(256*n_filters_factor))\n",
    "\n",
    "        self.conv9a = nn.Conv2d(in_channels=int(256*n_filters_factor),\n",
    "                                out_channels=int(128*n_filters_factor),\n",
    "                                kernel_size=filter_size,\n",
    "                                padding=\"same\")\n",
    "        self.conv9b = nn.Conv2d(in_channels=int(256*n_filters_factor),\n",
    "                                out_channels=int(128*n_filters_factor),\n",
    "                                kernel_size=filter_size,\n",
    "                                padding=\"same\")\n",
    "        self.conv9c = nn.Conv2d(in_channels=int(128*n_filters_factor),\n",
    "                                out_channels=int(128*n_filters_factor),\n",
    "                                kernel_size=filter_size,\n",
    "                                padding=\"same\")  # no batch norm on last layer\n",
    "\n",
    "        self.final_conv = nn.Conv2d(in_channels=int(128*n_filters_factor),\n",
    "                                    out_channels=n_output_classes*n_forecast_days,\n",
    "                                    kernel_size=filter_size,\n",
    "                                    padding=\"same\")\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # transpose from shape (b, h, w, c) to (b, c, h, w) for pytorch conv2d layers\n",
    "        x = torch.movedim(x, -1, 1)  # move c from last to second dim\n",
    "\n",
    "        # run through network\n",
    "        conv1 = self.conv1a(x)  # input to 128\n",
    "        conv1 = F.relu(conv1)\n",
    "        conv1 = self.conv1b(conv1)  # 128 to 128\n",
    "        conv1 = F.relu(conv1)\n",
    "        bn1 = self.bn1(conv1)\n",
    "        pool1 = F.max_pool2d(bn1, kernel_size=(2, 2))\n",
    "\n",
    "        conv2 = self.conv2a(pool1)  # 128 to 256\n",
    "        conv2 = F.relu(conv2)\n",
    "        conv2 = self.conv2b(conv2)  # 256 to 256\n",
    "        conv2 = F.relu(conv2)\n",
    "        bn2 = self.bn2(conv2)\n",
    "        pool2 = F.max_pool2d(bn2, kernel_size=(2, 2))\n",
    "\n",
    "        # conv3 = self.conv3a(pool2)  # 256 to 512\n",
    "        # conv3 = F.relu(conv3)\n",
    "        # conv3 = self.conv3b(conv3)  # 512 to 512\n",
    "        # conv3 = F.relu(conv3)\n",
    "        # bn3 = self.bn3(conv3)\n",
    "        # pool3 = F.max_pool2d(bn3, kernel_size=(2, 2))\n",
    "\n",
    "        # conv4 = self.conv4a(pool3)  # 512 to 512\n",
    "        # conv4 = F.relu(conv4)\n",
    "        # conv4 = self.conv4b(conv4)  # 512 to 512\n",
    "        # conv4 = F.relu(conv4)\n",
    "        # bn4 = self.bn4(conv4)\n",
    "        # pool4 = F.max_pool2d(bn4, kernel_size=(2, 2))\n",
    "\n",
    "        # conv5 = self.conv5a(pool4)  # 512 to 1024\n",
    "        # conv5 = F.relu(conv5)\n",
    "        # conv5 = self.conv5b(conv5)  # 1024 to 1024\n",
    "        # conv5 = F.relu(conv5)\n",
    "        # bn5 = self.bn5(conv5)\n",
    "\n",
    "        # up6 = F.upsample(bn5, scale_factor=2, mode=\"nearest\")\n",
    "        # up6 = self.conv6a(up6)  # 1024 to 512\n",
    "        # up6 = F.relu(up6)\n",
    "        # merge6 = torch.cat([bn4, up6], dim=1) # 512 and 512 to 1024 along c dimension\n",
    "        # conv6 = self.conv6b(merge6)  # 1024 to 512\n",
    "        # conv6 = F.relu(conv6)\n",
    "        # conv6 = self.conv6c(conv6)  # 512 to 512\n",
    "        # conv6 = F.relu(conv6)\n",
    "        # bn6 = self.bn6(conv6)\n",
    "\n",
    "        # up7 = F.upsample(bn6, scale_factor=2, mode=\"nearest\")\n",
    "        # up7 = self.conv7a(up7)  # 1024 to 512\n",
    "        # up7 = F.relu(up7)\n",
    "        # merge7 = torch.cat([bn3, up7], dim=1) # 512 and 512 to 1024 along c dimension\n",
    "        # conv7 = self.conv7b(merge7)  # 1024 to 512\n",
    "        # conv7 = F.relu(conv7)\n",
    "        # conv7 = self.conv7c(conv7)  # 512 to 512\n",
    "        # conv7 = F.relu(conv7)\n",
    "        # bn7 = self.bn7(conv7)\n",
    "\n",
    "        # up8 = F.upsample(bn7, scale_factor=2, mode=\"nearest\")\n",
    "        # up8 = self.conv8a(up8)  # 512 to 256\n",
    "        # up8 = F.relu(up8)\n",
    "        # merge8 = torch.cat([bn2, up8], dim=1) # 256 and 256 to 512 along c dimension\n",
    "        # conv8 = self.conv8b(merge8)  # 512 to 256\n",
    "        # conv8 = F.relu(conv8)\n",
    "        # conv8 = self.conv8c(conv8)  # 256 to 256\n",
    "        # conv8 = F.relu(conv8)\n",
    "        # bn8 = self.bn8(conv8)\n",
    "\n",
    "        up9 = F.upsample(bn8, scale_factor=2, mode=\"nearest\")\n",
    "        up9 = self.conv9a(up9)  # 256 to 128\n",
    "        up9 = F.relu(up9)\n",
    "        merge9 = torch.cat([bn1, up9], dim=1) # 128 and 128 to 256 along c dimension\n",
    "        conv9 = self.conv9b(merge9)  # 256 to 128\n",
    "        conv9 = F.relu(conv9)\n",
    "        conv9 = self.conv9c(conv9)  # 128 to 128\n",
    "        conv9 = F.relu(conv9)  # no batch norm on last layer\n",
    " \n",
    "        final_layer_logits = self.final_conv(conv9)\n",
    "\n",
    "        # transpose from shape (b, c, h, w) back to (b, h, w, c) to align with training data\n",
    "        final_layer_logits = torch.movedim(final_layer_logits, 1, -1)  # move c from second to final dim\n",
    "        b, h, w, c = final_layer_logits.shape\n",
    "\n",
    "        # unpack c=classes*months dimension into classes, months as separate dimensions\n",
    "        final_layer_logits = final_layer_logits.reshape((b, h, w, self.n_output_classes, self.n_forecast_days))\n",
    "\n",
    "        output = F.softmax(final_layer_logits, dim=-2)  # apply over n_output_classes dimension\n",
    "        \n",
    "        return output  # shape (b, h, w, c, t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some metrics for evaluating IceNet performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import Metric\n",
    "\n",
    "class IceNetAccuracy(Metric):\n",
    "    \"\"\"\n",
    "    Binary accuracy metric for use at multiple leadtimes.\n",
    "    \"\"\"    \n",
    "\n",
    "    # Set class properties\n",
    "    is_differentiable: bool = False\n",
    "    higher_is_better: bool = True\n",
    "    full_state_update: bool = True\n",
    "\n",
    "    def __init__(self, leadtimes_to_evaluate: list):\n",
    "        \"\"\"\n",
    "        Construct a binary accuracy metric for use at multiple leadtimes.\n",
    "        :param leadtimes_to_evaluate: A list of leadtimes to consider\n",
    "            e.g., [0, 1, 2, 3, 4, 5] to consider all six months in accuracy computation or\n",
    "            e.g., [0] to only look at the first month's accuracy\n",
    "            e.g., [5] to only look at the sixth month's accuracy\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.leadtimes_to_evaluate = leadtimes_to_evaluate\n",
    "        self.add_state(\"weighted_score\", default=torch.tensor(0.), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"possible_score\", default=torch.tensor(0.), dist_reduce_fx=\"sum\")\n",
    "\n",
    "    def update(self, preds: torch.Tensor, target: torch.Tensor, sample_weight: torch.Tensor):\n",
    "        # preds and target are shape (b, h, w, t)\n",
    "        # sum marginal and full ice for binary eval\n",
    "        preds = (preds > 0).long()\n",
    "        target = (target > 0).long()\n",
    "        base_score = preds[:, :, :, self.leadtimes_to_evaluate] == target[:, :, :, self.leadtimes_to_evaluate]\n",
    "        self.weighted_score += torch.sum(base_score * sample_weight[:, :, :, self.leadtimes_to_evaluate])\n",
    "        self.possible_score += torch.sum(sample_weight[:, :, :, self.leadtimes_to_evaluate])\n",
    "\n",
    "    def compute(self):\n",
    "        return self.weighted_score.float() / self.possible_score\n",
    "\n",
    "\n",
    "class SIEError(Metric):\n",
    "    \"\"\"\n",
    "    Sea Ice Extent error metric (in km^2) for use at multiple leadtimes.\n",
    "    \"\"\" \n",
    "\n",
    "    # Set class properties\n",
    "    is_differentiable: bool = False\n",
    "    higher_is_better: bool = False\n",
    "    full_state_update: bool = True\n",
    "\n",
    "    def __init__(self, leadtimes_to_evaluate: list):\n",
    "        \"\"\"\n",
    "        Construct an SIE error metric (in km^2) for use at multiple leadtimes.\n",
    "        :param leadtimes_to_evaluate: A list of leadtimes to consider\n",
    "            e.g., [0, 1, 2, 3, 4, 5] to consider all six months in computation or\n",
    "            e.g., [0] to only look at the first month\n",
    "            e.g., [5] to only look at the sixth month\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.leadtimes_to_evaluate = leadtimes_to_evaluate\n",
    "        self.add_state(\"pred_sie\", default=torch.tensor(0.), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"true_sie\", default=torch.tensor(0.), dist_reduce_fx=\"sum\")\n",
    "\n",
    "    def update(self, preds: torch.Tensor, target: torch.Tensor, sample_weight: torch.Tensor):\n",
    "        # preds and target are shape (b, h, w, t)\n",
    "        # sum marginal and full ice for binary eval\n",
    "        preds = (preds > 0).long()\n",
    "        target = (target > 0).long()\n",
    "        self.pred_sie += preds[:, :, :, self.leadtimes_to_evaluate].sum()\n",
    "        self.true_sie += target[:, :, :, self.leadtimes_to_evaluate].sum()\n",
    "\n",
    "    def compute(self):\n",
    "        return (self.pred_sie - self.true_sie) * 25**2 # each pixel is 25x25 km"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A _LightningModule_ wrapper for UNet model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning.pytorch as pl\n",
    "from torchmetrics import MetricCollection\n",
    "\n",
    "class LitUNet(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    A LightningModule wrapping the UNet implementation of IceNet.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 model: nn.Module,\n",
    "                 criterion: callable,\n",
    "                 learning_rate: float):\n",
    "        \"\"\"\n",
    "        Construct a UNet LightningModule.\n",
    "        Note that we keep hyperparameters separate from dataloaders to prevent data leakage at test time.\n",
    "        :param model: PyTorch model\n",
    "        :param criterion: PyTorch loss function for training instantiated with reduction=\"none\"\n",
    "        :param learning_rate: Float learning rate for our optimiser\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_output_classes = model.n_output_classes  # this should be a property of the network\n",
    "\n",
    "        metrics = {\n",
    "            \"val_accuracy\": IceNetAccuracy(leadtimes_to_evaluate=list(range(self.model.n_forecast_days))),\n",
    "            \"val_sieerror\": SIEError(leadtimes_to_evaluate=list(range(self.model.n_forecast_days)))\n",
    "        }\n",
    "        for i in range(self.model.n_forecast_days):\n",
    "            metrics[f\"val_accuracy_{i}\"] = IceNetAccuracy(leadtimes_to_evaluate=[i])\n",
    "            metrics[f\"val_sieerror_{i}\"] = SIEError(leadtimes_to_evaluate=[i])\n",
    "        self.metrics = MetricCollection(metrics)\n",
    "\n",
    "        test_metrics = {\n",
    "            \"test_accuracy\": IceNetAccuracy(leadtimes_to_evaluate=list(range(self.model.n_forecast_days))),\n",
    "            \"test_sieerror\": SIEError(leadtimes_to_evaluate=list(range(self.model.n_forecast_days)))\n",
    "        }\n",
    "        for i in range(self.model.n_forecast_days):\n",
    "            test_metrics[f\"test_accuracy_{i}\"] = IceNetAccuracy(leadtimes_to_evaluate=[i])\n",
    "            test_metrics[f\"test_sieerror_{i}\"] = SIEError(leadtimes_to_evaluate=[i])\n",
    "        self.test_metrics = MetricCollection(test_metrics)\n",
    "\n",
    "        self.save_hyperparameters(ignore=[\"model\", \"criterion\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Implement forward function.\n",
    "        :param x: Inputs to model.\n",
    "        :return: Outputs of model.\n",
    "        \"\"\"\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        \"\"\"\n",
    "        Perform a pass through a batch of training data.\n",
    "        Apply pixel-weighted loss by manually reducing.\n",
    "        See e.g. https://discuss.pytorch.org/t/unet-pixel-wise-weighted-loss-function/46689/5.\n",
    "        :param batch: Batch of input, output, weight triplets\n",
    "        :param batch_idx: Index of batch\n",
    "        :return: Loss from this batch of data for use in backprop\n",
    "        \"\"\"\n",
    "        x, y, sample_weight = batch\n",
    "        y_hat = self.model(x)\n",
    "        # y and y_hat are shape (b, h, w, c, t) but loss expects (b, c, h, w, t)\n",
    "        # note that criterion needs reduction=\"none\" for weighting to work\n",
    "        if isinstance(self.criterion, nn.CrossEntropyLoss):  # requires int class encoding\n",
    "            loss = self.criterion(y_hat.movedim(-2, 1), y.argmax(-2).long())\n",
    "        else:  # requires one-hot encoding\n",
    "            loss = self.criterion(y_hat.movedim(-2, 1), y.movedim(-2, 1))\n",
    "        loss = torch.mean(loss * sample_weight.movedim(-2, 1))\n",
    "        self.log(\"train_loss\", loss, sync_dist=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        x, y, sample_weight = batch\n",
    "        y_hat = self.model(x)\n",
    "        # y and y_hat are shape (b, h, w, c, t) but loss expects (b, c, h, w, t)\n",
    "        # note that criterion needs reduction=\"none\" for weighting to work\n",
    "        if isinstance(self.criterion, nn.CrossEntropyLoss):  # requires int class encoding\n",
    "            loss = self.criterion(y_hat.movedim(-2, 1), y.argmax(-2).long())\n",
    "        else:  # requires one-hot encoding\n",
    "            loss = self.criterion(y_hat.movedim(-2, 1), y.movedim(-2, 1))\n",
    "        loss = torch.mean(loss * sample_weight.movedim(-2, 1))\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, sync_dist=True)  # epoch-level loss\n",
    "        y_hat_pred = y_hat.argmax(dim=-2).long()  # argmax over c where shape is (b, h, w, c, t)\n",
    "        self.metrics.update(y_hat_pred, y.argmax(dim=-2).long(), sample_weight.squeeze(dim=-2))  # shape (b, h, w, t)\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        self.log_dict(self.metrics.compute(), on_step=False, on_epoch=True, sync_dist=True)  # epoch-level metrics\n",
    "        self.metrics.reset()\n",
    "\n",
    "    def test_step(self, batch):\n",
    "        x, y, sample_weight = batch\n",
    "        y_hat = self.model(x)\n",
    "        # y and y_hat are shape (b, h, w, c, t) but loss expects (b, c, h, w, t)\n",
    "        # note that criterion needs reduction=\"none\" for weighting to work\n",
    "        if isinstance(self.criterion, nn.CrossEntropyLoss):  # requires int class encoding\n",
    "            loss = self.criterion(y_hat.movedim(-2, 1), y.argmax(-2).long())\n",
    "        else:  # requires one-hot encoding\n",
    "            loss = self.criterion(y_hat.movedim(-2, 1), y.movedim(-2, 1))\n",
    "        loss = torch.mean(loss * sample_weight.movedim(-2, 1))\n",
    "        self.log(\"test_loss\", loss, on_step=False, on_epoch=True, sync_dist=True)  # epoch-level loss\n",
    "        y_hat_pred = y_hat.argmax(dim=-2)  # argmax over c where shape is (b, h, w, c, t)\n",
    "        self.test_metrics.update(y_hat_pred, y.argmax(dim=-2).long(), sample_weight.squeeze(dim=-2))  # shape (b, h, w, t)\n",
    "        return loss\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        self.log_dict(self.test_metrics.compute(),on_step=False, on_epoch=True, sync_dist=True)  # epoch-level metrics\n",
    "        self.test_metrics.reset()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return {\n",
    "            \"optimizer\": optimizer\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for training UNet model using PyTorch Lightning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "\n",
    "def train_icenet(configuration_path,\n",
    "                 learning_rate,\n",
    "                 max_epochs,\n",
    "                 batch_size,\n",
    "                 n_workers,\n",
    "                 filter_size,\n",
    "                 n_filters_factor,\n",
    "                 seed):\n",
    "    \"\"\"\n",
    "    Train IceNet using the arguments specified in the `args` namespace.\n",
    "    :param args: Namespace of configuration parameters\n",
    "    \"\"\"\n",
    "    # init\n",
    "    pl.seed_everything(seed)\n",
    "    \n",
    "    # configure datasets and dataloaders\n",
    "    train_dataset = IceNetDataSetPyTorch(configuration_path, mode=\"train\")\n",
    "    val_dataset = IceNetDataSetPyTorch(configuration_path, mode=\"val\")\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=n_workers,\n",
    "                                  persistent_workers=True, shuffle=False)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, num_workers=n_workers,\n",
    "                                persistent_workers=True, shuffle=False)\n",
    "\n",
    "    # construct unet\n",
    "    model = UNet(\n",
    "        input_channels=len(train_dataset._ds._config[\"channels\"]),\n",
    "        filter_size=filter_size,\n",
    "        n_filters_factor=n_filters_factor,\n",
    "        n_forecast_days=train_dataset._ds._config[\"n_forecast_days\"]\n",
    "    )\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "    \n",
    "    # configure PyTorch Lightning module\n",
    "    lit_module = LitUNet(\n",
    "        model=model,\n",
    "        criterion=criterion,\n",
    "        learning_rate=learning_rate\n",
    "    )\n",
    "\n",
    "    # set up trainer configuration\n",
    "    trainer = pl.Trainer(\n",
    "        accelerator=\"auto\",\n",
    "        devices=1,\n",
    "        log_every_n_steps=10,\n",
    "        max_epochs=max_epochs,\n",
    "        num_sanity_val_steps=1,\n",
    "    )\n",
    "    trainer.callbacks.append(ModelCheckpoint(monitor=\"val_accuracy\", mode=\"max\"))\n",
    "\n",
    "    # train model\n",
    "    print(f\"Training {len(train_dataset)} examples / {len(train_dataloader)} batches (batch size {batch_size}).\")\n",
    "    print(f\"Validating {len(val_dataset)} examples / {len(val_dataloader)} batches (batch size {batch_size}).\")\n",
    "    trainer.fit(lit_module, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Global seed set to 45\n",
      "INFO:lightning.fabric.utilities.seed:Global seed set to 45\n",
      "INFO:root:Loading configuration dataset_config.pytorch_notebook.json\n",
      "WARNING:root:Running in configuration only mode, tfrecords were not generated for this dataset\n",
      "INFO:root:Loading configuration /data/hpcdata/users/rychan/notebooks/notebook-pipeline/loader.notebook_api_data.json\n",
      "INFO:root:Loading configuration dataset_config.pytorch_notebook.json\n",
      "WARNING:root:Running in configuration only mode, tfrecords were not generated for this dataset\n",
      "INFO:root:Loading configuration /data/hpcdata/users/rychan/notebooks/notebook-pipeline/loader.notebook_api_data.json\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO: IPU available: False, using: 0 IPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "  | Name         | Type             | Params\n",
      "--------------------------------------------------\n",
      "0 | model        | UNet             | 1.8 M \n",
      "1 | criterion    | CrossEntropyLoss | 0     \n",
      "2 | metrics      | MetricCollection | 0     \n",
      "3 | test_metrics | MetricCollection | 0     \n",
      "--------------------------------------------------\n",
      "1.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.8 M     Total params\n",
      "7.224     Total estimated model params size (MB)\n",
      "INFO:lightning.pytorch.callbacks.model_summary:\n",
      "  | Name         | Type             | Params\n",
      "--------------------------------------------------\n",
      "0 | model        | UNet             | 1.8 M \n",
      "1 | criterion    | CrossEntropyLoss | 0     \n",
      "2 | metrics      | MetricCollection | 0     \n",
      "3 | test_metrics | MetricCollection | 0     \n",
      "--------------------------------------------------\n",
      "1.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.8 M     Total params\n",
      "7.224     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 91 examples / 23 batches (batch size 4).\n",
      "Validating 21 examples / 6 batches (batch size 4).\n",
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    }
   ],
   "source": [
    "seed = 45\n",
    "train_icenet(configuration_path=dataset_config,\n",
    "             learning_rate=1e-4,\n",
    "             max_epochs=10,\n",
    "             batch_size=4,\n",
    "             n_workers=12,\n",
    "             filter_size=3,\n",
    "             n_filters_factor=1,\n",
    "             seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset._ds._config[\"channels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset._ds._config[\"n_forecast_days\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icenet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
