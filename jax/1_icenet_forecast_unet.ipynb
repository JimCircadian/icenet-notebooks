{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch UNet implementation using IceNet library for data download and post-processing of sea ice forecasting.\n",
    "\n",
    "This notebook has been designed to be independent of other notebooks.\n",
    "\n",
    "### Highlights\n",
    "The key features of this notebook are:\n",
    "* [1. Download](#1.-Download) \n",
    "* [2. Data Processing](#2.-Data-Processing)\n",
    "* [3. Train](#3.-Train)\n",
    "* [4. Prediction](#4.-Prediction)\n",
    "* [5. Outputs and Plotting](#5.-Outputs-and-Plotting)\n",
    "\n",
    "It currently uses a dev version of IceNet library (v0.2.8_dev) to run.\n",
    "\n",
    "To install, can use the conda `icenet-notebooks/pytorch/environment.yml` environment file on a Linux system to be able to set-up the necessary pytorch + tensorflow + cuda + other modules which could be a tricky mix to get working manually:\n",
    "\n",
    "```bash\n",
    "conda env create -f environment.yml\n",
    "```\n",
    "\n",
    "### Contributions\n",
    "#### PyTorch implementation of IceNet\n",
    "\n",
    "Andrew McDonald ([icenet-gan](https://github.com/ampersandmcd/icenet-gan))\n",
    "\n",
    "Bryn Noel Ubald (Refactor, updates for daily predictions and matching icenet library)\n",
    "\n",
    "#### Notebook\n",
    "Bryn Noel Ubald (author)\n",
    "\n",
    "#### PyTorch Integration\n",
    "Bryn Noel Ubald\n",
    "\n",
    "Ryan Chan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# We also set the logging level so that we get some feedback from the API\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JAX/FLAX imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax.training import checkpoints, train_state\n",
    "from flax import struct, serialization\n",
    "import orbax.checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icenet.data.sic.mask import Masks\n",
    "from icenet.data.interfaces.cds import ERA5Downloader\n",
    "from icenet.data.sic.osisaf import SICDownloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask data\n",
    "\n",
    "Create masks for masking data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = Masks(north=False, south=True)\n",
    "masks.generate(save_polarhole_masks=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Climate and Sea Ice data\n",
    "\n",
    "Download climate variables from ERA5 and sea ice concentration from OSI-SAF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "era5 = ERA5Downloader(\n",
    "    var_names=[\"tas\", \"zg\", \"uas\", \"vas\"],\n",
    "    levels=[None, [250, 500], None, None],\n",
    "    dates=[pd.to_datetime(date).date() for date in\n",
    "           pd.date_range(\"2020-01-01\", \"2020-04-30\", freq=\"D\")],\n",
    "    delete_tempfiles=False,\n",
    "    max_threads=64,\n",
    "    north=False,\n",
    "    south=True,\n",
    "    # NOTE: there appears to be a bug with the toolbox API at present (icenet#54)\n",
    "    use_toolbox=False\n",
    ")\n",
    "\n",
    "era5.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sic = SICDownloader(\n",
    "    dates=[pd.to_datetime(date).date() for date in\n",
    "           pd.date_range(\"2020-01-01\", \"2020-04-30\", freq=\"D\")],\n",
    "    delete_tempfiles=False,\n",
    "    north=False,\n",
    "    south=True,\n",
    "    parallel_opens=False,\n",
    ")\n",
    "\n",
    "sic.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-grid ERA5 reanalysis data, and rotate wind vector data from ERA5 to align with EASE2 projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "era5.regrid()\n",
    "era5.rotate_wind_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Processing\n",
    "\n",
    "Process downloaded datasets.\n",
    "\n",
    "To make life easier, setting up train, val, test dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_dates = dict(\n",
    "    train=[pd.to_datetime(el) for el in pd.date_range(\"2020-01-01\", \"2020-03-31\")],\n",
    "    val=[pd.to_datetime(el) for el in pd.date_range(\"2020-04-03\", \"2020-04-23\")],\n",
    "    test=[pd.to_datetime(el) for el in pd.date_range(\"2020-04-01\", \"2020-04-02\")],\n",
    ")\n",
    "processed_name = \"notebook_api_jax_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_dates[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create the data producer and configure them for the dataset we want to create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icenet.data.processors.era5 import IceNetERA5PreProcessor\n",
    "from icenet.data.processors.meta import IceNetMetaPreProcessor\n",
    "from icenet.data.processors.osi import IceNetOSIPreProcessor\n",
    "\n",
    "pp = IceNetERA5PreProcessor(\n",
    "    [\"uas\", \"vas\"],\n",
    "    [\"tas\", \"zg500\", \"zg250\"],\n",
    "    processed_name,\n",
    "    processing_dates[\"train\"],\n",
    "    processing_dates[\"val\"],\n",
    "    processing_dates[\"test\"],\n",
    "    linear_trends=tuple(),\n",
    "    north=False,\n",
    "    south=True\n",
    ")\n",
    "\n",
    "osi = IceNetOSIPreProcessor(\n",
    "    [\"siconca\"],\n",
    "    [],\n",
    "    processed_name,\n",
    "    processing_dates[\"train\"],\n",
    "    processing_dates[\"val\"],\n",
    "    processing_dates[\"test\"],\n",
    "    linear_trends=tuple(),\n",
    "    north=False,\n",
    "    south=True\n",
    ")\n",
    "\n",
    "meta = IceNetMetaPreProcessor(\n",
    "    processed_name,\n",
    "    north=False,\n",
    "    south=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we initialise the data processors using `init_source_data` which scans the data source directories to understand what data is available for processing based on the parameters. Since we named the processed data `\"notebook_api_data\"` above, it will create a data loader config file, `loader.notebook_api_data.json`, in the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.init_source_data(\n",
    "    lag_days=1,\n",
    ")\n",
    "pp.process()\n",
    "\n",
    "osi.init_source_data(\n",
    "    lag_days=1,\n",
    ")\n",
    "osi.process()\n",
    "\n",
    "meta.process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point the preprocessed data is ready to convert or create a configuration for the network dataset.\n",
    "\n",
    "### Dataset creation\n",
    "\n",
    "As with the `icenet_dataset_create` command we can create a dataset configuration for training the network. As before this can include cached data for the network in the format of a TFRecordDataset compatible set of tfrecords. To achieve this we create the `IceNetDataLoader`, which can both generate `IceNetDataSet` configurations (which easily provide the necessary functionality for training and prediction) as well as individual data samples for direct usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icenet.data.loaders import IceNetDataLoaderFactory\n",
    "\n",
    "implementation = \"dask\"\n",
    "loader_config = \"loader.notebook_api_jax_data.json\"\n",
    "dataset_name = \"notebook_api_jax_data\"\n",
    "lag = 1\n",
    "\n",
    "dl = IceNetDataLoaderFactory().create_data_loader(\n",
    "    implementation,\n",
    "    loader_config,\n",
    "    dataset_name,\n",
    "    lag,\n",
    "    n_forecast_days=7,\n",
    "    north=False,\n",
    "    south=True,\n",
    "    output_batch_size=1,\n",
    "    generate_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we can either use `generate` or `write_dataset_config_only` to produce a ready-to-go `IceNetDataSet` configuration. Both of these will generate a dataset config, `dataset_config.notebook_api_pytorch_data.json` (recall we set the dataset name as `notebook_api_pytorch_data` above).\n",
    "\n",
    "In this case, for pytorch, will read data in directly, rather than using cached tfrecords inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl.write_dataset_config_only()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create the IceNetDataSet object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icenet.data.dataset import IceNetDataSetPyTorch\n",
    "dataset_config = f\"dataset_config.{dataset_name}.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "shuffle = False\n",
    "persistent_workers=True\n",
    "num_workers = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataloader interface for Jax from Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax.tree_util import tree_map\n",
    "from torch.utils import data\n",
    "\n",
    "def numpy_collate(batch):\n",
    "  return tree_map(np.asarray, data.default_collate(batch))\n",
    "\n",
    "class NumpyDataLoader(data.DataLoader):\n",
    "  def __init__(self, dataset, batch_size=1,\n",
    "                shuffle=False, sampler=None,\n",
    "                batch_sampler=None, num_workers=0,\n",
    "                pin_memory=False, drop_last=False,\n",
    "                timeout=0, worker_init_fn=None):\n",
    "    super(self.__class__, self).__init__(dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        sampler=sampler,\n",
    "        batch_sampler=batch_sampler,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=numpy_collate,\n",
    "        pin_memory=pin_memory,\n",
    "        drop_last=drop_last,\n",
    "        timeout=timeout,\n",
    "        worker_init_fn=worker_init_fn)\n",
    "\n",
    "class FlattenAndCast(object):\n",
    "  def __call__(self, pic):\n",
    "    return np.ravel(np.array(pic, dtype=jnp.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train\n",
    "\n",
    "We implement a custom PyTorch class for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "from jax import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(batch_size, configuration_path, n_workers=1):\n",
    "    # configure datasets and dataloaders\n",
    "    train_dataset = IceNetDataSetPyTorch(configuration_path, mode=\"train\")\n",
    "    val_dataset = IceNetDataSetPyTorch(configuration_path, mode=\"val\")\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=n_workers,\n",
    "                                  persistent_workers=persistent_workers, shuffle=False)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, num_workers=n_workers,\n",
    "                                persistent_workers=persistent_workers, shuffle=False)\n",
    "    \n",
    "    test_dataset = IceNetDataSetPyTorch(configuration_path=dataset_config, mode=\"test\")\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers,\n",
    "                                persistent_workers=persistent_workers, shuffle=False)\n",
    "    return train_dataset, train_dataloader, val_dataset, val_dataloader, test_dataset, test_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IceNet UNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from flax import linen as nn\n",
    "from jax.image import resize\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"UNet implementation for binary classification for IceNet\"\"\"\n",
    "\n",
    "    padding = \"SAME\"\n",
    "    filter_size = (3, 3)\n",
    "    n_filters_factor = 0.1\n",
    "    n_forecast_days = 7\n",
    "    # kernel_init = nn.initializers.he_normal()\n",
    "    # kernel_init(jax.random.key(42), (3, 3), jnp.float32)\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, train=True):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        filter_size = self.filter_size\n",
    "        n_filters_factor = self.n_filters_factor\n",
    "        n_forecast_days = self.n_forecast_days\n",
    "\n",
    "        start_out_channels = 64\n",
    "        reduced_channels = int(start_out_channels * n_filters_factor)\n",
    "        channels = {\n",
    "            start_out_channels * 2**pow: reduced_channels * 2**pow\n",
    "            for pow in range(4)\n",
    "        }\n",
    "        \n",
    "        conv1 = nn.Conv(channels[64], kernel_size=filter_size, padding=self.padding)(x)\n",
    "        conv1 = nn.relu(conv1)\n",
    "        conv1 = nn.Conv(channels[64], kernel_size=filter_size, padding=self.padding)(conv1)\n",
    "        conv1 = nn.relu(conv1)\n",
    "        bn1 = nn.BatchNorm(use_running_average=not train)(conv1)\n",
    "        pool1 = nn.max_pool(bn1, window_shape=(2, 2), strides=(2, 2))\n",
    "        \n",
    "        conv2 = nn.Conv(channels[128], kernel_size=filter_size, padding=self.padding,)(pool1)\n",
    "        conv2 = nn.relu(conv2)\n",
    "        conv2 = nn.Conv(channels[128], kernel_size=filter_size, padding=self.padding,)(conv2)\n",
    "        conv2 = nn.relu(conv2)\n",
    "        bn2 = nn.BatchNorm(use_running_average=not train)(conv2)\n",
    "        pool2 = nn.max_pool(bn2, window_shape=(2, 2), strides=(2, 2))\n",
    "\n",
    "        conv3 = nn.Conv(channels[256], kernel_size=filter_size, padding=self.padding,)(pool2)\n",
    "        conv3 = nn.relu(conv3)\n",
    "        conv3 = nn.Conv(channels[256], kernel_size=filter_size, padding=self.padding,)(conv3)\n",
    "        conv3 = nn.relu(conv3)\n",
    "        bn3 = nn.BatchNorm(use_running_average=not train)(conv3)\n",
    "        pool3 = nn.max_pool(bn3, window_shape=(2, 2), strides=(2, 2))\n",
    "\n",
    "        conv4 = nn.Conv(channels[256], kernel_size=filter_size, padding=self.padding,)(pool3)\n",
    "        conv4 = nn.relu(conv4)\n",
    "        conv4 = nn.Conv(channels[256], kernel_size=filter_size, padding=self.padding,)(conv4)\n",
    "        conv4 = nn.relu(conv4)\n",
    "        bn4 = nn.BatchNorm(use_running_average=not train)(conv4)\n",
    "        pool4 = nn.max_pool(bn4, window_shape=(2, 2), strides=(2, 2))\n",
    "\n",
    "        conv5 = nn.Conv(channels[512], kernel_size=filter_size, padding=self.padding,)(pool4)\n",
    "        conv5 = nn.relu(conv5)\n",
    "        conv5 = nn.Conv(channels[512], kernel_size=filter_size, padding=self.padding,)(conv5)\n",
    "        conv5 = nn.relu(conv5)\n",
    "        bn5 = nn.BatchNorm(use_running_average=not train)(conv5)\n",
    "\n",
    "        bn5 = resize(bn5, shape=(bn5.shape[0], bn5.shape[1]*2, bn5.shape[2]*2, bn5.shape[3]), method=\"nearest\")\n",
    "        up6 = nn.Conv(channels[256], kernel_size=(2, 2), padding=self.padding,)(bn5)\n",
    "        merge6 = jnp.concatenate([bn4, up6], axis=-1)\n",
    "        conv6 = nn.Conv(channels[256], kernel_size=filter_size, padding=self.padding,)(merge6)\n",
    "        conv6 = nn.relu(conv6)\n",
    "        conv6 = nn.Conv(channels[256], kernel_size=filter_size, padding=self.padding,)(conv6)\n",
    "        conv6 = nn.relu(conv6)\n",
    "        bn6 = nn.BatchNorm(use_running_average=not train)(conv6)\n",
    "\n",
    "        bn6 = resize(bn6, shape=(bn6.shape[0], bn6.shape[1]*2, bn6.shape[2]*2, bn6.shape[3]), method=\"nearest\")\n",
    "        up7 = nn.Conv(channels[256], kernel_size=(2, 2), padding=self.padding,)(bn6)\n",
    "        merge7 = jnp.concatenate([bn3, up7], axis=-1)\n",
    "        conv7 = nn.Conv(channels[256], kernel_size=filter_size, padding=self.padding,)(merge7)\n",
    "        conv7 = nn.relu(conv7)\n",
    "        conv7 = nn.Conv(channels[256], kernel_size=filter_size, padding=self.padding,)(conv7)\n",
    "        conv7 = nn.relu(conv7)\n",
    "        bn7 = nn.BatchNorm(use_running_average=not train)(conv7)\n",
    "\n",
    "        bn7 = resize(bn7, shape=(bn7.shape[0], bn7.shape[1]*2, bn7.shape[2]*2, bn7.shape[3]), method=\"nearest\")\n",
    "        up8 = nn.Conv(channels[128], kernel_size=(2, 2), padding=self.padding,)(bn7)\n",
    "        merge8 = jnp.concatenate([bn2, up8], axis=-1)\n",
    "        conv8 = nn.Conv(channels[128], kernel_size=filter_size, padding=self.padding,)(merge8)\n",
    "        conv8 = nn.relu(conv8)\n",
    "        conv8 = nn.Conv(channels[128], kernel_size=filter_size, padding=self.padding,)(conv8)\n",
    "        conv8 = nn.relu(conv8)\n",
    "        bn8 = nn.BatchNorm(use_running_average=not train)(conv8)\n",
    "\n",
    "        bn8 = resize(bn8, shape=(bn8.shape[0], bn8.shape[1]*2, bn8.shape[2]*2, bn8.shape[3]), method=\"nearest\")\n",
    "        up9 = nn.Conv(channels[64], kernel_size=(2, 2), padding=self.padding,)(bn8)\n",
    "        merge9 = jnp.concatenate([bn1, up9], axis=-1)\n",
    "\n",
    "        conv9 = nn.Conv(channels[64], kernel_size=filter_size, padding=self.padding,)(merge9)\n",
    "        conv9 = nn.Conv(channels[64], kernel_size=filter_size, padding=self.padding,)(conv9)\n",
    "        conv9 = nn.Conv(channels[64], kernel_size=filter_size, padding=self.padding,)(conv9)\n",
    "\n",
    "        final_layer = nn.Conv(n_forecast_days, kernel_size=(1, 1),)(conv9)\n",
    "\n",
    "        # if not train:\n",
    "        #     return nn.sigmoid(final_layer)\n",
    "        return final_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View model layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp  # JAX NumPy\n",
    "\n",
    "unet = UNet()\n",
    "print(unet.tabulate(jax.random.key(0), jnp.ones((1, 432, 432, 9)),\n",
    "                   train=True))\n",
    "\n",
    "# Newer flax version (>0.8.0)\n",
    "# print(unet.tabulate(jax.random.key(0), jnp.ones((1, 432, 432, 9)),\n",
    "#                    train=True,\n",
    "#                    compute_flops=True, compute_vjp_flops=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clu import metrics\n",
    "from flax.training import train_state  # Useful dataclass to keep train state\n",
    "from flax import struct                # Flax dataclasses\n",
    "import optax                           # Common loss functions and optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define custom metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@struct.dataclass\n",
    "class BinaryAccuracy(metrics.Metric):\n",
    "    weighted_score: jnp.ndarray\n",
    "    possible_score: jnp.ndarray\n",
    "\n",
    "    @classmethod\n",
    "    def empty(cls):\n",
    "        return cls(weighted_score=jnp.array(0, jnp.float32), possible_score=jnp.array(0, jnp.int32))\n",
    "\n",
    "    @classmethod\n",
    "    def from_model_output(cls, *, predictions, targets, sample_weights, **kwargs):\n",
    "        predictions = predictions > 0.15\n",
    "        targets = targets > 0.15\n",
    "        base_score = predictions == targets\n",
    "        return cls(\n",
    "            weighted_score = jnp.sum(base_score*sample_weights),\n",
    "            possible_score = jnp.sum(sample_weights)\n",
    "        )\n",
    "\n",
    "    def compute(self):\n",
    "        binary_accuracy = self.weighted_score / self.possible_score * 100\n",
    "        return binary_accuracy\n",
    "\n",
    "@struct.dataclass\n",
    "class RMSE(metrics.Metric):\n",
    "    rmse: float\n",
    "\n",
    "    @classmethod\n",
    "    def empty(cls):\n",
    "        return cls(rmse=0.)\n",
    "\n",
    "    @classmethod\n",
    "    def from_model_output(cls, *, predictions, targets, sample_weights, **kwargs):\n",
    "        predictions = 100*(predictions > 0.15)*sample_weights\n",
    "        targets = 100*(targets > 0.15)*sample_weights\n",
    "        rmse = jnp.sqrt(jnp.mean(jnp.square(predictions - targets)))\n",
    "        return cls(\n",
    "            rmse = rmse\n",
    "        )\n",
    "\n",
    "    def compute(self):\n",
    "        return self.rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@struct.dataclass\n",
    "class Metrics(metrics.Collection):\n",
    "  # accuracy: metrics.Accuracy\n",
    "  accuracy: BinaryAccuracy\n",
    "  rmse : RMSE\n",
    "  loss: metrics.Average.from_output('loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define training state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainState(train_state.TrainState):\n",
    "  # Ref: https://flax.readthedocs.io/en/latest/guides/training_techniques/batch_norm.html#training-and-evaluation\n",
    "  batch_stats: any\n",
    "  metrics: Metrics\n",
    "\n",
    "def create_train_state(module, rng, learning_rate, momentum):\n",
    "  \"\"\"Creates an initial `TrainState`.\"\"\"\n",
    "  variables = module.init(rng, jnp.ones([1, 432, 432, 9]), train=False) # initialize parameters by passing a template image\n",
    "  params = variables['params'] # initialize parameters by passing a template image\n",
    "  batch_stats = variables['batch_stats']\n",
    "  tx = optax.sgd(learning_rate, momentum)\n",
    "  return TrainState.create(\n",
    "      apply_fn=module.apply,\n",
    "      params=params,\n",
    "      batch_stats=batch_stats,\n",
    "      tx=tx,\n",
    "      metrics=Metrics.empty()\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step(state, batch):\n",
    "  \"\"\"Train for a single step.\"\"\"\n",
    "  inputs, targets, sample_weights = batch\n",
    "  def loss_fn(params):\n",
    "    # Ref: https://flax.readthedocs.io/en/latest/guides/training_techniques/batch_norm.html#training-and-evaluation\n",
    "    logits, updates = state.apply_fn(\n",
    "      {'params': params, 'batch_stats': state.batch_stats},\n",
    "      x=inputs,\n",
    "      train=True,\n",
    "      mutable=['batch_stats']\n",
    "      )\n",
    "    batch_stats = updates['batch_stats']\n",
    "    # loss = optax.softmax_cross_entropy_with_integer_labels(\n",
    "    #     logits=logits, labels=batch['label']).mean()\n",
    "    # print(\"Checking shapes:\", logits.shape, targets.shape, sample_weights.shape)\n",
    "    loss = optax.l2_loss(100*nn.sigmoid(logits)*sample_weights, 100*targets*sample_weights).mean()\n",
    "    return loss, (logits, updates)\n",
    "  grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "  (loss, (logits, updates)), grads = grad_fn(state.params)\n",
    "  state = state.apply_gradients(grads=grads)\n",
    "  state = state.replace(batch_stats=updates['batch_stats'])\n",
    "\n",
    "  # preds = nn.sigmoid(logits) > 0.15\n",
    "  # targets = targets > 0.15\n",
    "  # base_score = preds == targets\n",
    "  # weighted_score = jnp.sum(base_score*sample_weights)\n",
    "  # possible_score = jnp.sum(sample_weights)\n",
    "  # metrics = {\n",
    "  #   'loss': loss,\n",
    "  #   'binary_accuracy': weighted_score / possible_score * 100\n",
    "  # }\n",
    "  return state#, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric computaton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def compute_metrics(*, state, batch):\n",
    "  inputs, targets, sample_weights = batch\n",
    "  # Ref: https://flax.readthedocs.io/en/latest/guides/training_techniques/batch_norm.html#training-and-evaluation\n",
    "  logits, updates = state.apply_fn(\n",
    "    {'params': state.params, 'batch_stats': state.batch_stats},\n",
    "    x=inputs,\n",
    "    train=True,\n",
    "    mutable=['batch_stats']\n",
    "    )\n",
    "  batch_stats = updates['batch_stats']\n",
    "#   loss = optax.softmax_cross_entropy_with_integer_labels(\n",
    "#         logits=logits, labels=batch['label']).mean()\n",
    "  loss = optax.l2_loss(\n",
    "    predictions=100*nn.sigmoid(logits)*sample_weights,\n",
    "    targets=100*targets*sample_weights\n",
    "    ).mean()\n",
    "  \n",
    "  metric_updates = state.metrics.single_from_model_output(\n",
    "    predictions=nn.sigmoid(logits)*sample_weights, targets=targets*sample_weights, sample_weights=sample_weights, loss=loss)\n",
    "  state = state.replace(metrics=metric_updates)\n",
    "  # metrics = state.metrics.merge(metric_updates)\n",
    "  # state = state.replace(metrics=metrics)\n",
    "\n",
    "\n",
    "  # preds = nn.sigmoid(logits) > 0.15\n",
    "  # targets = targets > 0.15\n",
    "  # base_score = preds == targets\n",
    "  # weighted_score = jnp.sum(base_score*sample_weights)\n",
    "  # possible_score = jnp.sum(sample_weights)\n",
    "  # metrics = {\n",
    "  #   'loss': loss,\n",
    "  #   'accuracy': weighted_score / possible_score * 100\n",
    "  # }\n",
    "  # state = state.replace(metrics=metrics)\n",
    "  return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "# batch_size = 4 # Defined earlier...\n",
    "\n",
    "train_dataset, train_ds, val_dataset, val_ds, test_dataset, test_ds = get_datasets(configuration_path=dataset_config, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set seed randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "init_rng = jax.random.key(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the `TrainState`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "momentum = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = create_train_state(unet, init_rng, learning_rate, momentum)\n",
    "del init_rng  # Must not be used anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "num_steps_per_epoch = math.ceil(len(processing_dates[\"train\"]) / batch_size)\n",
    "num_steps_per_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise metrics history and checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_history = {'train_loss': [],\n",
    "                   'train_accuracy': [],\n",
    "                   'train_rmse': [],\n",
    "                   'val_loss': [],\n",
    "                   'val_accuracy': [],\n",
    "                   'val_rmse': []\n",
    "                   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_dir = '/tmp/flax_ckpt'\n",
    "\n",
    "if os.path.exists(ckpt_dir):\n",
    "    shutil.rmtree(ckpt_dir)  # Remove any existing checkpoints from the last notebook run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax.training import orbax_utils\n",
    "\n",
    "# orbax_checkpointer = orbax.checkpoint.AsyncCheckpointer(orbax.checkpoint.PyTreeCheckpointHandler())\n",
    "# # options = orbax.checkpoint.CheckpointManagerOptions(max_to_keep=4, create=True, best_fn=lambda metrics: metrics[\"val_rmse\"], best_mode=\"min\")\n",
    "# options = orbax.checkpoint.CheckpointManagerOptions(max_to_keep=4, create=True)\n",
    "# out_path = os.path.abspath('managed-checkpoint')\n",
    "# checkpoint_manager = orbax.checkpoint.CheckpointManager(out_path, options=options)\n",
    "\n",
    "options = orbax.checkpoint.CheckpointManagerOptions(max_to_keep=1, create=True, best_fn=lambda metrics: metrics, best_mode='min')\n",
    "checkpoint_manager = orbax.checkpoint.CheckpointManager('/tmp/flax_ckpt/orbax/managed', options=options)\n",
    "\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "  for step, batch in enumerate(train_ds):\n",
    "\n",
    "    # Convert to numpy\n",
    "    batch = [element.numpy()[..., 0] if element.shape[-1] == 1 else element.numpy() for element in batch]\n",
    "    # batch[0] = jnp.expand_dims(batch[0], axis=-1)\n",
    "    # print(\"Init shapes\", batch[0].shape, batch[1].shape, batch[2].shape)\n",
    "\n",
    "    # Run optimization steps over training batches and compute batch metrics\n",
    "    state = train_step(state, batch) # get updated train state (which contains the updated parameters)\n",
    "    state = compute_metrics(state=state, batch=batch) # aggregate batch metrics\n",
    "\n",
    "    # print(f\"Step: {step+1}, num_steps_per_epoch: {num_steps_per_epoch}, check: {(step+1) % num_steps_per_epoch}\")\n",
    "    if (step+1) % num_steps_per_epoch == 0: # one training epoch has passed\n",
    "      print(\"\\tRunning validation set\")\n",
    "      for metric,value in state.metrics.compute().items(): # compute metrics\n",
    "        metrics_history[f'train_{metric}'].append(value) # record metrics\n",
    "      # state = state.replace(metrics=state.metrics.empty()) # reset train_metrics for next training epoch\n",
    "\n",
    "      # Compute metrics on the validation set after each training epoch\n",
    "      val_state = state\n",
    "      for val_batch in val_ds:\n",
    "        # print(\"Val shapes\", val_batch[0].shape, val_batch[1].shape, val_batch[2].shape)\n",
    "        val_batch = [element.numpy()[..., 0] if element.shape[-1] == 1 else element.numpy() for element in val_batch]\n",
    "        # print(\"Val shapes\", val_batch[0].shape, val_batch[1].shape, val_batch[2].shape)\n",
    "        val_state = compute_metrics(state=val_state, batch=val_batch)\n",
    "\n",
    "      for metric,value in val_state.metrics.compute().items():\n",
    "        metrics_history[f'val_{metric}'].append(value)\n",
    "\n",
    "      print(f\"train epoch: {(epoch)}, \"\n",
    "            f\"loss: {metrics_history['train_loss'][-1]}, \"\n",
    "            f\"accuracy: {metrics_history['train_accuracy'][-1]}, \"\n",
    "            f\"rmse: {metrics_history['train_rmse'][-1]}\"\n",
    "            )\n",
    "      print(f\"val epoch: {(epoch)}, \"\n",
    "            f\"loss: {metrics_history['val_loss'][-1]}, \"\n",
    "            f\"accuracy: {metrics_history['val_accuracy'][-1]}, \"\n",
    "            f\"rmse: {metrics_history['val_rmse'][-1]}\"\n",
    "            )\n",
    "\n",
    "      print(\"Checkpointing...\")\n",
    "      # Bundle everything together.\n",
    "      ckpt = {'model': state}\n",
    "      # save_args = orbax_utils.save_args_from_target(ckpt)\n",
    "      save_args = orbax.checkpoint.args.StandardSave(state)\n",
    "      # orbax_checkpointer.save('/tmp/flax_ckpt/orbax/single_save', ckpt, save_args=save_args, force=True)\n",
    "      val_rmse = state.metrics.rmse.rmse.item()\n",
    "      # checkpoint_manager.save(epoch, save_kwargs={'save_args': save_args}, metrics=val_rmse)\n",
    "      checkpoint_manager.save(epoch, args=save_args, metrics=val_rmse)\n",
    "      \n",
    "      state = state.replace(metrics=state.metrics.empty()) # reset train_metrics for next training epoch\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#       # Bundle everything together.\n",
    "#       ckpt = {'model': state}\n",
    "#       save_args = orbax_utils.save_args_from_target(ckpt)\n",
    "#       checkpoint_manager.save(step, ckpt, save_kwargs={'save_args': save_args}, force=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#       # print(type(state))\n",
    "#       # # state['step'] = step\n",
    "#       # state_save_args = jax.tree_map(lambda _: orbax.checkpoint.SaveArgs(), state)\n",
    "#       # # print(state)\n",
    "#       # # checkpoint_manager.save(step, state, metrics=state.metrics)\n",
    "#       # checkpoint_manager.save(\n",
    "#       #   step,\n",
    "#       #   # # {\n",
    "#       #   # #   \"model_state\": state,\n",
    "#       #   # # }\n",
    "#       #   # # metrics=state.metrics\n",
    "#       #   # items={\n",
    "#       #   #     'state': state,\n",
    "#       #   # },\n",
    "#       #   # # save_kwargs must be a dict with the same keys as items.\n",
    "#       #   # # not all keys in items have to be provided, in which case default kwargs\n",
    "#       #   # # are used each value must be a dict with keyword args passed to the\n",
    "#       #   # # underlying CheckpointHandler for that item (see CheckpointManager\n",
    "#       #   # # object construction)\n",
    "#       #   # save_kwargs={'state': {\n",
    "#       #   #     'save_args': state_save_args\n",
    "#       #   # }},\n",
    "#       #   )\n",
    "\n",
    "#       state = state.replace(metrics=state.metrics.empty()) # reset train_metrics for next training epoch\n",
    "\n",
    "checkpoint_manager.wait_until_finished()\n",
    "print(f'Checkpointed epochs: {checkpoint_manager.all_steps()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('/tmp/flax_ckpt/orbax/managed')  # Because max_to_keep=2, only step 3 and 4 are retained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load latest saved checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = checkpoint_manager.latest_step()\n",
    "state_restored = checkpoint_manager.restore(epoch, items=ckpt)[\"model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def pred_step(state, batch):\n",
    "  inputs, targets, sample_weights = batch\n",
    "  logits, updates = state.apply_fn(\n",
    "    {'params': state.params, 'batch_stats': state.batch_stats},\n",
    "    x=inputs,\n",
    "    train=False,\n",
    "    mutable=['batch_stats']\n",
    "    )\n",
    "  return nn.sigmoid(logits)\n",
    "\n",
    "predictions = []\n",
    "for step, test_batch in enumerate(test_ds):\n",
    "  print(f\"Batch: {step}\")\n",
    "  # Convert to numpy\n",
    "  test_batch = [element.numpy()[..., 0] if element.shape[-1] == 1 else element.numpy() for element in test_batch]\n",
    "  pred = pred_step(state_restored, test_batch)\n",
    "  predictions.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Outputs and Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create prediction output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = \"pytorch_notebook\"\n",
    "network_name = \"api_jax_dataset\"\n",
    "output_name = \"example_jax_forecast\"\n",
    "output_folder = os.path.join(\".\", \"results\", \"predict\", output_name,\n",
    "                                \"{}.{}\".format(network_name, 42))\n",
    "os.makedirs(output_folder, exist_ok=output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert and output predictions to numpy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "for workers, prediction in enumerate(predictions):\n",
    "    for batch in range(prediction.shape[0]):\n",
    "        date = pd.Timestamp(test_dataset.dates[idx].replace('_', '-'))\n",
    "        output_path = os.path.join(output_folder, date.strftime(\"%Y_%m_%d.npy\"))\n",
    "        forecast = prediction[batch, :, :, :]\n",
    "        # # forecast_np = forecast.detach().cpu().numpy()\n",
    "        np.save(output_path, forecast)\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a csv file with all the test dates we have predicted for, and to use in generating the final netCDF output using `icenet_output`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!printf \"2020-04-01\\n2020-04-02\" | tee testdates.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!icenet_output -m -o results/predict example_jax_forecast notebook_api_jax_data testdates.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import datetime as dt\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icenet.plotting.video import xarray_to_video as xvid\n",
    "from icenet.data.sic.mask import Masks\n",
    "\n",
    "ds = xr.open_dataset(\"results/predict/example_jax_forecast.nc\")\n",
    "land_mask = Masks(south=True, north=False).get_land_mask()\n",
    "ds.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_date = ds.time.values[0]\n",
    "fc = ds.sic_mean.isel(time=0).drop_vars(\"time\").rename(dict(leadtime=\"time\"))\n",
    "fc['time'] = [pd.to_datetime(forecast_date) \\\n",
    "              + dt.timedelta(days=int(e)) for e in fc.time.values]\n",
    "\n",
    "anim = xvid(fc, 15, figsize=4, mask=land_mask)\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load original input dataset\n",
    "\n",
    "This is the original input dataset (pre-normalisation) for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original input dataset (domain not normalised)\n",
    "xr.plot.contourf(xr.open_dataset(\"data/osisaf/south/siconca/2020.nc\").isel(time=92).ice_conc, levels=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version\n",
    "- IceNet Codebase: v0.2.8_dev"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
