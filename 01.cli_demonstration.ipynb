{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IceNet CLI Usage\n",
    "\n",
    "## Context\n",
    "\n",
    "### Purpose\n",
    "The IceNet library provides the ability to download, process, train and predict from end to end via a set of command-line interfaces. \n",
    "\n",
    "This notebook illustrates the CLI utilities that are available natively from the library and in conjunction with helpers scripts from the pipeline repository, for testing and producing operational forecasts.\n",
    "\n",
    "### Modelling approach\n",
    "This modelling approach allows users to immediately utilise the library for producing sea ice concentraion forecasts.\n",
    "\n",
    "### Highlights\n",
    "The key features of an end to end run are: \n",
    "* [Setup](#Setup)\n",
    "* [Download](#Download) \n",
    "* [Process](#Process)\n",
    "* [Train](#Train)\n",
    "* [Predict](#Predict)\n",
    "\n",
    "### Contributions\n",
    "#### Notebook\n",
    "James Byrne (author)\n",
    "\n",
    "__Please raise issues [in this repository](https://github.com/icenet-ai/icenet-notebooks/issues) to suggest updates to this notebook!__ \n",
    "\n",
    "Contact me at _jambyr \\<at\\> bas.ac.uk_ for anything else...\n",
    "\n",
    "#### Modelling codebase\n",
    "James Byrne (code author), Tom Andersson (science author)\n",
    "\n",
    "#### Modelling publications\n",
    "Andersson, T.R., Hosking, J.S., Pérez-Ortiz, M. et al. Seasonal Arctic sea ice forecasting with probabilistic deep learning. Nat Commun 12, 5124 (2021). https://doi.org/10.1038/s41467-021-25257-4\n",
    "\n",
    "#### Involved organisations\n",
    "The Alan Turing Institute and British Antarctic Survey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "In order to undertake the following, I'm assuming you have a the following at your disposal:\n",
    "\n",
    "* A host to run this on\n",
    "* A working conda installation on that host\n",
    "* Either a slurm cluster to submit jobs to or run locally\n",
    "* Wherever you run, you want GPUs for training (predictions run fine without)\n",
    "* Git, python and shell knowledge to a basic degree :-)\n",
    "* There are numerous external facilities that we interface with, which it's assumed you're set up to use (otherwise check the options as they can be disabled/overlooked)\n",
    "  * Data sources under [Climate and Sea Ice Data](#Climate-and-Sea-Ice-Data)\n",
    "  * Wandb (Weights and Biases) - can be disabled when using `icenet_train`\n",
    "  * Azure - we demonstrate native uploading which can be skipped if required\n",
    "\n",
    "We'll assume that you're running in a local clone of icenet-notebooks for the sake of this tutorial. If you already have some data available (as we do in `../data` then remember that you can symlink to it using `ln -s ../data`).\n",
    "\n",
    "If you're starting right from scratch: \n",
    "\n",
    "```bash\n",
    "git clone https://www.github.com/icenet-ai/icenet-notebooks.git notebooks\n",
    "git clone https://www.github.com/icenet-ai/icenet-pipeline.git notebook-pipeline\n",
    "cd notebook-pipeline\n",
    "```\n",
    "\n",
    "### Environment Configuration\n",
    "\n",
    "The icenet conda environment will need installing. To do this, [please follow the instructions for installing environments in icenet-pipeline](https://www.github.com/icenet-ai/icenet-pipeline) and enable the environment with `conda activate icenet`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Commands\n",
    "\n",
    "Once the icenet library is installed, you'll be able to access all commands made available by the library. Some are utilities that won't be covered, but using `icenet_<TAB>`-complete you should be able to see a list that includes (but ___is not limited to___):\n",
    "\n",
    "* icenet_data_cmip\n",
    "* icenet_data_era5\n",
    "* icenet_data_hres\n",
    "* icenet_data_masks\n",
    "* icenet_data_sic\n",
    "* icenet_dataset_create\n",
    "* icenet_output\n",
    "* icenet_predict\n",
    "* icenet_process_cmip\n",
    "* icenet_process_era5\n",
    "* icenet_process_hres\n",
    "* icenet_process_metadata\n",
    "* icenet_process_sic\n",
    "* icenet_train\n",
    "* icenet_upload_azure\n",
    "\n",
    "All of these commands are either directly or indirectly (through pipeline shell scripts) used in this notebook...\n",
    "\n",
    "All commands accept options such as `-v` for turning on verbose logging and `-h` for obtaining help about what options they offer. ___As is best practice for all commands in *nix land, use `-h` to obtain information about options___.\n",
    "\n",
    "### The idea behind end to end runs\n",
    "\n",
    "The IceNet package is designed to support automated runs from end to end by exposing the above CLI operations. These are simple wrappers around the library itself, and __any__ step of this can be undertaken manually or programmatically by inspecting the relevant endpoints. \n",
    "\n",
    "___TL;DR: for those of you just wanting to skip straight to an end to end example in shell, [please look at the daily execution script](#Daily-execution)...___\n",
    "\n",
    "The end to end execution methodology is illustrated by this diagram:\n",
    "\n",
    "![Full IceNet operational workflow...](https://raw.githubusercontent.com/wiki/alan-turing-institute/IceNet-Project/Pipeline%20Layout.png)\n",
    "\n",
    "The portion of this you're really interested in understand is in the green box however, with the IceNet-Pipeline directory (e.g. `green`) corresponding to the green box and thus being, essentially, an ephemeral environment. \n",
    "\n",
    "#### A tip behind source data\n",
    "\n",
    "You'll see that `Source Data Store` is located outside the green box. Because of the expense and time required to interface with external sources, _we recommend the following step so that source data can be shared between environments_...\n",
    "\n",
    "```bash\n",
    "# Make a source data store outside our ephemeral environment\n",
    "# For the sake of brevity the rest of the notebooks use a fresh environment\n",
    "mkdir ../data\n",
    "ln -s ../data\n",
    "```\n",
    "\n",
    "### Pipeline versus CLI verses Library usage\n",
    "\n",
    "Though this notebook is tailored around use of the [IceNet-Pipeline repository](https://github.com/antarctica/IceNet-Pipeline) there is no dependency on this repository for using the `icenet_*` commands. The pipeline repository just offers helpers scripts written in [`bash`](https://tldp.org/LDP/abs/html) for running an end to end pipeline out of the box. \n",
    "\n",
    "You are welcome to use any arbitrary directory to run the CLI scripts below. However, when it comes to the sections on [training](#Train) and [prediction](#Predict), as well as [running daily predictions](#Daily execution), you'll notice that we leverage scripts from the pipeline repository. This is because these scripts interact with the [model ensembling tool from BAS](https://github.com/JimCircadian/model-ensembler) to train and predict across multiple models instances. \n",
    "\n",
    "The rule of thumb to follow: \n",
    "\n",
    "* Use the pipeline repository if you want to run the end to end IceNet processing out of the box.\n",
    "* Adapt or customise this process using `icenet_*` commands described in this notebook and in the scripts contained in the pipeline repo.\n",
    "* For ultimate customisation, you can interact with the IceNet repository programmatically (which is how the CLI commands operate.) For more information look at the [IceNet CLI implementations](https://github.com/JimCircadian/icenet2/blob/main/setup.py#L32) and the [library notebook](03.library_usage.ipynb), along with the [library documentation](#TODO). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "## Download\n",
    "\n",
    "### Mask data\n",
    "\n",
    "IceNet relies on some generated masks for training/prediction, which can be automatically generated very easily using `icenet_data_masks {north,south}`. Once performed, this does not need to be rerun under the pipeline directory..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23-11-22 09:51:22 :INFO    ] - Creating path: ./data/masks\n",
      "--2022-11-23 09:51:22--  ftp://osisaf.met.no/reprocessed/ice/conc/v2p0/2000/01/ice_conc_sh_ease2-250_cdr-v2p0_200001021200.nc\n",
      "           => ‘./data/masks/south/siconca/2000/01/.listing’\n",
      "Resolving osisaf.met.no (osisaf.met.no)... 157.249.75.10\n",
      "Connecting to osisaf.met.no (osisaf.met.no)|157.249.75.10|:21... connected.\n",
      "Logging in as anonymous ... Logged in!\n",
      "==> SYST ... done.    ==> PWD ... done.\n",
      "==> TYPE I ... done.  ==> CWD (1) /reprocessed/ice/conc/v2p0/2000/01 ... done.\n",
      "==> PASV ... done.    ==> LIST ... done.\n",
      "\n",
      "    [ <=>                                   ] 6,567       --.-K/s   in 0.03s   \n",
      "\n",
      "2022-11-23 09:51:23 (217 KB/s) - ‘./data/masks/south/siconca/2000/01/.listing’ saved [6567]\n",
      "\n",
      "--2022-11-23 09:51:23--  ftp://osisaf.met.no/reprocessed/ice/conc/v2p0/2000/01/ice_conc_sh_ease2-250_cdr-v2p0_200001021200.nc\n",
      "           => ‘./data/masks/south/siconca/2000/01/ice_conc_sh_ease2-250_cdr-v2p0_200001021200.nc’\n",
      "==> CWD not required.\n",
      "==> PASV ... done.    ==> RETR ice_conc_sh_ease2-250_cdr-v2p0_200001021200.nc ... done.\n",
      "Length: 9856141 (9.4M)\n",
      "\n",
      "100%[======================================>] 9,856,141   9.98MB/s   in 0.9s   \n",
      "\n",
      "2022-11-23 09:51:24 (9.98 MB/s) - ‘./data/masks/south/siconca/2000/01/ice_conc_sh_ease2-250_cdr-v2p0_200001021200.nc’ saved [9856141]\n",
      "\n",
      "FINISHED --2022-11-23 09:51:24--\n",
      "Total wall clock time: 2.0s\n",
      "Downloaded: 2 files, 9.4M in 1.0s (9.68 MB/s)\n",
      "[23-11-22 09:51:24 :INFO    ] - Child returned: 0\n",
      "[23-11-22 09:51:25 :INFO    ] - Saving ./data/masks/south/masks/active_grid_cell_mask_01.npy\n",
      "[23-11-22 09:51:25 :INFO    ] - Saving ./data/masks/south/masks/land_mask.npy\n",
      "--2022-11-23 09:51:25--  ftp://osisaf.met.no/reprocessed/ice/conc/v2p0/2000/02/ice_conc_sh_ease2-250_cdr-v2p0_200002021200.nc\n",
      "           => ‘./data/masks/south/siconca/2000/02/.listing’\n",
      "Resolving osisaf.met.no (osisaf.met.no)... 157.249.75.10\n",
      "Connecting to osisaf.met.no (osisaf.met.no)|157.249.75.10|:21... connected.\n",
      "Logging in as anonymous ... Logged in!\n",
      "==> SYST ... done.    ==> PWD ... done.\n",
      "==> TYPE I ... done.  ==> CWD (1) /reprocessed/ice/conc/v2p0/2000/02 ... done.\n",
      "==> PASV ... done.    ==> LIST ... done.\n",
      "\n",
      "    [ <=>                                   ] 6,151       --.-K/s   in 0.03s   \n",
      "\n",
      "2022-11-23 09:51:25 (238 KB/s) - ‘./data/masks/south/siconca/2000/02/.listing’ saved [6151]\n",
      "\n",
      "--2022-11-23 09:51:25--  ftp://osisaf.met.no/reprocessed/ice/conc/v2p0/2000/02/ice_conc_sh_ease2-250_cdr-v2p0_200002021200.nc\n",
      "           => ‘./data/masks/south/siconca/2000/02/ice_conc_sh_ease2-250_cdr-v2p0_200002021200.nc’\n",
      "==> CWD not required.\n",
      "==> PASV ... done.    ==> RETR ice_conc_sh_ease2-250_cdr-v2p0_200002021200.nc ... done.\n",
      "Length: 9856141 (9.4M)\n",
      "\n",
      "100%[======================================>] 9,856,141   7.73MB/s   in 1.2s   \n",
      "\n",
      "2022-11-23 09:51:27 (7.73 MB/s) - ‘./data/masks/south/siconca/2000/02/ice_conc_sh_ease2-250_cdr-v2p0_200002021200.nc’ saved [9856141]\n",
      "\n",
      "FINISHED --2022-11-23 09:51:27--\n",
      "Total wall clock time: 2.2s\n",
      "Downloaded: 2 files, 9.4M in 1.2s (7.58 MB/s)\n",
      "[23-11-22 09:51:27 :INFO    ] - Child returned: 0\n",
      "[23-11-22 09:51:27 :INFO    ] - Saving ./data/masks/south/masks/active_grid_cell_mask_02.npy\n",
      "--2022-11-23 09:51:27--  ftp://osisaf.met.no/reprocessed/ice/conc/v2p0/2000/03/ice_conc_sh_ease2-250_cdr-v2p0_200003021200.nc\n",
      "           => ‘./data/masks/south/siconca/2000/03/.listing’\n",
      "Resolving osisaf.met.no (osisaf.met.no)... 157.249.75.10\n",
      "Connecting to osisaf.met.no (osisaf.met.no)|157.249.75.10|:21... connected.\n",
      "Logging in as anonymous ... Logged in!\n",
      "==> SYST ... done.    ==> PWD ... done.\n",
      "==> TYPE I ... done.  ==> CWD (1) /reprocessed/ice/conc/v2p0/2000/03 ... done.\n",
      "==> PASV ... done.    ==> LIST ... done.\n",
      "\n",
      "    [ <=>                                   ] 6,567       --.-K/s   in 0.03s   \n",
      "\n",
      "2022-11-23 09:51:28 (253 KB/s) - ‘./data/masks/south/siconca/2000/03/.listing’ saved [6567]\n",
      "\n",
      "--2022-11-23 09:51:28--  ftp://osisaf.met.no/reprocessed/ice/conc/v2p0/2000/03/ice_conc_sh_ease2-250_cdr-v2p0_200003021200.nc\n",
      "           => ‘./data/masks/south/siconca/2000/03/ice_conc_sh_ease2-250_cdr-v2p0_200003021200.nc’\n",
      "==> CWD not required.\n",
      "==> PASV ... done.    ==> RETR ice_conc_sh_ease2-250_cdr-v2p0_200003021200.nc ... done.\n",
      "Length: 9856141 (9.4M)\n",
      "\n",
      "100%[======================================>] 9,856,141   3.83MB/s   in 2.5s   \n",
      "\n",
      "2022-11-23 09:51:30 (3.83 MB/s) - ‘./data/masks/south/siconca/2000/03/ice_conc_sh_ease2-250_cdr-v2p0_200003021200.nc’ saved [9856141]\n",
      "\n",
      "FINISHED --2022-11-23 09:51:30--\n",
      "Total wall clock time: 3.5s\n",
      "Downloaded: 2 files, 9.4M in 2.5s (3.80 MB/s)\n",
      "[23-11-22 09:51:30 :INFO    ] - Child returned: 0\n",
      "[23-11-22 09:51:31 :INFO    ] - Saving ./data/masks/south/masks/active_grid_cell_mask_03.npy\n",
      "--2022-11-23 09:51:31--  ftp://osisaf.met.no/reprocessed/ice/conc/v2p0/2000/04/ice_conc_sh_ease2-250_cdr-v2p0_200004021200.nc\n",
      "           => ‘./data/masks/south/siconca/2000/04/.listing’\n",
      "Resolving osisaf.met.no (osisaf.met.no)... 157.249.75.10\n",
      "Connecting to osisaf.met.no (osisaf.met.no)|157.249.75.10|:21... connected.\n",
      "Logging in as anonymous ... Logged in!\n",
      "==> SYST ... done.    ==> PWD ... done.\n",
      "==> TYPE I ... done.  ==> CWD (1) /reprocessed/ice/conc/v2p0/2000/04 ... done.\n",
      "==> PASV ... done.    ==> LIST ... done.\n",
      "\n",
      "    [ <=>                                   ] 6,359       --.-K/s   in 0.03s   \n",
      "\n",
      "2022-11-23 09:51:31 (192 KB/s) - ‘./data/masks/south/siconca/2000/04/.listing’ saved [6359]\n",
      "\n",
      "--2022-11-23 09:51:31--  ftp://osisaf.met.no/reprocessed/ice/conc/v2p0/2000/04/ice_conc_sh_ease2-250_cdr-v2p0_200004021200.nc\n",
      "           => ‘./data/masks/south/siconca/2000/04/ice_conc_sh_ease2-250_cdr-v2p0_200004021200.nc’\n",
      "==> CWD not required.\n",
      "==> PASV ... done.    ==> RETR ice_conc_sh_ease2-250_cdr-v2p0_200004021200.nc ... done.\n",
      "Length: 9856141 (9.4M)\n",
      "\n",
      "100%[======================================>] 9,856,141   7.72MB/s   in 1.2s   \n",
      "\n",
      "2022-11-23 09:51:33 (7.72 MB/s) - ‘./data/masks/south/siconca/2000/04/ice_conc_sh_ease2-250_cdr-v2p0_200004021200.nc’ saved [9856141]\n",
      "\n",
      "FINISHED --2022-11-23 09:51:33--\n",
      "Total wall clock time: 2.2s\n",
      "Downloaded: 2 files, 9.4M in 1.2s (7.52 MB/s)\n",
      "[23-11-22 09:51:33 :INFO    ] - Child returned: 0\n",
      "[23-11-22 09:51:33 :INFO    ] - Saving ./data/masks/south/masks/active_grid_cell_mask_04.npy\n",
      "--2022-11-23 09:51:33--  ftp://osisaf.met.no/reprocessed/ice/conc/v2p0/2000/05/ice_conc_sh_ease2-250_cdr-v2p0_200005021200.nc\n",
      "           => ‘./data/masks/south/siconca/2000/05/.listing’\n",
      "Resolving osisaf.met.no (osisaf.met.no)... 157.249.75.10\n",
      "Connecting to osisaf.met.no (osisaf.met.no)|157.249.75.10|:21... connected.\n",
      "Logging in as anonymous ... Logged in!\n",
      "==> SYST ... done.    ==> PWD ... done.\n",
      "==> TYPE I ... done.  ==> CWD (1) /reprocessed/ice/conc/v2p0/2000/05 ... done.\n",
      "==> PASV ... done.    ==> LIST ... done.\n",
      "\n",
      "    [ <=>                                   ] 6,567       --.-K/s   in 0.03s   \n",
      "\n",
      "2022-11-23 09:51:34 (226 KB/s) - ‘./data/masks/south/siconca/2000/05/.listing’ saved [6567]\n",
      "\n",
      "--2022-11-23 09:51:34--  ftp://osisaf.met.no/reprocessed/ice/conc/v2p0/2000/05/ice_conc_sh_ease2-250_cdr-v2p0_200005021200.nc\n",
      "           => ‘./data/masks/south/siconca/2000/05/ice_conc_sh_ease2-250_cdr-v2p0_200005021200.nc’\n",
      "==> CWD not required.\n",
      "==> PASV ... done.    ==> RETR ice_conc_sh_ease2-250_cdr-v2p0_200005021200.nc ... done.\n",
      "Length: 9856141 (9.4M)\n",
      "\n",
      "100%[======================================>] 9,856,141   8.20MB/s   in 1.1s   \n",
      "\n",
      "2022-11-23 09:51:35 (8.20 MB/s) - ‘./data/masks/south/siconca/2000/05/ice_conc_sh_ease2-250_cdr-v2p0_200005021200.nc’ saved [9856141]\n",
      "\n",
      "FINISHED --2022-11-23 09:51:35--\n",
      "Total wall clock time: 2.2s\n",
      "Downloaded: 2 files, 9.4M in 1.2s (8.00 MB/s)\n",
      "[23-11-22 09:51:35 :INFO    ] - Child returned: 0\n",
      "[23-11-22 09:51:35 :INFO    ] - Saving ./data/masks/south/masks/active_grid_cell_mask_05.npy\n",
      "--2022-11-23 09:51:35--  ftp://osisaf.met.no/reprocessed/ice/conc/v2p0/2000/06/ice_conc_sh_ease2-250_cdr-v2p0_200006021200.nc\n",
      "           => ‘./data/masks/south/siconca/2000/06/.listing’\n",
      "Resolving osisaf.met.no (osisaf.met.no)... 157.249.75.10\n",
      "Connecting to osisaf.met.no (osisaf.met.no)|157.249.75.10|:21... connected.\n",
      "Logging in as anonymous ... Logged in!\n",
      "==> SYST ... done.    ==> PWD ... done.\n",
      "==> TYPE I ... done.  ==> CWD (1) /reprocessed/ice/conc/v2p0/2000/06 ... done.\n",
      "==> PASV ... done.    ==> LIST ... done.\n",
      "\n",
      "    [ <=>                                   ] 6,359       --.-K/s   in 0.03s   \n",
      "\n",
      "2022-11-23 09:51:36 (240 KB/s) - ‘./data/masks/south/siconca/2000/06/.listing’ saved [6359]\n",
      "\n",
      "--2022-11-23 09:51:36--  ftp://osisaf.met.no/reprocessed/ice/conc/v2p0/2000/06/ice_conc_sh_ease2-250_cdr-v2p0_200006021200.nc\n",
      "           => ‘./data/masks/south/siconca/2000/06/ice_conc_sh_ease2-250_cdr-v2p0_200006021200.nc’\n",
      "==> CWD not required.\n",
      "==> PASV ... done.    ==> RETR ice_conc_sh_ease2-250_cdr-v2p0_200006021200.nc ... done.\n",
      "Length: 9856141 (9.4M)\n",
      "\n",
      "100%[======================================>] 9,856,141   8.28MB/s   in 1.1s   \n",
      "\n",
      "2022-11-23 09:51:37 (8.28 MB/s) - ‘./data/masks/south/siconca/2000/06/ice_conc_sh_ease2-250_cdr-v2p0_200006021200.nc’ saved [9856141]\n",
      "\n",
      "FINISHED --2022-11-23 09:51:37--\n",
      "Total wall clock time: 2.2s\n",
      "Downloaded: 2 files, 9.4M in 1.2s (8.10 MB/s)\n",
      "[23-11-22 09:51:37 :INFO    ] - Child returned: 0\n",
      "[23-11-22 09:51:37 :INFO    ] - Saving ./data/masks/south/masks/active_grid_cell_mask_06.npy\n",
      "--2022-11-23 09:51:37--  ftp://osisaf.met.no/reprocessed/ice/conc/v2p0/2000/07/ice_conc_sh_ease2-250_cdr-v2p0_200007021200.nc\n",
      "           => ‘./data/masks/south/siconca/2000/07/.listing’\n",
      "Resolving osisaf.met.no (osisaf.met.no)... 157.249.75.10\n",
      "Connecting to osisaf.met.no (osisaf.met.no)|157.249.75.10|:21... connected.\n",
      "Logging in as anonymous ... Logged in!\n",
      "==> SYST ... done.    ==> PWD ... done.\n",
      "==> TYPE I ... done.  ==> CWD (1) /reprocessed/ice/conc/v2p0/2000/07 ... done.\n",
      "==> PASV ... done.    ==> LIST ... done.\n",
      "\n",
      "    [ <=>                                   ] 6,567       --.-K/s   in 0.03s   \n",
      "\n",
      "2022-11-23 09:51:38 (233 KB/s) - ‘./data/masks/south/siconca/2000/07/.listing’ saved [6567]\n",
      "\n",
      "--2022-11-23 09:51:38--  ftp://osisaf.met.no/reprocessed/ice/conc/v2p0/2000/07/ice_conc_sh_ease2-250_cdr-v2p0_200007021200.nc\n",
      "           => ‘./data/masks/south/siconca/2000/07/ice_conc_sh_ease2-250_cdr-v2p0_200007021200.nc’\n",
      "==> CWD not required.\n",
      "==> PASV ... done.    ==> RETR ice_conc_sh_ease2-250_cdr-v2p0_200007021200.nc ... done.\n",
      "Length: 9856141 (9.4M)\n",
      "\n",
      "100%[======================================>] 9,856,141   7.68MB/s   in 1.2s   \n",
      "\n",
      "2022-11-23 09:51:40 (7.68 MB/s) - ‘./data/masks/south/siconca/2000/07/ice_conc_sh_ease2-250_cdr-v2p0_200007021200.nc’ saved [9856141]\n",
      "\n",
      "FINISHED --2022-11-23 09:51:40--\n",
      "Total wall clock time: 2.2s\n",
      "Downloaded: 2 files, 9.4M in 1.3s (7.52 MB/s)\n",
      "[23-11-22 09:51:40 :INFO    ] - Child returned: 0\n",
      "[23-11-22 09:51:40 :INFO    ] - Saving ./data/masks/south/masks/active_grid_cell_mask_07.npy\n",
      "--2022-11-23 09:51:40--  ftp://osisaf.met.no/reprocessed/ice/conc/v2p0/2000/08/ice_conc_sh_ease2-250_cdr-v2p0_200008021200.nc\n",
      "           => ‘./data/masks/south/siconca/2000/08/.listing’\n",
      "Resolving osisaf.met.no (osisaf.met.no)... 157.249.75.10\n",
      "Connecting to osisaf.met.no (osisaf.met.no)|157.249.75.10|:21... connected.\n",
      "Logging in as anonymous ... Logged in!\n",
      "==> SYST ... done.    ==> PWD ... done.\n",
      "==> TYPE I ... done.  ==> CWD (1) /reprocessed/ice/conc/v2p0/2000/08 ... done.\n",
      "==> PASV ... done.    ==> LIST ... done.\n",
      "\n",
      "    [ <=>                                   ] 6,567       --.-K/s   in 0.03s   \n",
      "\n",
      "2022-11-23 09:51:40 (235 KB/s) - ‘./data/masks/south/siconca/2000/08/.listing’ saved [6567]\n",
      "\n",
      "--2022-11-23 09:51:40--  ftp://osisaf.met.no/reprocessed/ice/conc/v2p0/2000/08/ice_conc_sh_ease2-250_cdr-v2p0_200008021200.nc\n",
      "           => ‘./data/masks/south/siconca/2000/08/ice_conc_sh_ease2-250_cdr-v2p0_200008021200.nc’\n",
      "==> CWD not required.\n",
      "==> PASV ... done.    ==> RETR ice_conc_sh_ease2-250_cdr-v2p0_200008021200.nc ... done.\n",
      "Length: 9856141 (9.4M)\n",
      "\n",
      "100%[======================================>] 9,856,141   7.84MB/s   in 1.2s   \n",
      "\n",
      "2022-11-23 09:51:42 (7.84 MB/s) - ‘./data/masks/south/siconca/2000/08/ice_conc_sh_ease2-250_cdr-v2p0_200008021200.nc’ saved [9856141]\n",
      "\n",
      "FINISHED --2022-11-23 09:51:42--\n",
      "Total wall clock time: 2.2s\n",
      "Downloaded: 2 files, 9.4M in 1.2s (7.67 MB/s)\n",
      "[23-11-22 09:51:42 :INFO    ] - Child returned: 0\n",
      "[23-11-22 09:51:42 :INFO    ] - Saving ./data/masks/south/masks/active_grid_cell_mask_08.npy\n",
      "--2022-11-23 09:51:42--  ftp://osisaf.met.no/reprocessed/ice/conc/v2p0/2000/09/ice_conc_sh_ease2-250_cdr-v2p0_200009021200.nc\n",
      "           => ‘./data/masks/south/siconca/2000/09/.listing’\n",
      "Resolving osisaf.met.no (osisaf.met.no)... 157.249.75.10\n",
      "Connecting to osisaf.met.no (osisaf.met.no)|157.249.75.10|:21... connected.\n",
      "Logging in as anonymous ... Logged in!\n",
      "==> SYST ... done.    ==> PWD ... done.\n",
      "==> TYPE I ... done.  ==> CWD (1) /reprocessed/ice/conc/v2p0/2000/09 ... done.\n",
      "==> PASV ... done.    ==> LIST ... done.\n",
      "\n",
      "    [ <=>                                   ] 6,359       --.-K/s   in 0.03s   \n",
      "\n",
      "2022-11-23 09:51:43 (221 KB/s) - ‘./data/masks/south/siconca/2000/09/.listing’ saved [6359]\n",
      "\n",
      "--2022-11-23 09:51:43--  ftp://osisaf.met.no/reprocessed/ice/conc/v2p0/2000/09/ice_conc_sh_ease2-250_cdr-v2p0_200009021200.nc\n",
      "           => ‘./data/masks/south/siconca/2000/09/ice_conc_sh_ease2-250_cdr-v2p0_200009021200.nc’\n",
      "==> CWD not required.\n",
      "==> PASV ... done.    ==> RETR ice_conc_sh_ease2-250_cdr-v2p0_200009021200.nc ... done.\n",
      "Length: 9856141 (9.4M)\n",
      "\n",
      "100%[======================================>] 9,856,141   10.5MB/s   in 0.9s   \n",
      "\n",
      "2022-11-23 09:51:44 (10.5 MB/s) - ‘./data/masks/south/siconca/2000/09/ice_conc_sh_ease2-250_cdr-v2p0_200009021200.nc’ saved [9856141]\n",
      "\n",
      "FINISHED --2022-11-23 09:51:44--\n",
      "Total wall clock time: 1.9s\n",
      "Downloaded: 2 files, 9.4M in 0.9s (10.1 MB/s)\n",
      "[23-11-22 09:51:44 :INFO    ] - Child returned: 0\n",
      "[23-11-22 09:51:44 :INFO    ] - Saving ./data/masks/south/masks/active_grid_cell_mask_09.npy\n",
      "--2022-11-23 09:51:44--  ftp://osisaf.met.no/reprocessed/ice/conc/v2p0/2000/10/ice_conc_sh_ease2-250_cdr-v2p0_200010021200.nc\n",
      "           => ‘./data/masks/south/siconca/2000/10/.listing’\n",
      "Resolving osisaf.met.no (osisaf.met.no)... 157.249.75.10\n",
      "Connecting to osisaf.met.no (osisaf.met.no)|157.249.75.10|:21... connected.\n",
      "Logging in as anonymous ... Logged in!\n",
      "==> SYST ... done.    ==> PWD ... done.\n",
      "==> TYPE I ... done.  ==> CWD (1) /reprocessed/ice/conc/v2p0/2000/10 ... done.\n",
      "==> PASV ... done.    ==> LIST ... done.\n",
      "\n",
      "    [ <=>                                   ] 6,567       --.-K/s   in 0.03s   \n",
      "\n",
      "2022-11-23 09:51:45 (222 KB/s) - ‘./data/masks/south/siconca/2000/10/.listing’ saved [6567]\n",
      "\n",
      "--2022-11-23 09:51:45--  ftp://osisaf.met.no/reprocessed/ice/conc/v2p0/2000/10/ice_conc_sh_ease2-250_cdr-v2p0_200010021200.nc\n",
      "           => ‘./data/masks/south/siconca/2000/10/ice_conc_sh_ease2-250_cdr-v2p0_200010021200.nc’\n",
      "==> CWD not required.\n",
      "==> PASV ... done.    ==> RETR ice_conc_sh_ease2-250_cdr-v2p0_200010021200.nc ... done.\n",
      "Length: 9856141 (9.4M)\n",
      "\n",
      "100%[======================================>] 9,856,141   7.75MB/s   in 1.2s   \n",
      "\n",
      "2022-11-23 09:51:46 (7.75 MB/s) - ‘./data/masks/south/siconca/2000/10/ice_conc_sh_ease2-250_cdr-v2p0_200010021200.nc’ saved [9856141]\n",
      "\n",
      "FINISHED --2022-11-23 09:51:46--\n",
      "Total wall clock time: 2.2s\n",
      "Downloaded: 2 files, 9.4M in 1.2s (7.57 MB/s)\n",
      "[23-11-22 09:51:46 :INFO    ] - Child returned: 0\n",
      "[23-11-22 09:51:46 :INFO    ] - Saving ./data/masks/south/masks/active_grid_cell_mask_10.npy\n",
      "--2022-11-23 09:51:46--  ftp://osisaf.met.no/reprocessed/ice/conc/v2p0/2000/11/ice_conc_sh_ease2-250_cdr-v2p0_200011021200.nc\n",
      "           => ‘./data/masks/south/siconca/2000/11/.listing’\n",
      "Resolving osisaf.met.no (osisaf.met.no)... 157.249.75.10\n",
      "Connecting to osisaf.met.no (osisaf.met.no)|157.249.75.10|:21... connected.\n",
      "Logging in as anonymous ... Logged in!\n",
      "==> SYST ... done.    ==> PWD ... done.\n",
      "==> TYPE I ... done.  ==> CWD (1) /reprocessed/ice/conc/v2p0/2000/11 ... done.\n",
      "==> PASV ... done.    ==> LIST ... done.\n",
      "\n",
      "    [ <=>                                   ] 6,359       --.-K/s   in 0.03s   \n",
      "\n",
      "2022-11-23 09:51:47 (200 KB/s) - ‘./data/masks/south/siconca/2000/11/.listing’ saved [6359]\n",
      "\n",
      "--2022-11-23 09:51:47--  ftp://osisaf.met.no/reprocessed/ice/conc/v2p0/2000/11/ice_conc_sh_ease2-250_cdr-v2p0_200011021200.nc\n",
      "           => ‘./data/masks/south/siconca/2000/11/ice_conc_sh_ease2-250_cdr-v2p0_200011021200.nc’\n",
      "==> CWD not required.\n",
      "==> PASV ... done.    ==> RETR ice_conc_sh_ease2-250_cdr-v2p0_200011021200.nc ... done.\n",
      "Length: 9856141 (9.4M)\n",
      "\n",
      "100%[======================================>] 9,856,141   7.58MB/s   in 1.2s   \n",
      "\n",
      "2022-11-23 09:51:48 (7.58 MB/s) - ‘./data/masks/south/siconca/2000/11/ice_conc_sh_ease2-250_cdr-v2p0_200011021200.nc’ saved [9856141]\n",
      "\n",
      "FINISHED --2022-11-23 09:51:48--\n",
      "Total wall clock time: 2.2s\n",
      "Downloaded: 2 files, 9.4M in 1.3s (7.40 MB/s)\n",
      "[23-11-22 09:51:48 :INFO    ] - Child returned: 0\n",
      "[23-11-22 09:51:48 :INFO    ] - Saving ./data/masks/south/masks/active_grid_cell_mask_11.npy\n",
      "--2022-11-23 09:51:48--  ftp://osisaf.met.no/reprocessed/ice/conc/v2p0/2000/12/ice_conc_sh_ease2-250_cdr-v2p0_200012021200.nc\n",
      "           => ‘./data/masks/south/siconca/2000/12/.listing’\n",
      "Resolving osisaf.met.no (osisaf.met.no)... 157.249.75.10\n",
      "Connecting to osisaf.met.no (osisaf.met.no)|157.249.75.10|:21... connected.\n",
      "Logging in as anonymous ... Logged in!\n",
      "==> SYST ... done.    ==> PWD ... done.\n",
      "==> TYPE I ... done.  ==> CWD (1) /reprocessed/ice/conc/v2p0/2000/12 ... done.\n",
      "==> PASV ... done.    ==> LIST ... done.\n",
      "\n",
      "    [ <=>                                   ] 6,359       --.-K/s   in 0.03s   \n",
      "\n",
      "2022-11-23 09:51:49 (211 KB/s) - ‘./data/masks/south/siconca/2000/12/.listing’ saved [6359]\n",
      "\n",
      "--2022-11-23 09:51:49--  ftp://osisaf.met.no/reprocessed/ice/conc/v2p0/2000/12/ice_conc_sh_ease2-250_cdr-v2p0_200012021200.nc\n",
      "           => ‘./data/masks/south/siconca/2000/12/ice_conc_sh_ease2-250_cdr-v2p0_200012021200.nc’\n",
      "==> CWD not required.\n",
      "==> PASV ... done.    ==> RETR ice_conc_sh_ease2-250_cdr-v2p0_200012021200.nc ... done.\n",
      "Length: 9856141 (9.4M)\n",
      "\n",
      "100%[======================================>] 9,856,141   10.4MB/s   in 0.9s   \n",
      "\n",
      "2022-11-23 09:51:50 (10.4 MB/s) - ‘./data/masks/south/siconca/2000/12/ice_conc_sh_ease2-250_cdr-v2p0_200012021200.nc’ saved [9856141]\n",
      "\n",
      "FINISHED --2022-11-23 09:51:50--\n",
      "Total wall clock time: 1.9s\n",
      "Downloaded: 2 files, 9.4M in 0.9s (10.1 MB/s)\n",
      "[23-11-22 09:51:50 :INFO    ] - Child returned: 0\n",
      "[23-11-22 09:51:50 :INFO    ] - Saving ./data/masks/south/masks/active_grid_cell_mask_12.npy\n"
     ]
    }
   ],
   "source": [
    "!icenet_data_masks south"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "### Climate and Sea Ice Data\n",
    "\n",
    "Obtaining and preparing data is simply achieved using `icenet_data_*` commands (you need to __configure the [CDS API](https://cds.climate.copernicus.eu/) token yourself__), which share common arguments `hemisphere`, `start_date` and `end_date`. There are also implementation specific options worth reviewing under `--help`. We specify the variables and levels via these commands.\n",
    "\n",
    "_Please ignore \"NOT IMPLEMENTED YET\", this is indicative of the commands not checking before overwriting files._\n",
    "\n",
    "__The `-d` flag prevents the downloaded data from being downloaded each time.__\n",
    "\n",
    "___Even small data ranges like this can take a while to retrieve (each variable in this case, for four months, is 3GB.) Please refer to [CDS requests page](https://cds.climate.copernicus.eu/cdsapp#!/yourrequests) to monitor ERA5 downloads...___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23-11-22 12:42:08 :INFO    ] - ERA5 Data Downloading\n",
      "[23-11-22 12:42:08 :INFO    ] - Building request(s), downloading and daily averaging from ERA5 API\n",
      "[23-11-22 12:42:08 :INFO    ] - Processing single download for uas @ None with 121 dates\n",
      "[23-11-22 12:42:08 :INFO    ] - Processing single download for vas @ None with 121 dates\n",
      "[23-11-22 12:42:08 :WARNING ] - NOT IMPLEMENTED YET, WE'LL JUST DOWNLOAD ANYWAY\n",
      "[23-11-22 12:42:08 :INFO    ] - Processing single download for tas @ None with 121 dates\n",
      "[23-11-22 12:42:08 :WARNING ] - NOT IMPLEMENTED YET, WE'LL JUST DOWNLOAD ANYWAY\n",
      "[23-11-22 12:42:08 :INFO    ] - Processing single download for zg @ 500 with 121 dates\n",
      "[23-11-22 12:42:08 :INFO    ] - Skipping actual download\n",
      "[23-11-22 12:42:08 :INFO    ] - Processing single download for zg @ 250 with 121 dates\n",
      "[23-11-22 12:42:08 :WARNING ] - NOT IMPLEMENTED YET, WE'LL JUST DOWNLOAD ANYWAY\n",
      "[23-11-22 12:42:08 :INFO    ] - Skipping actual download\n",
      "[23-11-22 12:42:08 :WARNING ] - NOT IMPLEMENTED YET, WE'LL JUST DOWNLOAD ANYWAY\n",
      "[23-11-22 12:42:08 :INFO    ] - Postprocessing CDS API data at ./data/era5/south/uas/latlon_2020.nc\n",
      "[23-11-22 12:42:08 :WARNING ] - NOT IMPLEMENTED YET, WE'LL JUST DOWNLOAD ANYWAY\n",
      "[23-11-22 12:42:08 :INFO    ] - Skipping actual download\n",
      "[23-11-22 12:42:08 :INFO    ] - Postprocessing CDS API data at ./data/era5/south/vas/latlon_2020.nc\n",
      "[23-11-22 12:42:08 :INFO    ] - Skipping actual download\n",
      "[23-11-22 12:42:08 :INFO    ] - Postprocessing CDS API data at ./data/era5/south/tas/latlon_2020.nc\n",
      "[23-11-22 12:42:08 :INFO    ] - Skipping actual download\n",
      "[23-11-22 12:42:08 :INFO    ] - Postprocessing CDS API data at ./data/era5/south/zg500/latlon_2020.nc\n",
      "[23-11-22 12:42:08 :INFO    ] - Postprocessing CDS API data at ./data/era5/south/zg250/latlon_2020.nc\n",
      "[23-11-22 12:42:27 :INFO    ] - 5 daily files downloaded\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/icenet/data/interfaces/downloader.py:241: FutureWarning: Ignoring a datum in netCDF load for consistency with existing behaviour. In a future version of Iris, this datum will be applied. To apply the datum when loading, use the iris.FUTURE.datum_support flag.\n",
      "  iris.load_cube(sic_day_path, 'sea_ice_area_fraction')\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/icenet/data/interfaces/downloader.py:241: FutureWarning: Ignoring a datum in netCDF load for consistency with existing behaviour. In a future version of Iris, this datum will be applied. To apply the datum when loading, use the iris.FUTURE.datum_support flag.\n",
      "  iris.load_cube(sic_day_path, 'sea_ice_area_fraction')\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/icenet/data/interfaces/downloader.py:241: FutureWarning: Ignoring a datum in netCDF load for consistency with existing behaviour. In a future version of Iris, this datum will be applied. To apply the datum when loading, use the iris.FUTURE.datum_support flag.\n",
      "  iris.load_cube(sic_day_path, 'sea_ice_area_fraction')\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/icenet/data/interfaces/downloader.py:241: FutureWarning: Ignoring a datum in netCDF load for consistency with existing behaviour. In a future version of Iris, this datum will be applied. To apply the datum when loading, use the iris.FUTURE.datum_support flag.\n",
      "  iris.load_cube(sic_day_path, 'sea_ice_area_fraction')\n",
      "[23-11-22 12:42:28 :INFO    ] - Saving regridded data to ./data/era5/south/uas/2020.nc... \n",
      "[23-11-22 12:42:30 :INFO    ] - Saving regridded data to ./data/era5/south/zg500/2020.nc... \n",
      "[23-11-22 12:42:32 :INFO    ] - Saving regridded data to ./data/era5/south/vas/2020.nc... \n",
      "[23-11-22 12:42:33 :INFO    ] - Saving regridded data to ./data/era5/south/tas/2020.nc... \n",
      "[23-11-22 12:42:35 :INFO    ] - Saving regridded data to ./data/era5/south/zg250/2020.nc... \n",
      "[23-11-22 12:42:37 :INFO    ] - Rotating wind data in ./data/era5/south/uas ./data/era5/south/vas\n",
      "[23-11-22 12:42:37 :INFO    ] - 1 files for uas\n",
      "[23-11-22 12:42:37 :INFO    ] - 1 files for vas\n",
      "[23-11-22 12:42:37 :INFO    ] - Rotating ./data/era5/south/uas/2020.nc and ./data/era5/south/vas/2020.nc\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/icenet/data/interfaces/downloader.py:409: FutureWarning: Ignoring a datum in netCDF load for consistency with existing behaviour. In a future version of Iris, this datum will be applied. To apply the datum when loading, use the iris.FUTURE.datum_support flag.\n",
      "  wind_cubes[apply_to[0]] = iris.load_cube(wind_file_0)\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/icenet/data/interfaces/downloader.py:410: FutureWarning: Ignoring a datum in netCDF load for consistency with existing behaviour. In a future version of Iris, this datum will be applied. To apply the datum when loading, use the iris.FUTURE.datum_support flag.\n",
      "  wind_cubes[apply_to[1]] = iris.load_cube(wind_file_1)\n"
     ]
    }
   ],
   "source": [
    "!icenet_data_era5 south -d --vars uas,vas,tas,zg --levels ',,,500|250' 2020-1-1 2020-4-30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23-11-22 12:44:45 :INFO    ] - OSASIF-SIC Data Downloading\n",
      "[23-11-22 12:44:45 :INFO    ] - Creating path: ./data/osisaf\n",
      "[23-11-22 12:44:45 :INFO    ] - Downloading SIC datafiles to .temp intermediates...\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/icenet/data/sic/osisaf.py:269: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n",
      "  if el in self._invalid_dates:\n",
      "[23-11-22 12:44:45 :INFO    ] - FTP opening\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "[23-11-22 12:48:34 :INFO    ] - Saving ./data/osisaf/south/siconca/2020.nc\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/flox/aggregations.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n",
      "[23-11-22 12:48:39 :INFO    ] - Opening for interpolation: ['./data/osisaf/south/siconca/2020.nc']\n",
      "[23-11-22 12:48:39 :INFO    ] - Processing 0 missing dates\n"
     ]
    }
   ],
   "source": [
    "!icenet_data_sic south -d 2020-1-1 2020-4-30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "By default, the IceNet commands regrid and rotates data as required to align with the OSISAF SIC data, which is used as the output for the dataset. Programmatic usage allows you to avoid this ([see notebook 03](0.3.library_usage)).\n",
    "\n",
    "At time of writing there are the following downloaders: \n",
    "\n",
    "* `icenet_data_era5` - downloads [ERA5 reanalysis](https://cds.climate.copernicus.eu/cdsapp#!/search?type=dataset&keywords=((%20%22Product%20type:%20Reanalysis%22%20) data using either the CDS Toolbox or direct API\n",
    "* `icenet_data_cmip` - downloads the prescribed experiments from [CMIP6](https://esgf-node.llnl.gov/search/cmip6/) for the original IceNet paper runs\n",
    "* `icenet_data_hres` - downloads up to date [forecast generated data from the ECMWF MARS API](https://www.ecmwf.int/en/forecasts/datasets/catalogue-ecmwf-real-time-products)\n",
    "* `icenet_data_sic` - downloads [OSISAF sea-ice concentration (SIC) data](https://osisaf-hl.met.no/v2p1-sea-ice-index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "## Process\n",
    "\n",
    "Processing takes the data made available through the source data store and undertakes the necessary normalisation for use as input channels to the UNet architecture. This intermediary step means that the original source data can be reused numerous times with varying training, validation and test date setups.\n",
    "\n",
    "### Command example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-23 22:27:14.041852: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-23 22:27:17.786698: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-23 22:27:17.786856: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-23 22:27:17.786882: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "[23-11-22 22:27:22 :INFO    ] - Got 91 dates for train\n",
      "[23-11-22 22:27:22 :INFO    ] - Got 21 dates for val\n",
      "[23-11-22 22:27:22 :INFO    ] - Got 2 dates for test\n",
      "[23-11-22 22:27:22 :INFO    ] - Creating path: ./processed/notebook_data/era5\n",
      "[23-11-22 22:27:23 :INFO    ] - Processing 91 dates for train category\n",
      "[23-11-22 22:27:23 :INFO    ] - Including lag of 1 days\n",
      "[23-11-22 22:27:23 :INFO    ] - Including lead of 93 days\n",
      "[23-11-22 22:27:23 :INFO    ] - No data found for 2019-12-31, outside data boundary perhaps?\n",
      "[23-11-22 22:27:23 :INFO    ] - Processing 21 dates for val category\n",
      "[23-11-22 22:27:23 :INFO    ] - Including lag of 1 days\n",
      "[23-11-22 22:27:23 :INFO    ] - Including lead of 93 days\n",
      "[23-11-22 22:27:23 :INFO    ] - Processing 2 dates for test category\n",
      "[23-11-22 22:27:23 :INFO    ] - Including lag of 1 days\n",
      "[23-11-22 22:27:23 :INFO    ] - Including lead of 93 days\n",
      "[23-11-22 22:27:23 :INFO    ] - Got 1 files for tas\n",
      "[23-11-22 22:27:23 :INFO    ] - Got 1 files for uas\n",
      "[23-11-22 22:27:23 :INFO    ] - Got 1 files for vas\n",
      "[23-11-22 22:27:23 :INFO    ] - Got 1 files for zg250\n",
      "[23-11-22 22:27:23 :INFO    ] - Got 1 files for zg500\n",
      "[23-11-22 22:27:23 :INFO    ] - Opening files for uas\n",
      "[23-11-22 22:27:25 :INFO    ] - Filtered to 121 units long based on configuration requirements\n",
      "[23-11-22 22:27:26 :INFO    ] - Normalising uas\n",
      "[23-11-22 22:27:27 :INFO    ] - Opening files for vas\n",
      "[23-11-22 22:27:27 :INFO    ] - Filtered to 121 units long based on configuration requirements\n",
      "[23-11-22 22:27:27 :INFO    ] - Normalising vas\n",
      "[23-11-22 22:27:28 :INFO    ] - Opening files for tas\n",
      "[23-11-22 22:27:28 :INFO    ] - Filtered to 121 units long based on configuration requirements\n",
      "[23-11-22 22:27:28 :INFO    ] - Generating climatology ./processed/notebook_data/era5/south/params/climatology.tas\n",
      "[23-11-22 22:27:32 :WARNING ] - We don't have a full climatology (1,2,3) compared with data (1,2,3,4)\n",
      "[23-11-22 22:27:34 :INFO    ] - Normalising tas\n",
      "[23-11-22 22:27:34 :INFO    ] - Opening files for zg500\n",
      "[23-11-22 22:27:34 :INFO    ] - Filtered to 121 units long based on configuration requirements\n",
      "[23-11-22 22:27:34 :INFO    ] - Generating climatology ./processed/notebook_data/era5/south/params/climatology.zg500\n",
      "[23-11-22 22:27:36 :WARNING ] - We don't have a full climatology (1,2,3) compared with data (1,2,3,4)\n",
      "[23-11-22 22:27:37 :INFO    ] - Normalising zg500\n",
      "[23-11-22 22:27:38 :INFO    ] - Opening files for zg250\n",
      "[23-11-22 22:27:38 :INFO    ] - Filtered to 121 units long based on configuration requirements\n",
      "[23-11-22 22:27:38 :INFO    ] - Generating climatology ./processed/notebook_data/era5/south/params/climatology.zg250\n",
      "[23-11-22 22:27:39 :WARNING ] - We don't have a full climatology (1,2,3) compared with data (1,2,3,4)\n",
      "[23-11-22 22:27:40 :INFO    ] - Normalising zg250\n",
      "[23-11-22 22:27:41 :INFO    ] - Writing configuration to ./loader.notebook_data.json\n",
      "2022-11-23 22:27:42.411920: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-23 22:27:43.283065: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-23 22:27:43.283148: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-23 22:27:43.283163: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "[23-11-22 22:27:45 :INFO    ] - Got 91 dates for train\n",
      "[23-11-22 22:27:45 :INFO    ] - Got 20 dates for val\n",
      "[23-11-22 22:27:45 :INFO    ] - Got 2 dates for test\n",
      "[23-11-22 22:27:45 :INFO    ] - Creating path: ./processed/notebook_data/osisaf\n",
      "[23-11-22 22:27:45 :INFO    ] - Processing 91 dates for train category\n",
      "[23-11-22 22:27:45 :INFO    ] - Including lag of 1 days\n",
      "[23-11-22 22:27:45 :INFO    ] - Including lead of 93 days\n",
      "[23-11-22 22:27:45 :INFO    ] - No data found for 2019-12-31, outside data boundary perhaps?\n",
      "[23-11-22 22:27:45 :INFO    ] - Processing 20 dates for val category\n",
      "[23-11-22 22:27:45 :INFO    ] - Including lag of 1 days\n",
      "[23-11-22 22:27:45 :INFO    ] - Including lead of 93 days\n",
      "[23-11-22 22:27:45 :INFO    ] - Processing 2 dates for test category\n",
      "[23-11-22 22:27:45 :INFO    ] - Including lag of 1 days\n",
      "[23-11-22 22:27:45 :INFO    ] - Including lead of 93 days\n",
      "[23-11-22 22:27:45 :INFO    ] - Got 1 files for siconca\n",
      "[23-11-22 22:27:45 :INFO    ] - Opening files for siconca\n",
      "[23-11-22 22:27:46 :INFO    ] - Filtered to 121 units long based on configuration requirements\n",
      "[23-11-22 22:27:47 :INFO    ] - No normalisation for siconca\n",
      "[23-11-22 22:27:47 :INFO    ] - Loading configuration ./loader.notebook_data.json\n",
      "[23-11-22 22:27:47 :INFO    ] - Writing configuration to ./loader.notebook_data.json\n",
      "2022-11-23 22:27:49.056475: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-23 22:27:49.796305: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-23 22:27:49.796383: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-23 22:27:49.796396: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "[23-11-22 22:27:50 :INFO    ] - Creating path: ./processed/notebook_data/meta\n",
      "[23-11-22 22:27:50 :INFO    ] - Loading configuration ./loader.notebook_data.json\n",
      "[23-11-22 22:27:50 :INFO    ] - Writing configuration to ./loader.notebook_data.json\n"
     ]
    }
   ],
   "source": [
    "!icenet_process_era5 notebook_data south \\\n",
    "    -ns 2020-1-1 -ne 2020-3-31 -vs 2020-4-3 -ve 2020-4-23 -ts 2020-4-1 -te 2020-4-2 \\\n",
    "    -l 1 --abs uas,vas --anom tas,zg500,zg250\n",
    "\n",
    "!icenet_process_sic notebook_data south \\\n",
    "    -ns 2020-1-1 -ne 2020-3-31 -vs 2020-4-1 -ve 2020-4-20 -ts 2020-4-1 -te 2020-4-2 \\\n",
    "    -l 1 --abs siconca\n",
    "\n",
    "!icenet_process_metadata notebook_data south"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "Consulting the command options will make the above more obvious (as well as further options) but a few things we can note that are helpful: \n",
    "\n",
    "* Options `-ns`, `-ne`, `-vs`, `-ve`, `-ts`, `-te`, which correspond to training, validation and test sets, allow ranges to be comma-delimited. The above example produces a split training set, for example, that spans two periods: 2000-2009 and 2011-2019.\n",
    "* These date ranges can be randomised and subsampled using `-d`, __though this is still a bit experimental__\n",
    "* The `-l` option (which is for `--lag`) specified the number of days back we look at input data variables for the output in question.\n",
    "\n",
    "There are plenty of other options available for preprocessing the data, but it should be noted that whilst this is not strongly coupled to dataset creation, options like the lag specified here might influence the creation of datasets in the next step. \n",
    "\n",
    "These commands, especially with decadal ranges, can take a long time (12+ hours) to complete depending on the hosts/storage in use.\n",
    "\n",
    "### Dataset creation\n",
    "\n",
    "Once the above preprocessing is taken care of datasets can easily be created thus. This operation _creates a cached dataset_ in the filesystem that can be fed in for training runs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-23 22:30:42.467744: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-23 22:30:43.391726: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-23 22:30:43.391856: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-23 22:30:43.391878: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "[23-11-22 22:30:44 :INFO    ] - Got 0 dates for train\n",
      "[23-11-22 22:30:44 :INFO    ] - Got 0 dates for val\n",
      "[23-11-22 22:30:44 :INFO    ] - Got 0 dates for test\n",
      "[23-11-22 22:30:44 :INFO    ] - Creating path: ./network_datasets/notebook_data\n",
      "[23-11-22 22:30:44 :INFO    ] - Loading configuration loader.notebook_data.json\n",
      "2022-11-23 22:30:48.112196: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-23 22:30:48.113281: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-23 22:30:48.189345: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-23 22:30:48.261104: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-23 22:30:48.854244: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-23 22:30:48.854323: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-23 22:30:48.854337: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2022-11-23 22:30:48.922686: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-23 22:30:48.922776: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-23 22:30:48.922792: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2022-11-23 22:30:49.002793: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-23 22:30:49.002880: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-23 22:30:49.002896: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2022-11-23 22:30:49.011911: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-23 22:30:49.011996: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-23 22:30:49.012010: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "[23-11-22 22:30:50 :INFO    ] - Dashboard at localhost:8888\n",
      "[23-11-22 22:30:50 :INFO    ] - Using dask client <Client: 'tcp://127.0.0.1:40363' processes=4 threads=4, memory=503.15 GiB>\n",
      "[23-11-22 22:30:50 :INFO    ] - 91 train dates to process, generating cache data.\n",
      "[23-11-22 22:31:02 :INFO    ] - Finished output ./network_datasets/notebook_data/south/train/00000000.tfrecord\n",
      "[23-11-22 22:31:02 :INFO    ] - Finished output ./network_datasets/notebook_data/south/train/00000001.tfrecord\n",
      "[23-11-22 22:31:02 :INFO    ] - Finished output ./network_datasets/notebook_data/south/train/00000002.tfrecord\n",
      "[23-11-22 22:31:02 :INFO    ] - Finished output ./network_datasets/notebook_data/south/train/00000003.tfrecord\n",
      "[23-11-22 22:31:02 :INFO    ] - Finished output ./network_datasets/notebook_data/south/train/00000004.tfrecord\n",
      "[23-11-22 22:31:02 :INFO    ] - Finished output ./network_datasets/notebook_data/south/train/00000005.tfrecord\n",
      "[23-11-22 22:31:02 :INFO    ] - Finished output ./network_datasets/notebook_data/south/train/00000006.tfrecord\n",
      "[23-11-22 22:31:02 :INFO    ] - Finished output ./network_datasets/notebook_data/south/train/00000007.tfrecord\n",
      "[23-11-22 22:31:13 :INFO    ] - Finished output ./network_datasets/notebook_data/south/train/00000008.tfrecord\n",
      "[23-11-22 22:31:13 :INFO    ] - Finished output ./network_datasets/notebook_data/south/train/00000009.tfrecord\n",
      "[23-11-22 22:31:13 :INFO    ] - Finished output ./network_datasets/notebook_data/south/train/00000010.tfrecord\n",
      "[23-11-22 22:31:13 :INFO    ] - Finished output ./network_datasets/notebook_data/south/train/00000011.tfrecord\n",
      "[23-11-22 22:31:13 :INFO    ] - Finished output ./network_datasets/notebook_data/south/train/00000012.tfrecord\n",
      "[23-11-22 22:31:13 :INFO    ] - Finished output ./network_datasets/notebook_data/south/train/00000013.tfrecord\n",
      "[23-11-22 22:31:13 :INFO    ] - Finished output ./network_datasets/notebook_data/south/train/00000014.tfrecord\n",
      "[23-11-22 22:31:13 :INFO    ] - Finished output ./network_datasets/notebook_data/south/train/00000015.tfrecord\n",
      "[23-11-22 22:31:24 :INFO    ] - Finished output ./network_datasets/notebook_data/south/train/00000016.tfrecord\n",
      "[23-11-22 22:31:24 :INFO    ] - Finished output ./network_datasets/notebook_data/south/train/00000017.tfrecord\n",
      "[23-11-22 22:31:24 :INFO    ] - Finished output ./network_datasets/notebook_data/south/train/00000018.tfrecord\n",
      "[23-11-22 22:31:24 :INFO    ] - Finished output ./network_datasets/notebook_data/south/train/00000019.tfrecord\n",
      "[23-11-22 22:31:24 :INFO    ] - Finished output ./network_datasets/notebook_data/south/train/00000020.tfrecord\n",
      "[23-11-22 22:31:24 :INFO    ] - Finished output ./network_datasets/notebook_data/south/train/00000021.tfrecord\n",
      "[23-11-22 22:31:24 :INFO    ] - Finished output ./network_datasets/notebook_data/south/train/00000022.tfrecord\n",
      "[23-11-22 22:31:24 :INFO    ] - Finished output ./network_datasets/notebook_data/south/train/00000023.tfrecord\n",
      "[23-11-22 22:31:35 :INFO    ] - Finished output ./network_datasets/notebook_data/south/train/00000024.tfrecord\n",
      "[23-11-22 22:31:35 :INFO    ] - Finished output ./network_datasets/notebook_data/south/train/00000025.tfrecord\n",
      "[23-11-22 22:31:35 :INFO    ] - Finished output ./network_datasets/notebook_data/south/train/00000026.tfrecord\n",
      "[23-11-22 22:31:35 :INFO    ] - Finished output ./network_datasets/notebook_data/south/train/00000027.tfrecord\n",
      "[23-11-22 22:31:35 :INFO    ] - Finished output ./network_datasets/notebook_data/south/train/00000028.tfrecord\n",
      "[23-11-22 22:31:35 :INFO    ] - Finished output ./network_datasets/notebook_data/south/train/00000029.tfrecord\n",
      "[23-11-22 22:31:35 :INFO    ] - Finished output ./network_datasets/notebook_data/south/train/00000030.tfrecord\n",
      "[23-11-22 22:31:35 :INFO    ] - Finished output ./network_datasets/notebook_data/south/train/00000031.tfrecord\n",
      "[23-11-22 22:31:44 :INFO    ] - Finished output ./network_datasets/notebook_data/south/train/00000032.tfrecord\n",
      "[23-11-22 22:31:44 :INFO    ] - Finished output ./network_datasets/notebook_data/south/train/00000033.tfrecord\n",
      "[23-11-22 22:31:44 :INFO    ] - Finished output ./network_datasets/notebook_data/south/train/00000034.tfrecord\n",
      "[23-11-22 22:31:44 :INFO    ] - Finished output ./network_datasets/notebook_data/south/train/00000035.tfrecord\n",
      "[23-11-22 22:31:44 :INFO    ] - Finished output ./network_datasets/notebook_data/south/train/00000036.tfrecord\n",
      "[23-11-22 22:31:44 :INFO    ] - Finished output ./network_datasets/notebook_data/south/train/00000037.tfrecord\n",
      "[23-11-22 22:31:44 :INFO    ] - Finished output ./network_datasets/notebook_data/south/train/00000038.tfrecord\n",
      "[23-11-22 22:31:44 :INFO    ] - Finished output ./network_datasets/notebook_data/south/train/00000039.tfrecord\n",
      "[23-11-22 22:31:52 :INFO    ] - Finished output ./network_datasets/notebook_data/south/train/00000040.tfrecord\n",
      "[23-11-22 22:31:52 :INFO    ] - Finished output ./network_datasets/notebook_data/south/train/00000041.tfrecord\n",
      "[23-11-22 22:31:52 :INFO    ] - Finished output ./network_datasets/notebook_data/south/train/00000042.tfrecord\n",
      "[23-11-22 22:31:52 :INFO    ] - Finished output ./network_datasets/notebook_data/south/train/00000043.tfrecord\n",
      "[23-11-22 22:31:52 :INFO    ] - Finished output ./network_datasets/notebook_data/south/train/00000044.tfrecord\n",
      "[23-11-22 22:31:52 :INFO    ] - Finished output ./network_datasets/notebook_data/south/train/00000045.tfrecord\n",
      "[23-11-22 22:31:52 :INFO    ] - 23 val dates to process, generating cache data.\n",
      "[23-11-22 22:32:03 :INFO    ] - Finished output ./network_datasets/notebook_data/south/val/00000000.tfrecord\n",
      "[23-11-22 22:32:03 :INFO    ] - Finished output ./network_datasets/notebook_data/south/val/00000001.tfrecord\n",
      "[23-11-22 22:32:03 :INFO    ] - Finished output ./network_datasets/notebook_data/south/val/00000002.tfrecord\n",
      "[23-11-22 22:32:03 :INFO    ] - Finished output ./network_datasets/notebook_data/south/val/00000003.tfrecord\n",
      "[23-11-22 22:32:03 :INFO    ] - Finished output ./network_datasets/notebook_data/south/val/00000004.tfrecord\n",
      "[23-11-22 22:32:03 :INFO    ] - Finished output ./network_datasets/notebook_data/south/val/00000005.tfrecord\n",
      "[23-11-22 22:32:03 :INFO    ] - Finished output ./network_datasets/notebook_data/south/val/00000006.tfrecord\n",
      "[23-11-22 22:32:03 :INFO    ] - Finished output ./network_datasets/notebook_data/south/val/00000007.tfrecord\n",
      "[23-11-22 22:32:09 :INFO    ] - Finished output ./network_datasets/notebook_data/south/val/00000008.tfrecord\n",
      "[23-11-22 22:32:09 :INFO    ] - Finished output ./network_datasets/notebook_data/south/val/00000009.tfrecord\n",
      "[23-11-22 22:32:09 :INFO    ] - Finished output ./network_datasets/notebook_data/south/val/00000010.tfrecord\n",
      "[23-11-22 22:32:09 :INFO    ] - Finished output ./network_datasets/notebook_data/south/val/00000011.tfrecord\n",
      "[23-11-22 22:32:09 :INFO    ] - 2 test dates to process, generating cache data.\n",
      "[23-11-22 22:32:12 :INFO    ] - Finished output ./network_datasets/notebook_data/south/test/00000000.tfrecord\n",
      "[23-11-22 22:32:12 :INFO    ] - Average sample generation time: 3.883745174983452\n",
      "[23-11-22 22:32:12 :INFO    ] - Writing configuration to ./dataset_config.notebook_data.json\n"
     ]
    }
   ],
   "source": [
    "!icenet_dataset_create -l 1 -fd 7 -ob 2 -w 4 notebook_data south"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "The common options used here: \n",
    "\n",
    "* `-fn` allows us to specify how far forward to forecast to. For this example we're limiting to 7 days based on the limited amount of SIC groud truth data we downloaded.\n",
    "* `-l` as in the preprocessing stage. If experimenting and using full date ranges, creating a dataset with a different lag can save having to reprocess everything.\n",
    "* `-ob` is the output batch size for the tfrecords. It is advisable to keep this smaller except where there are seriously large numbers of sets, preferably near to the expected size being used for training.\n",
    "* `-w` specifies the number of worker subprocesses to use for producing the output. Probably advisable to keep this below the number of cores on your host! :) \n",
    "\n",
    "#### Config-only operation / Prediction datasets\n",
    "\n",
    "Datasets used to predict don't benefit from caching, so adding the `-c` option and dropping `-w` and `-ob` will create a configuration for the dataset without writing sets to disk. You can also use this option to create a dataset that is fed directly from the preprocessed data, though bear in mind, depending on your infrastructure, that this requires the batches to be created on the fly and can have a significant impact on performance. By specifying `-fn` we ensure the dataset is given a different name to the previously cached one above (though this is more commonly used for prediction datasets where caching isn't necessary...) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-23 22:32:15.332712: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-23 22:32:16.193565: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-23 22:32:16.193646: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-23 22:32:16.193659: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "[23-11-22 22:32:17 :INFO    ] - Got 0 dates for train\n",
      "[23-11-22 22:32:17 :INFO    ] - Got 0 dates for val\n",
      "[23-11-22 22:32:17 :INFO    ] - Got 0 dates for test\n",
      "[23-11-22 22:32:17 :INFO    ] - Creating path: ./network_datasets/notebook_raw_dataset\n",
      "[23-11-22 22:32:17 :INFO    ] - Loading configuration loader.notebook_data.json\n",
      "[23-11-22 22:32:17 :INFO    ] - Writing dataset configuration without data generation\n",
      "[23-11-22 22:32:17 :INFO    ] - 91 train dates in total, NOT generating cache data.\n",
      "[23-11-22 22:32:17 :INFO    ] - 23 val dates in total, NOT generating cache data.\n",
      "[23-11-22 22:32:17 :INFO    ] - 2 test dates in total, NOT generating cache data.\n",
      "[23-11-22 22:32:17 :INFO    ] - Writing configuration to ./dataset_config.notebook_raw_dataset.json\n"
     ]
    }
   ],
   "source": [
    "!icenet_dataset_create -fd 7 -l 1 -c -fn notebook_raw_dataset notebook_data south"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "## Train\n",
    "\n",
    "Once the dataset is prepared, running a network is then as simple as using `icenet_train` with the appropriate parameters. Some key parameters are illustrated in the following commands:\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-23 23:21:39.674563: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-23 23:21:40.728677: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/\n",
      "2022-11-23 23:21:40.728768: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/\n",
      "2022-11-23 23:21:40.728804: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "[23-11-22 23:21:43 :WARNING ] - Setting seed for best attempt at determinism, value 42\n",
      "[23-11-22 23:21:43 :INFO    ] - Loading configuration dataset_config.notebook_data.json\n",
      "[23-11-22 23:21:43 :INFO    ] - Training dataset path: ./network_datasets/notebook_data/south/train\n",
      "[23-11-22 23:21:43 :INFO    ] - Validation dataset path: ./network_datasets/notebook_data/south/val\n",
      "[23-11-22 23:21:43 :INFO    ] - Test dataset path: ./network_datasets/notebook_data/south/test\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjambyr\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/data/hpcdata/users/jambyr/icenet/notebooks/wandb/run-20221123_232145-38bug7e0\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mnotebook_testrun.42\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jambyr/icenet\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jambyr/icenet/runs/38bug7e0\u001b[0m\n",
      "[23-11-22 23:21:51 :INFO    ] - Hyperparameters: {'seed': 42, 'learning_rate': 0.0001, 'filter_size': 3, 'n_filters_factor': 0.6, 'lr_10e_decay_fac': 1.0, 'lr_decay': -0.0, 'lr_decay_start': 10, 'lr_decay_end': 30, 'batch_size': 2}\n",
      "[23-11-22 23:21:51 :INFO    ] - Adding tensorboard callback\n",
      "[23-11-22 23:21:51 :INFO    ] - Adding wandb callback\n",
      "2022-11-23 23:21:54.394428: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-23 23:21:55.205915: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7404 MB memory:  -> device: 0, name: Quadro P4000, pci bus id: 0000:3b:00.0, compute capability: 6.1\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 432, 432, 9  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 432, 432, 38  3116        ['input_1[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 432, 432, 38  13034       ['conv2d[0][0]']                 \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 432, 432, 38  152        ['conv2d_1[0][0]']               \n",
      " alization)                     )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 216, 216, 38  0           ['batch_normalization[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 216, 216, 76  26068       ['max_pooling2d[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 216, 216, 76  52060       ['conv2d_2[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 216, 216, 76  304        ['conv2d_3[0][0]']               \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 108, 108, 76  0          ['batch_normalization_1[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 108, 108, 15  104805      ['max_pooling2d_1[0][0]']        \n",
      "                                3)                                                                \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 108, 108, 15  210834      ['conv2d_4[0][0]']               \n",
      "                                3)                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 108, 108, 15  612        ['conv2d_5[0][0]']               \n",
      " rmalization)                   3)                                                                \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 54, 54, 153)  0          ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 54, 54, 153)  210834      ['max_pooling2d_2[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 54, 54, 153)  210834      ['conv2d_6[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 54, 54, 153)  612        ['conv2d_7[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPooling2D)  (None, 27, 27, 153)  0          ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 27, 27, 307)  423046      ['max_pooling2d_3[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 27, 27, 307)  848548      ['conv2d_8[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 27, 27, 307)  1228       ['conv2d_9[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " up_sampling2d (UpSampling2D)   (None, 54, 54, 307)  0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 54, 54, 153)  188037      ['up_sampling2d[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 54, 54, 306)  0           ['batch_normalization_3[0][0]',  \n",
      "                                                                  'conv2d_10[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 54, 54, 153)  421515      ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 54, 54, 153)  210834      ['conv2d_11[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 54, 54, 153)  612        ['conv2d_12[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " up_sampling2d_1 (UpSampling2D)  (None, 108, 108, 15  0          ['batch_normalization_5[0][0]']  \n",
      "                                3)                                                                \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 108, 108, 15  93789       ['up_sampling2d_1[0][0]']        \n",
      "                                3)                                                                \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 108, 108, 30  0           ['batch_normalization_2[0][0]',  \n",
      "                                6)                                'conv2d_13[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 108, 108, 15  421515      ['concatenate_1[0][0]']          \n",
      "                                3)                                                                \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 108, 108, 15  210834      ['conv2d_14[0][0]']              \n",
      "                                3)                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 108, 108, 15  612        ['conv2d_15[0][0]']              \n",
      " rmalization)                   3)                                                                \n",
      "                                                                                                  \n",
      " up_sampling2d_2 (UpSampling2D)  (None, 216, 216, 15  0          ['batch_normalization_6[0][0]']  \n",
      "                                3)                                                                \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 216, 216, 76  46588       ['up_sampling2d_2[0][0]']        \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 216, 216, 15  0           ['batch_normalization_1[0][0]',  \n",
      "                                2)                                'conv2d_16[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 216, 216, 76  104044      ['concatenate_2[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 216, 216, 76  52060       ['conv2d_17[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 216, 216, 76  304        ['conv2d_18[0][0]']              \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " up_sampling2d_3 (UpSampling2D)  (None, 432, 432, 76  0          ['batch_normalization_7[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 432, 432, 38  11590       ['up_sampling2d_3[0][0]']        \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 432, 432, 76  0           ['conv2d_1[0][0]',               \n",
      "                                )                                 'conv2d_19[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)             (None, 432, 432, 38  26030       ['concatenate_3[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)             (None, 432, 432, 38  13034       ['conv2d_20[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)             (None, 432, 432, 38  13034       ['conv2d_21[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)             (None, 432, 432, 7)  273         ['conv2d_22[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,920,792\n",
      "Trainable params: 3,918,574\n",
      "Non-trainable params: 2,218\n",
      "__________________________________________________________________________________________________\n",
      "[23-11-22 23:21:55 :INFO    ] - Datasets: 46 train, 12 val and 1 test filenames\n",
      "[23-11-22 23:21:55 :INFO    ] - Reducing datasets to 1.0 of total files\n",
      "[23-11-22 23:21:55 :INFO    ] - Reduced: 46 train, 12 val and 1 test filenames\n",
      "[23-11-22 23:21:56 :INFO    ] - \n",
      "Setting learning rate to: 9.999999747378752e-05\n",
      "\n",
      "Epoch 1/10\n",
      "2022-11-23 23:22:00.689487: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8100\n",
      "2022-11-23 23:22:03.799576: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x7fe3b80953f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-11-23 23:22:03.799645: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): Quadro P4000, Compute Capability 6.1\n",
      "2022-11-23 23:22:03.808055: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-11-23 23:22:04.174494: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "\n",
      "Epoch 1: val_rmse improved from inf to 40.09857, saving model to ./results/networks/notebook_testrun/notebook_testrun.network_notebook_data.42.h5\n",
      "46/46 - 35s - loss: 189.2667 - binacc: 51.6029 - mae: 25.4169 - rmse: 32.2657 - mse: 2090.2957 - val_loss: 292.3133 - val_binacc: 36.9813 - val_mae: 37.7046 - val_rmse: 40.0986 - val_mse: 2218.0181 - lr: 1.0000e-04 - 35s/epoch - 762ms/step\n",
      "[23-11-22 23:22:31 :INFO    ] - \n",
      "Setting learning rate to: 9.999999747378752e-05\n",
      "\n",
      "Epoch 2/10\n",
      "\n",
      "Epoch 2: val_rmse did not improve from 40.09857\n",
      "46/46 - 12s - loss: 21.6535 - binacc: 94.0598 - mae: 5.7006 - rmse: 10.9136 - mse: 1312.9708 - val_loss: 301.1012 - val_binacc: 37.0538 - val_mae: 38.5763 - val_rmse: 40.6969 - val_mse: 1751.1008 - lr: 1.0000e-04 - 12s/epoch - 258ms/step\n",
      "[23-11-22 23:22:42 :INFO    ] - \n",
      "Setting learning rate to: 9.999999747378752e-05\n",
      "\n",
      "Epoch 3/10\n",
      "\n",
      "Epoch 3: val_rmse improved from 40.09857 to 39.49470, saving model to ./results/networks/notebook_testrun/notebook_testrun.network_notebook_data.42.h5\n",
      "46/46 - 12s - loss: 13.0153 - binacc: 96.0119 - mae: 3.9652 - rmse: 8.4612 - mse: 1236.2612 - val_loss: 283.5754 - val_binacc: 37.5874 - val_mae: 37.5036 - val_rmse: 39.4947 - val_mse: 1630.6532 - lr: 1.0000e-04 - 12s/epoch - 266ms/step\n",
      "[23-11-22 23:22:55 :INFO    ] - \n",
      "Setting learning rate to: 9.999999747378752e-05\n",
      "\n",
      "Epoch 4/10\n",
      "\n",
      "Epoch 4: val_rmse improved from 39.49470 to 33.94327, saving model to ./results/networks/notebook_testrun/notebook_testrun.network_notebook_data.42.h5\n",
      "46/46 - 12s - loss: 10.0559 - binacc: 96.6098 - mae: 3.4076 - rmse: 7.4373 - mse: 1219.0111 - val_loss: 209.4586 - val_binacc: 71.7823 - val_mae: 26.7566 - val_rmse: 33.9433 - val_mse: 1255.3135 - lr: 1.0000e-04 - 12s/epoch - 266ms/step\n",
      "[23-11-22 23:23:07 :INFO    ] - \n",
      "Setting learning rate to: 9.999999747378752e-05\n",
      "\n",
      "Epoch 5/10\n",
      "\n",
      "Epoch 5: val_rmse improved from 33.94327 to 30.29539, saving model to ./results/networks/notebook_testrun/notebook_testrun.network_notebook_data.42.h5\n",
      "46/46 - 12s - loss: 8.7521 - binacc: 96.9368 - mae: 3.1304 - rmse: 6.9384 - mse: 1215.0828 - val_loss: 166.8569 - val_binacc: 90.8511 - val_mae: 20.0343 - val_rmse: 30.2954 - val_mse: 905.9563 - lr: 1.0000e-04 - 12s/epoch - 266ms/step\n",
      "[23-11-22 23:23:19 :INFO    ] - \n",
      "Setting learning rate to: 9.999999747378752e-05\n",
      "\n",
      "Epoch 6/10\n",
      "\n",
      "Epoch 6: val_rmse improved from 30.29539 to 26.03325, saving model to ./results/networks/notebook_testrun/notebook_testrun.network_notebook_data.42.h5\n",
      "46/46 - 12s - loss: 7.7374 - binacc: 97.1680 - mae: 2.9133 - rmse: 6.5238 - mse: 1201.4587 - val_loss: 123.2105 - val_binacc: 93.4496 - val_mae: 16.5785 - val_rmse: 26.0333 - val_mse: 680.7811 - lr: 1.0000e-04 - 12s/epoch - 267ms/step\n",
      "[23-11-22 23:23:32 :INFO    ] - \n",
      "Setting learning rate to: 9.999999747378752e-05\n",
      "\n",
      "Epoch 7/10\n",
      "\n",
      "Epoch 7: val_rmse improved from 26.03325 to 22.38416, saving model to ./results/networks/notebook_testrun/notebook_testrun.network_notebook_data.42.h5\n",
      "46/46 - 12s - loss: 6.8245 - binacc: 97.3632 - mae: 2.7110 - rmse: 6.1269 - mse: 1185.0846 - val_loss: 91.0904 - val_binacc: 95.0103 - val_mae: 14.1075 - val_rmse: 22.3842 - val_mse: 593.7509 - lr: 1.0000e-04 - 12s/epoch - 267ms/step\n",
      "[23-11-22 23:23:44 :INFO    ] - \n",
      "Setting learning rate to: 9.999999747378752e-05\n",
      "\n",
      "Epoch 8/10\n",
      "\n",
      "Epoch 8: val_rmse improved from 22.38416 to 18.46667, saving model to ./results/networks/notebook_testrun/notebook_testrun.network_notebook_data.42.h5\n",
      "46/46 - 12s - loss: 6.1726 - binacc: 97.5036 - mae: 2.5576 - rmse: 5.8269 - mse: 1152.4609 - val_loss: 61.9966 - val_binacc: 95.9773 - val_mae: 11.5218 - val_rmse: 18.4667 - val_mse: 578.2522 - lr: 1.0000e-04 - 12s/epoch - 268ms/step\n",
      "[23-11-22 23:23:56 :INFO    ] - \n",
      "Setting learning rate to: 9.999999747378752e-05\n",
      "\n",
      "Epoch 9/10\n",
      "\n",
      "Epoch 9: val_rmse improved from 18.46667 to 15.22500, saving model to ./results/networks/notebook_testrun/notebook_testrun.network_notebook_data.42.h5\n",
      "46/46 - 12s - loss: 5.8670 - binacc: 97.5820 - mae: 2.4721 - rmse: 5.6808 - mse: 1114.6622 - val_loss: 42.1411 - val_binacc: 96.3749 - val_mae: 9.3100 - val_rmse: 15.2250 - val_mse: 568.8254 - lr: 1.0000e-04 - 12s/epoch - 269ms/step\n",
      "[23-11-22 23:24:09 :INFO    ] - \n",
      "Setting learning rate to: 9.999999747378752e-05\n",
      "\n",
      "Epoch 10/10\n",
      "\n",
      "Epoch 10: val_rmse improved from 15.22500 to 11.46211, saving model to ./results/networks/notebook_testrun/notebook_testrun.network_notebook_data.42.h5\n",
      "46/46 - 12s - loss: 6.1153 - binacc: 97.5321 - mae: 2.5010 - rmse: 5.7998 - mse: 1058.7278 - val_loss: 23.8847 - val_binacc: 96.8101 - val_mae: 7.0663 - val_rmse: 11.4621 - val_mse: 535.9595 - lr: 1.0000e-04 - 12s/epoch - 267ms/step\n",
      "[23-11-22 23:24:21 :INFO    ] - Saving network to: ./results/networks/notebook_testrun/notebook_testrun.network_notebook_data.42.h5\n",
      "[23-11-22 23:24:24 :WARNING ] - Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 24). These functions will not be directly callable after loading.\n",
      "[23-11-22 23:24:26 :INFO    ] - Running evaluation against test set\n",
      "WARNING:tensorflow:Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.\n",
      "[23-11-22 23:24:26 :WARNING ] - Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.\n",
      "WARNING:tensorflow:Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.\n",
      "[23-11-22 23:24:26 :WARNING ] - Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.\n",
      "WARNING:tensorflow:Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.\n",
      "[23-11-22 23:24:26 :WARNING ] - Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.\n",
      "WARNING:tensorflow:Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.\n",
      "[23-11-22 23:24:26 :WARNING ] - Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.\n",
      "[23-11-22 23:24:28 :INFO    ] - Datasets: 46 train, 12 val and 1 test filenames\n",
      "[23-11-22 23:24:28 :INFO    ] - Reducing datasets to 1.0 of total files\n",
      "[23-11-22 23:24:28 :INFO    ] - Reduced: 46 train, 12 val and 1 test filenames\n",
      "[23-11-22 23:24:28 :WARNING ] - Using validation data source for evaluation, rather than test set\n",
      "[23-11-22 23:24:28 :INFO    ] - Metric creation for lead time of 7 days\n",
      "[23-11-22 23:24:28 :INFO    ] - Evaluating... \n",
      "[23-11-22 23:24:30 :INFO    ] - Done in 2.2s\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     binacc ▁▇████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▂▃▃▄▅▆▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       loss █▂▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        mae █▂▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        mse █▃▂▂▂▂▂▂▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       rmse █▂▂▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: val_binacc ▁▁▁▅▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss ███▆▅▄▃▂▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_mae ███▅▄▃▃▂▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_mse █▆▆▄▃▂▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_rmse ███▆▆▄▄▃▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    best_epoch 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: best_val_rmse 11.46211\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        binacc 97.53213\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         epoch 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          loss 6.1153\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            lr 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           mae 2.50104\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           mse 1058.72778\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          rmse 5.79981\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_binacc 96.8101\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      val_loss 23.88472\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_mae 7.06627\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_mse 535.95947\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      val_rmse 11.46211\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mnotebook_testrun.42\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/jambyr/icenet/runs/38bug7e0\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20221123_232145-38bug7e0/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!icenet_train notebook_data notebook_testrun 42 -b 2 -e 10 -m -qs 4 -w 4 -n 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-23 23:24:53.300788: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-23 23:24:54.355024: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/\n",
      "2022-11-23 23:24:54.355124: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/\n",
      "2022-11-23 23:24:54.355138: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "[23-11-22 23:24:56 :WARNING ] - Setting seed for best attempt at determinism, value 42\n",
      "[23-11-22 23:24:56 :INFO    ] - Loading configuration dataset_config.notebook_data.json\n",
      "[23-11-22 23:24:56 :INFO    ] - Training dataset path: ./network_datasets/notebook_data/south/train\n",
      "[23-11-22 23:24:56 :INFO    ] - Validation dataset path: ./network_datasets/notebook_data/south/val\n",
      "[23-11-22 23:24:56 :INFO    ] - Test dataset path: ./network_datasets/notebook_data/south/test\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjambyr\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/data/hpcdata/users/jambyr/icenet/notebooks/wandb/run-20221123_232458-ccq1bxjn\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mnotebook_testrun.42\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jambyr/icenet\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jambyr/icenet/runs/ccq1bxjn\u001b[0m\n",
      "[23-11-22 23:25:04 :INFO    ] - Hyperparameters: {'seed': 42, 'learning_rate': 0.0001, 'filter_size': 3, 'n_filters_factor': 0.6, 'lr_10e_decay_fac': 1.0, 'lr_decay': -0.0, 'lr_decay_start': 10, 'lr_decay_end': 30, 'batch_size': 2}\n",
      "[23-11-22 23:25:04 :INFO    ] - Adding tensorboard callback\n",
      "[23-11-22 23:25:04 :INFO    ] - Adding wandb callback\n",
      "2022-11-23 23:25:08.040965: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-23 23:25:08.840329: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7404 MB memory:  -> device: 0, name: Quadro P4000, pci bus id: 0000:3b:00.0, compute capability: 6.1\n",
      "[23-11-22 23:25:09 :INFO    ] - Loading network weights from ./results/networks/notebook_testrun/notebook_testrun.network_notebook_data.42.h5\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 432, 432, 9  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 432, 432, 38  3116        ['input_1[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 432, 432, 38  13034       ['conv2d[0][0]']                 \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 432, 432, 38  152        ['conv2d_1[0][0]']               \n",
      " alization)                     )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 216, 216, 38  0           ['batch_normalization[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 216, 216, 76  26068       ['max_pooling2d[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 216, 216, 76  52060       ['conv2d_2[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 216, 216, 76  304        ['conv2d_3[0][0]']               \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 108, 108, 76  0          ['batch_normalization_1[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 108, 108, 15  104805      ['max_pooling2d_1[0][0]']        \n",
      "                                3)                                                                \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 108, 108, 15  210834      ['conv2d_4[0][0]']               \n",
      "                                3)                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 108, 108, 15  612        ['conv2d_5[0][0]']               \n",
      " rmalization)                   3)                                                                \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 54, 54, 153)  0          ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 54, 54, 153)  210834      ['max_pooling2d_2[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 54, 54, 153)  210834      ['conv2d_6[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 54, 54, 153)  612        ['conv2d_7[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPooling2D)  (None, 27, 27, 153)  0          ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 27, 27, 307)  423046      ['max_pooling2d_3[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 27, 27, 307)  848548      ['conv2d_8[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 27, 27, 307)  1228       ['conv2d_9[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " up_sampling2d (UpSampling2D)   (None, 54, 54, 307)  0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 54, 54, 153)  188037      ['up_sampling2d[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 54, 54, 306)  0           ['batch_normalization_3[0][0]',  \n",
      "                                                                  'conv2d_10[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 54, 54, 153)  421515      ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 54, 54, 153)  210834      ['conv2d_11[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 54, 54, 153)  612        ['conv2d_12[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " up_sampling2d_1 (UpSampling2D)  (None, 108, 108, 15  0          ['batch_normalization_5[0][0]']  \n",
      "                                3)                                                                \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 108, 108, 15  93789       ['up_sampling2d_1[0][0]']        \n",
      "                                3)                                                                \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 108, 108, 30  0           ['batch_normalization_2[0][0]',  \n",
      "                                6)                                'conv2d_13[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 108, 108, 15  421515      ['concatenate_1[0][0]']          \n",
      "                                3)                                                                \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 108, 108, 15  210834      ['conv2d_14[0][0]']              \n",
      "                                3)                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 108, 108, 15  612        ['conv2d_15[0][0]']              \n",
      " rmalization)                   3)                                                                \n",
      "                                                                                                  \n",
      " up_sampling2d_2 (UpSampling2D)  (None, 216, 216, 15  0          ['batch_normalization_6[0][0]']  \n",
      "                                3)                                                                \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 216, 216, 76  46588       ['up_sampling2d_2[0][0]']        \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 216, 216, 15  0           ['batch_normalization_1[0][0]',  \n",
      "                                2)                                'conv2d_16[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 216, 216, 76  104044      ['concatenate_2[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 216, 216, 76  52060       ['conv2d_17[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 216, 216, 76  304        ['conv2d_18[0][0]']              \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " up_sampling2d_3 (UpSampling2D)  (None, 432, 432, 76  0          ['batch_normalization_7[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 432, 432, 38  11590       ['up_sampling2d_3[0][0]']        \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 432, 432, 76  0           ['conv2d_1[0][0]',               \n",
      "                                )                                 'conv2d_19[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)             (None, 432, 432, 38  26030       ['concatenate_3[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)             (None, 432, 432, 38  13034       ['conv2d_20[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)             (None, 432, 432, 38  13034       ['conv2d_21[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)             (None, 432, 432, 7)  273         ['conv2d_22[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,920,792\n",
      "Trainable params: 3,918,574\n",
      "Non-trainable params: 2,218\n",
      "__________________________________________________________________________________________________\n",
      "[23-11-22 23:25:09 :INFO    ] - Datasets: 46 train, 12 val and 1 test filenames\n",
      "[23-11-22 23:25:09 :INFO    ] - Reducing datasets to 1.0 of total files\n",
      "[23-11-22 23:25:09 :INFO    ] - Reduced: 46 train, 12 val and 1 test filenames\n",
      "[23-11-22 23:25:09 :INFO    ] - \n",
      "Setting learning rate to: 9.999999747378752e-05\n",
      "\n",
      "Epoch 1/2\n",
      "2022-11-23 23:25:14.579361: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8100\n",
      "2022-11-23 23:25:17.830095: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x7f79e1e54e70 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-11-23 23:25:17.830159: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): Quadro P4000, Compute Capability 6.1\n",
      "2022-11-23 23:25:17.844696: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-11-23 23:25:18.264918: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "\n",
      "Epoch 1: val_rmse improved from inf to 11.41547, saving model to ./results/networks/notebook_testrun/notebook_testrun.network_notebook_data.42.h5\n",
      "46/46 - 36s - loss: 5.9633 - binacc: 97.5806 - mae: 2.4521 - rmse: 5.7273 - mse: 934.2524 - val_loss: 23.6907 - val_binacc: 96.8726 - val_mae: 6.6069 - val_rmse: 11.4155 - val_mse: 552.8801 - lr: 1.0000e-04 - 36s/epoch - 777ms/step\n",
      "[23-11-22 23:25:45 :INFO    ] - \n",
      "Setting learning rate to: 9.999999747378752e-05\n",
      "\n",
      "Epoch 2/2\n",
      "\n",
      "Epoch 2: val_rmse improved from 11.41547 to 9.32188, saving model to ./results/networks/notebook_testrun/notebook_testrun.network_notebook_data.42.h5\n",
      "46/46 - 12s - loss: 5.4767 - binacc: 97.6760 - mae: 2.3211 - rmse: 5.4886 - mse: 911.4945 - val_loss: 15.7979 - val_binacc: 96.1300 - val_mae: 5.3839 - val_rmse: 9.3219 - val_mse: 537.3751 - lr: 1.0000e-04 - 12s/epoch - 261ms/step\n",
      "[23-11-22 23:25:57 :INFO    ] - Saving network to: ./results/networks/notebook_testrun/notebook_testrun.network_notebook_data.42.h5\n",
      "[23-11-22 23:26:00 :WARNING ] - Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 24). These functions will not be directly callable after loading.\n",
      "[23-11-22 23:26:02 :INFO    ] - Running evaluation against test set\n",
      "WARNING:tensorflow:Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.\n",
      "[23-11-22 23:26:02 :WARNING ] - Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.\n",
      "WARNING:tensorflow:Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.\n",
      "[23-11-22 23:26:02 :WARNING ] - Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.\n",
      "WARNING:tensorflow:Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.\n",
      "[23-11-22 23:26:02 :WARNING ] - Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.\n",
      "WARNING:tensorflow:Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.\n",
      "[23-11-22 23:26:02 :WARNING ] - Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.\n",
      "[23-11-22 23:26:03 :INFO    ] - Datasets: 46 train, 12 val and 1 test filenames\n",
      "[23-11-22 23:26:03 :INFO    ] - Reducing datasets to 1.0 of total files\n",
      "[23-11-22 23:26:03 :INFO    ] - Reduced: 46 train, 12 val and 1 test filenames\n",
      "[23-11-22 23:26:04 :WARNING ] - Using validation data source for evaluation, rather than test set\n",
      "[23-11-22 23:26:04 :INFO    ] - Metric creation for lead time of 7 days\n",
      "[23-11-22 23:26:04 :INFO    ] - Evaluating... \n",
      "[23-11-22 23:26:06 :INFO    ] - Done in 2.2s\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     binacc ▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       loss █▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        mae █▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        mse █▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       rmse █▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: val_binacc █▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss █▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_mae █▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_mse █▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_rmse █▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    best_epoch 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: best_val_rmse 9.32188\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        binacc 97.67599\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         epoch 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          loss 5.47669\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            lr 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           mae 2.32107\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           mse 911.49451\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          rmse 5.48863\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_binacc 96.12997\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      val_loss 15.79785\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_mae 5.38389\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_mse 537.37512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      val_rmse 9.32188\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mnotebook_testrun.42\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/jambyr/icenet/runs/ccq1bxjn\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20221123_232458-ccq1bxjn/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!icenet_train notebook_data notebook_testrun 42 -b 2 -e 2 -m -qs 4 -w 4 -n 0.6 \\\n",
    "    -p ./results/networks/notebook_testrun/notebook_testrun.network_notebook_data.42.h5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "These runs demonstrate using the aforementioned dataset, in `-b` batches of 4 for a run of `-e` five epochs. Using `-m` for multiprocessing we enable up to `-w` four process workers to load data at a time into a data queue `-qs` of length four. We could specify a `-r` ratio we use only 0.2x of the files from the dataset (_useful when testing on a low power machine with a large dataset, but unnecessary with our example here_) supplying a UNet built with 0.6x the `-n` numbers of filters as normal. \n",
    "\n",
    "With the second command we `-p` pickup the output weights from the previous run to continue training.\n",
    "\n",
    "There are a few things to note about the `icenet_train` and `icenet_predict` (see [the prediction section below](#Predict)) commands and the switches they provide: \n",
    "\n",
    "* Common switches such as `-n` should be applied consistently between training and prediction. \n",
    "* These commands work with __individual network runs__ (see the next section).\n",
    "\n",
    "### Ensemble running\n",
    "\n",
    "For producing forecasts in the described pipeline we actually run a set of models using the [model-ensembler](https://github.com/JimCircadian/model-ensembler) tool and as such there are convenience scripts for doing this as part of the end to end run. \n",
    "\n",
    "To do this we need to be running in a clone of the `icenet-pipeline` repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jp-MarkdownHeadingCollapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARGS: -b 4 -e 5 -f 0.6 -n node022 -p bashpc.sh -q 4 -j 3 notebook_data notebook_data notebook_ensemble\n",
      "ARGS = -x arg_batch=4 arg_epochs=5 nodelist=node022 arg_prep=bashpc.sh arg_queue=4 , Leftovers: notebook_data notebook_data notebook_ensemble\n",
      "Running model_ensemble ./tmp.DqWuX3IhWF.train slurm -x arg_batch=4 arg_epochs=5 nodelist=node022 arg_prep=bashpc.sh arg_queue=4 arg_filter_factor=1.44 \n",
      "[23-11-22 23:40:59    :INFO    ] - Model Ensemble Runner\n",
      "[23-11-22 23:40:59    :INFO    ] - Validated configuration file ./tmp.DqWuX3IhWF.train successfully\n",
      "[23-11-22 23:40:59    :INFO    ] - Importing model_ensembler.cluster.slurm\n",
      "[23-11-22 23:40:59    :INFO    ] - Running batcher\n",
      "[23-11-22 23:40:59    :INFO    ] - Running command: mkdir -p ./results/networks\n",
      "[23-11-22 23:40:59    :INFO    ] - Start batch: 2022-11-23 23:40:59.767031\n",
      "[23-11-22 23:40:59    :INFO    ] - Running cycle 1\n",
      "[23-11-22 23:40:59    :INFO    ] - Start run notebook_ensemble-0 at 2022-11-23 23:40:59.770282\n",
      "[23-11-22 23:40:59    :INFO    ] - rsync -aXE ../template/ /data/hpcdata/users/jambyr/icenet/notebook-pipeline/ensemble/notebook_ensemble/notebook_ensemble-0/\n",
      "[23-11-22 23:40:59    :INFO    ] - Start run notebook_ensemble-1 at 2022-11-23 23:40:59.774229\n",
      "[23-11-22 23:40:59    :INFO    ] - rsync -aXE ../template/ /data/hpcdata/users/jambyr/icenet/notebook-pipeline/ensemble/notebook_ensemble/notebook_ensemble-1/\n",
      "[23-11-22 23:40:59    :INFO    ] - Start run notebook_ensemble-2 at 2022-11-23 23:40:59.777880\n",
      "[23-11-22 23:40:59    :INFO    ] - rsync -aXE ../template/ /data/hpcdata/users/jambyr/icenet/notebook-pipeline/ensemble/notebook_ensemble/notebook_ensemble-2/\n",
      "[23-11-22 23:40:59    :INFO    ] - Start run notebook_ensemble-3 at 2022-11-23 23:40:59.781652\n",
      "[23-11-22 23:40:59    :INFO    ] - rsync -aXE ../template/ /data/hpcdata/users/jambyr/icenet/notebook-pipeline/ensemble/notebook_ensemble/notebook_ensemble-3/\n",
      "[23-11-22 23:40:59    :INFO    ] - Start run notebook_ensemble-4 at 2022-11-23 23:40:59.784975\n",
      "[23-11-22 23:40:59    :INFO    ] - rsync -aXE ../template/ /data/hpcdata/users/jambyr/icenet/notebook-pipeline/ensemble/notebook_ensemble/notebook_ensemble-4/\n",
      "[23-11-22 23:40:59    :INFO    ] - Templating /data/hpcdata/users/jambyr/icenet/notebook-pipeline/ensemble/notebook_ensemble/notebook_ensemble-0/icenet_train.sh.j2 to /data/hpcdata/users/jambyr/icenet/notebook-pipeline/ensemble/notebook_ensemble/notebook_ensemble-0/icenet_train.sh\n",
      "[23-11-22 23:40:59    :INFO    ] - Templating /data/hpcdata/users/jambyr/icenet/notebook-pipeline/ensemble/notebook_ensemble/notebook_ensemble-1/icenet_train.sh.j2 to /data/hpcdata/users/jambyr/icenet/notebook-pipeline/ensemble/notebook_ensemble/notebook_ensemble-1/icenet_train.sh\n",
      "[23-11-22 23:40:59    :INFO    ] - Templating /data/hpcdata/users/jambyr/icenet/notebook-pipeline/ensemble/notebook_ensemble/notebook_ensemble-3/icenet_train.sh.j2 to /data/hpcdata/users/jambyr/icenet/notebook-pipeline/ensemble/notebook_ensemble/notebook_ensemble-3/icenet_train.sh\n",
      "[23-11-22 23:40:59    :INFO    ] - Templating /data/hpcdata/users/jambyr/icenet/notebook-pipeline/ensemble/notebook_ensemble/notebook_ensemble-2/icenet_train.sh.j2 to /data/hpcdata/users/jambyr/icenet/notebook-pipeline/ensemble/notebook_ensemble/notebook_ensemble-2/icenet_train.sh\n",
      "[23-11-22 23:40:59    :INFO    ] - Templating /data/hpcdata/users/jambyr/icenet/notebook-pipeline/ensemble/notebook_ensemble/notebook_ensemble-4/icenet_train.sh.j2 to /data/hpcdata/users/jambyr/icenet/notebook-pipeline/ensemble/notebook_ensemble/notebook_ensemble-4/icenet_train.sh\n",
      "[23-11-22 23:40:59    :INFO    ] - Submitted job with ID 5478158\n",
      "[23-11-22 23:40:59    :INFO    ] - Submitted job with ID 5478159\n",
      "[23-11-22 23:40:59    :INFO    ] - Submitted job with ID 5478160\n",
      "[23-11-22 23:40:59    :WARNING ] - Could not retrieve job from list\n",
      "[23-11-22 23:41:00    :WARNING ] - Could not retrieve job from list\n",
      "[23-11-22 23:41:00    :WARNING ] - Could not retrieve job from list\n",
      "[23-11-22 23:41:30    :INFO    ] - notebook_ensemble-0 monitor got state COMPLETED for job 5478158\n",
      "[23-11-22 23:41:30    :INFO    ] - End run notebook_ensemble-0 at 2022-11-23 23:41:30.253261\n",
      "[23-11-22 23:41:30    :INFO    ] - Start run notebook_ensemble-5 at 2022-11-23 23:41:30.258164\n",
      "[23-11-22 23:41:30    :INFO    ] - rsync -aXE ../template/ /data/hpcdata/users/jambyr/icenet/notebook-pipeline/ensemble/notebook_ensemble/notebook_ensemble-5/\n",
      "[23-11-22 23:41:30    :INFO    ] - notebook_ensemble-1 monitor got state COMPLETED for job 5478159\n",
      "[23-11-22 23:41:30    :INFO    ] - End run notebook_ensemble-1 at 2022-11-23 23:41:30.265820\n",
      "[23-11-22 23:41:30    :INFO    ] - notebook_ensemble-3 monitor got state COMPLETED for job 5478160\n",
      "[23-11-22 23:41:30    :INFO    ] - End run notebook_ensemble-3 at 2022-11-23 23:41:30.266183\n",
      "[23-11-22 23:41:30    :INFO    ] - Start run notebook_ensemble-6 at 2022-11-23 23:41:30.266954\n",
      "[23-11-22 23:41:30    :INFO    ] - rsync -aXE ../template/ /data/hpcdata/users/jambyr/icenet/notebook-pipeline/ensemble/notebook_ensemble/notebook_ensemble-6/\n",
      "[23-11-22 23:41:30    :INFO    ] - Start run notebook_ensemble-7 at 2022-11-23 23:41:30.271813\n",
      "[23-11-22 23:41:30    :INFO    ] - rsync -aXE ../template/ /data/hpcdata/users/jambyr/icenet/notebook-pipeline/ensemble/notebook_ensemble/notebook_ensemble-7/\n",
      "[23-11-22 23:41:30    :INFO    ] - Templating /data/hpcdata/users/jambyr/icenet/notebook-pipeline/ensemble/notebook_ensemble/notebook_ensemble-5/icenet_train.sh.j2 to /data/hpcdata/users/jambyr/icenet/notebook-pipeline/ensemble/notebook_ensemble/notebook_ensemble-5/icenet_train.sh\n",
      "[23-11-22 23:41:30    :INFO    ] - Templating /data/hpcdata/users/jambyr/icenet/notebook-pipeline/ensemble/notebook_ensemble/notebook_ensemble-6/icenet_train.sh.j2 to /data/hpcdata/users/jambyr/icenet/notebook-pipeline/ensemble/notebook_ensemble/notebook_ensemble-6/icenet_train.sh\n",
      "[23-11-22 23:41:30    :INFO    ] - Templating /data/hpcdata/users/jambyr/icenet/notebook-pipeline/ensemble/notebook_ensemble/notebook_ensemble-7/icenet_train.sh.j2 to /data/hpcdata/users/jambyr/icenet/notebook-pipeline/ensemble/notebook_ensemble/notebook_ensemble-7/icenet_train.sh\n",
      "[23-11-22 23:41:30    :INFO    ] - Submitted job with ID 5478163\n",
      "[23-11-22 23:41:30    :INFO    ] - Submitted job with ID 5478164\n",
      "[23-11-22 23:41:30    :WARNING ] - Could not retrieve job from list\n",
      "[23-11-22 23:41:30    :WARNING ] - Could not retrieve job from list\n",
      "[23-11-22 23:41:31    :INFO    ] - Submitted job with ID 5478165\n",
      "[23-11-22 23:41:31    :WARNING ] - Could not retrieve job from list\n",
      "[23-11-22 23:42:00    :INFO    ] - notebook_ensemble-5 monitor got state COMPLETED for job 5478164\n",
      "[23-11-22 23:42:00    :INFO    ] - End run notebook_ensemble-5 at 2022-11-23 23:42:00.747431\n",
      "[23-11-22 23:42:00    :INFO    ] - Start run notebook_ensemble-8 at 2022-11-23 23:42:00.752524\n",
      "[23-11-22 23:42:00    :INFO    ] - rsync -aXE ../template/ /data/hpcdata/users/jambyr/icenet/notebook-pipeline/ensemble/notebook_ensemble/notebook_ensemble-8/\n",
      "[23-11-22 23:42:00    :INFO    ] - notebook_ensemble-4 monitor got state COMPLETED for job 5478163\n",
      "[23-11-22 23:42:00    :INFO    ] - End run notebook_ensemble-4 at 2022-11-23 23:42:00.760300\n",
      "[23-11-22 23:42:00    :INFO    ] - Start run notebook_ensemble-9 at 2022-11-23 23:42:00.761131\n",
      "[23-11-22 23:42:00    :INFO    ] - rsync -aXE ../template/ /data/hpcdata/users/jambyr/icenet/notebook-pipeline/ensemble/notebook_ensemble/notebook_ensemble-9/\n",
      "[23-11-22 23:42:00    :INFO    ] - Templating /data/hpcdata/users/jambyr/icenet/notebook-pipeline/ensemble/notebook_ensemble/notebook_ensemble-8/icenet_train.sh.j2 to /data/hpcdata/users/jambyr/icenet/notebook-pipeline/ensemble/notebook_ensemble/notebook_ensemble-8/icenet_train.sh\n",
      "[23-11-22 23:42:00    :INFO    ] - Templating /data/hpcdata/users/jambyr/icenet/notebook-pipeline/ensemble/notebook_ensemble/notebook_ensemble-9/icenet_train.sh.j2 to /data/hpcdata/users/jambyr/icenet/notebook-pipeline/ensemble/notebook_ensemble/notebook_ensemble-9/icenet_train.sh\n",
      "[23-11-22 23:42:01    :INFO    ] - notebook_ensemble-2 monitor got state COMPLETED for job 5478165\n",
      "[23-11-22 23:42:01    :INFO    ] - End run notebook_ensemble-2 at 2022-11-23 23:42:01.689394\n",
      "[23-11-22 23:42:01    :INFO    ] - Submitted job with ID 5478169\n",
      "[23-11-22 23:42:01    :INFO    ] - Submitted job with ID 5478170\n",
      "[23-11-22 23:42:01    :WARNING ] - Could not retrieve job from list\n",
      "[23-11-22 23:42:02    :WARNING ] - Could not retrieve job from list\n",
      "[23-11-22 23:42:02    :INFO    ] - Submitted job with ID 5478171\n",
      "[23-11-22 23:42:32    :INFO    ] - notebook_ensemble-6 monitor got state COMPLETED for job 5478169\n",
      "[23-11-22 23:42:32    :INFO    ] - End run notebook_ensemble-6 at 2022-11-23 23:42:32.194566\n",
      "[23-11-22 23:42:32    :INFO    ] - notebook_ensemble-7 monitor got state COMPLETED for job 5478170\n",
      "[23-11-22 23:42:32    :INFO    ] - End run notebook_ensemble-7 at 2022-11-23 23:42:32.221185\n",
      "[23-11-22 23:42:33    :INFO    ] - Submitted job with ID 5478176\n",
      "[23-11-22 23:42:33    :WARNING ] - Could not retrieve job from list\n",
      "[23-11-22 23:43:03    :INFO    ] - notebook_ensemble-8 monitor got state COMPLETED for job 5478171\n",
      "[23-11-22 23:43:03    :INFO    ] - End run notebook_ensemble-8 at 2022-11-23 23:43:03.319355\n",
      "[23-11-22 23:43:03    :INFO    ] - notebook_ensemble-9 monitor got state COMPLETED for job 5478176\n",
      "[23-11-22 23:43:03    :INFO    ] - End run notebook_ensemble-9 at 2022-11-23 23:43:03.754270\n",
      "[23-11-22 23:43:03    :INFO    ] - Running command: /usr/bin/echo \"No postprocessing in place for training ensemble\"\n",
      "[23-11-22 23:43:03    :INFO    ] - Batch notebook_ensemble completed: 2022-11-23 23:43:03.765097\n",
      "Removing temporary configuration ./tmp.DqWuX3IhWF.train\n"
     ]
    }
   ],
   "source": [
    "!./run_train_ensemble.sh \\\n",
    "    -b 4 -e 5 -f 0.6 -n node022 -p bashpc.sh -q 4 -j 3 \\\n",
    "    notebook_data notebook_data notebook_ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "Many of the arguments are equivalent to the above `icenet_train` command. However, the `-n` filters factor is actually `-f` in this example (note that because I'm running on a cluster I've doubled this) and we have additional arguments `-n` for the node to run on, `-p` for the pre_run script to use and `-j` for the number of simultaneous runs to execute on the SLURM cluster we use at BAS. However, these arguments are not necessarily required for other clusters, nor is the model-ensembler limited to running on SLURM (it can, at present, also run locally.)  \n",
    "\n",
    "The pipeline repository shell scripts that provide this functionality are easily adaptable, as well as the ensemble itself which is stored in the pipeline repository under `/ensemble/`.\n",
    "\n",
    "_Please review the `-h` help option for the script to gain further insight the options available._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "## Predict\n",
    "\n",
    "One the network is trained it is possible to run any suitable sets through the network for training. __This is the purpose of configuration only datasets__ which are used by the `run_predict_ensemble` to, similarly to the training process, run predictions through all of the ensemble members. \n",
    "\n",
    "To run an individual sets through the test network from the test dataset we produced earlier can be easily achieved. The steps are to create a date file, which can be produced from the configuration created by `icenet_process` in the [processing section](#Process). This date file then can be supplied to the `icenet_predict` command to produce files using either cached data (useful for test data prepared at the same time as the training and validation sets) or directly from the normalised data (as is the case for nearly all data that isn't part of the training run.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-01\n",
      "2020-04-02\n"
     ]
    }
   ],
   "source": [
    "!./loader_test_dates.sh notebook_data | tee testdates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-23 23:43:42.970056: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-23 23:43:43.909033: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/\n",
      "2022-11-23 23:43:43.909171: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/\n",
      "2022-11-23 23:43:43.909185: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "[23-11-22 23:43:45 :INFO    ] - Loading configuration ./dataset_config.notebook_data.json\n",
      "[23-11-22 23:43:45 :INFO    ] - Training dataset path: ./network_datasets/notebook_data/south/train\n",
      "[23-11-22 23:43:45 :INFO    ] - Validation dataset path: ./network_datasets/notebook_data/south/val\n",
      "[23-11-22 23:43:45 :INFO    ] - Test dataset path: ./network_datasets/notebook_data/south/test\n",
      "[23-11-22 23:43:45 :INFO    ] - Loading configuration loader.notebook_data.json\n",
      "[23-11-22 23:43:45 :INFO    ] - Loading model from ./results/networks/notebook_testrun/notebook_testrun.network_notebook_data.42.h5...\n",
      "2022-11-23 23:43:45.449119: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-23 23:43:46.216459: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7404 MB memory:  -> device: 0, name: Quadro P4000, pci bus id: 0000:3b:00.0, compute capability: 6.1\n",
      "[23-11-22 23:43:46 :INFO    ] - Datasets: 46 train, 12 val and 1 test filenames\n",
      "[23-11-22 23:43:47 :INFO    ] - Processing test batch 1, item 0 (date 2020-04-01)\n",
      "[23-11-22 23:43:47 :INFO    ] - Running prediction 2020-04-01\n",
      "2022-11-23 23:43:47.739293: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8100\n",
      "[23-11-22 23:43:49 :INFO    ] - Saving 2020-04-01 - forecast output (1, 432, 432, 7)\n",
      "[23-11-22 23:43:49 :INFO    ] - Saving 2020-04-01 - generated input (432, 432, 9)\n",
      "[23-11-22 23:43:49 :INFO    ] - Saving 2020-04-01 - generated outputs (432, 432, 7, 1)\n",
      "[23-11-22 23:43:49 :INFO    ] - Saving 2020-04-01 - generated weights (432, 432, 7, 1)\n",
      "[23-11-22 23:43:49 :INFO    ] - Processing test batch 1, item 1 (date 2020-04-02)\n",
      "[23-11-22 23:43:49 :INFO    ] - Running prediction 2020-04-02\n",
      "[23-11-22 23:43:50 :WARNING ] - ./results/predict/example_south_forecast/notebook_testrun.42 output already exists\n",
      "[23-11-22 23:43:50 :INFO    ] - Saving 2020-04-02 - forecast output (1, 432, 432, 7)\n",
      "[23-11-22 23:43:50 :INFO    ] - Saving 2020-04-02 - generated input (432, 432, 9)\n",
      "[23-11-22 23:43:50 :INFO    ] - Saving 2020-04-02 - generated outputs (432, 432, 7, 1)\n",
      "[23-11-22 23:43:50 :INFO    ] - Saving 2020-04-02 - generated weights (432, 432, 7, 1)\n"
     ]
    }
   ],
   "source": [
    "!icenet_predict -n 0.6 -t \\\n",
    "    notebook_data notebook_testrun example_south_forecast 42 testdates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "The example uses the cached test data from the training run, but the process is the same for any other processed data with only the need to _omit the `-t` option, which specifies to source from cached test data_.\n",
    "\n",
    "### Outputs\n",
    "\n",
    "In the above example, there are three outputs: \n",
    "\n",
    "* __forecast__: the ___predicted___ forecast data from the model output layer\n",
    "* __outputs__: the outputs from the data loader which would be used for training\n",
    "* __weights__: the generated sample weights from the data loader for the training sample\n",
    "\n",
    "The outputs initially are stored as Numpy arrays under the `results` directory thusly: \n",
    "\n",
    "```\n",
    "results/predict/example_south_forecast/south_run.42/2010_09_01.npy\n",
    "results/predict/example_south_forecast/south_run.42/loader/outputs/2010_09_01.npy\n",
    "results/predict/example_south_forecast/south_run.42/loader/weights/2010_09_01.npy\n",
    "```\n",
    "\n",
    "It should be noted that when predicting the __first__ of these files is what we're really interested in, as the generated data uses a linear trend forecast to produce the data sample. \n",
    "\n",
    "### Ensemble running\n",
    "\n",
    "When producing daily forecasts for IceNet we train on an ensemble of models and also run predictions across them producing a mean and error across that model ensemble. To do this the pipeline repository offers the `run_predict_ensemble` which operates similarly to the above training script. An example of running the ensemble: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARGS: -b 1 -f 0.6 -p bashpc.sh notebook_ensemble notebook_data example_south_ensemble_forecast testdates\n",
      "ARGS = -x arg_batch=1 arg_prep=bashpc.sh , Leftovers: notebook_ensemble notebook_data example_south_ensemble_forecast testdates\n",
      "Running model_ensemble -v  ./tmp.GhYiKd00Zi.predict slurm -x arg_batch=1 arg_prep=bashpc.sh arg_filter_factor=1.44 \n",
      "[23-11-22 23:44:08    :INFO    ] - Model Ensemble Runner\n",
      "[23-11-22 23:44:08    :DEBUG   ] - Assessing /data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/model_ensembler/model-ensemble.json against ./tmp.GhYiKd00Zi.predict\n",
      "[23-11-22 23:44:08    :INFO    ] - Validated configuration file ./tmp.GhYiKd00Zi.predict successfully\n",
      "[23-11-22 23:44:08    :DEBUG   ] - Using selector: EpollSelector\n",
      "[23-11-22 23:44:08    :INFO    ] - Importing model_ensembler.cluster.slurm\n",
      "[23-11-22 23:44:08    :INFO    ] - Running batcher\n",
      "[23-11-22 23:44:08    :INFO    ] - Start batch: 2022-11-23 23:44:08.663621\n",
      "[23-11-22 23:44:08    :DEBUG   ] - Batch(name='example_south_ensemble_forecast', templates=['icenet_predict.sh.j2'], templatedir='../template', job_file='icenet_predict.sh', basedir='./ensemble/example_south_ensemble_forecast', runs=[{'seed': 42}, {'seed': 46}, {'seed': 45}], maxruns=100, maxjobs=10, repeat=False, cluster='short', email='someone@example.com', nodes=1, ntasks=8, length='00:30:00', pre_batch=[{'name': 'execute', 'args': {'cmd': '/usr/bin/ln -s ../../data'}}], pre_run=[], post_run=[], post_batch=[{'name': 'execute', 'args': {'cmd': 'icenet_output -m -r ../.. -o ../../results/predict example_south_ensemble_forecast notebook_data predict_dates.csv'}}])\n",
      "[23-11-22 23:44:08    :INFO    ] - Running cycle 1\n",
      "[23-11-22 23:44:08    :DEBUG   ] - TASK CWD: /data/hpcdata/users/jambyr/icenet/notebook-pipeline/ensemble/example_south_ensemble_forecast\n",
      "[23-11-22 23:44:08    :DEBUG   ] - TASK CTX: {'arg_dataset': 'notebook_data',\n",
      " 'arg_filter_factor': 1,\n",
      " 'arg_network': 'notebook_ensemble',\n",
      " 'arg_testset': True,\n",
      " 'basedir': './ensemble/example_south_ensemble_forecast',\n",
      " 'cluster': 'short',\n",
      " 'email': 'someone@example.com',\n",
      " 'job_file': 'icenet_predict.sh',\n",
      " 'length': '00:30:00',\n",
      " 'maxjobs': 10,\n",
      " 'maxruns': 100,\n",
      " 'mem': '224gb',\n",
      " 'name': 'example_south_ensemble_forecast',\n",
      " 'nodes': 1,\n",
      " 'ntasks': 8,\n",
      " 'repeat': False,\n",
      " 'symlinks': ['../../../data',\n",
      "              '../../../loader.notebook_data.json',\n",
      "              '../../../dataset_config.notebook_data.json',\n",
      "              '../../../network_datasets',\n",
      "              '../../../processed',\n",
      "              '../../../results'],\n",
      " 'templatedir': '../template',\n",
      " 'templates': ['icenet_predict.sh.j2']}\n",
      "[23-11-22 23:44:08    :DEBUG   ] - TASK FUNC: Task(name='execute', args={'cmd': '/usr/bin/ln -s ../../data'}, value=None)\n",
      "[23-11-22 23:44:08    :DEBUG   ] - Calling with cmd = /usr/bin/ln -s ../../data\n",
      "[23-11-22 23:44:08    :INFO    ] - Running command: /usr/bin/ln -s ../../data\n",
      "[23-11-22 23:44:08    :DEBUG   ] - Executing command /usr/bin/ln -s ../../data, cwd unset\n",
      "[23-11-22 23:44:08    :DEBUG   ] - Command successful\n",
      "[23-11-22 23:44:08    :INFO    ] - Start run example_south_ensemble_forecast-0 at 2022-11-23 23:44:08.672495\n",
      "[23-11-22 23:44:08    :DEBUG   ] - Run(arg_dataset='notebook_data', arg_filter_factor='1.44', arg_network='notebook_ensemble', arg_testset=True, symlinks=['../../../data', '../../../loader.notebook_data.json', '../../../dataset_config.notebook_data.json', '../../../network_datasets', '../../../processed', '../../../results'], mem='224gb', name='example_south_ensemble_forecast', templates=['icenet_predict.sh.j2'], templatedir='../template', job_file='icenet_predict.sh', basedir='./ensemble/example_south_ensemble_forecast', maxruns=100, maxjobs=10, repeat=False, cluster='short', email='someone@example.com', nodes=1, ntasks=8, length='00:30:00', seed=42, idx=0, id='example_south_ensemble_forecast-0', dir='/data/hpcdata/users/jambyr/icenet/notebook-pipeline/ensemble/example_south_ensemble_forecast/example_south_ensemble_forecast-0', arg_batch='1', arg_prep='bashpc.sh')\n",
      "[23-11-22 23:44:08    :INFO    ] - rsync -aXE ../template/ /data/hpcdata/users/jambyr/icenet/notebook-pipeline/ensemble/example_south_ensemble_forecast/example_south_ensemble_forecast-0/\n",
      "[23-11-22 23:44:08    :INFO    ] - Start run example_south_ensemble_forecast-1 at 2022-11-23 23:44:08.675700\n",
      "[23-11-22 23:44:08    :DEBUG   ] - Run(arg_dataset='notebook_data', arg_filter_factor='1.44', arg_network='notebook_ensemble', arg_testset=True, symlinks=['../../../data', '../../../loader.notebook_data.json', '../../../dataset_config.notebook_data.json', '../../../network_datasets', '../../../processed', '../../../results'], mem='224gb', name='example_south_ensemble_forecast', templates=['icenet_predict.sh.j2'], templatedir='../template', job_file='icenet_predict.sh', basedir='./ensemble/example_south_ensemble_forecast', maxruns=100, maxjobs=10, repeat=False, cluster='short', email='someone@example.com', nodes=1, ntasks=8, length='00:30:00', seed=46, idx=1, id='example_south_ensemble_forecast-1', dir='/data/hpcdata/users/jambyr/icenet/notebook-pipeline/ensemble/example_south_ensemble_forecast/example_south_ensemble_forecast-1', arg_batch='1', arg_prep='bashpc.sh')\n",
      "[23-11-22 23:44:08    :INFO    ] - rsync -aXE ../template/ /data/hpcdata/users/jambyr/icenet/notebook-pipeline/ensemble/example_south_ensemble_forecast/example_south_ensemble_forecast-1/\n",
      "[23-11-22 23:44:08    :INFO    ] - Start run example_south_ensemble_forecast-2 at 2022-11-23 23:44:08.678533\n",
      "[23-11-22 23:44:08    :DEBUG   ] - Run(arg_dataset='notebook_data', arg_filter_factor='1.44', arg_network='notebook_ensemble', arg_testset=True, symlinks=['../../../data', '../../../loader.notebook_data.json', '../../../dataset_config.notebook_data.json', '../../../network_datasets', '../../../processed', '../../../results'], mem='224gb', name='example_south_ensemble_forecast', templates=['icenet_predict.sh.j2'], templatedir='../template', job_file='icenet_predict.sh', basedir='./ensemble/example_south_ensemble_forecast', maxruns=100, maxjobs=10, repeat=False, cluster='short', email='someone@example.com', nodes=1, ntasks=8, length='00:30:00', seed=45, idx=2, id='example_south_ensemble_forecast-2', dir='/data/hpcdata/users/jambyr/icenet/notebook-pipeline/ensemble/example_south_ensemble_forecast/example_south_ensemble_forecast-2', arg_batch='1', arg_prep='bashpc.sh')\n",
      "[23-11-22 23:44:08    :INFO    ] - rsync -aXE ../template/ /data/hpcdata/users/jambyr/icenet/notebook-pipeline/ensemble/example_south_ensemble_forecast/example_south_ensemble_forecast-2/\n",
      "[23-11-22 23:44:08    :INFO    ] - Templating /data/hpcdata/users/jambyr/icenet/notebook-pipeline/ensemble/example_south_ensemble_forecast/example_south_ensemble_forecast-0/icenet_predict.sh.j2 to /data/hpcdata/users/jambyr/icenet/notebook-pipeline/ensemble/example_south_ensemble_forecast/example_south_ensemble_forecast-0/icenet_predict.sh\n",
      "[23-11-22 23:44:08    :DEBUG   ] - PRE CHECK\n",
      "[23-11-22 23:44:08    :DEBUG   ] - Calling with limit = 10\n",
      "[23-11-22 23:44:08    :DEBUG   ] - Calling with match = example_south_ensemble_forecast\n",
      "[23-11-22 23:44:08    :DEBUG   ] - Executing command squeue -o \"%j,%T\" -h -p short, cwd /data/hpcdata/users/jambyr/icenet/notebook-pipeline/ensemble/example_south_ensemble_forecast/example_south_ensemble_forecast-0\n",
      "[23-11-22 23:44:08    :INFO    ] - Templating /data/hpcdata/users/jambyr/icenet/notebook-pipeline/ensemble/example_south_ensemble_forecast/example_south_ensemble_forecast-1/icenet_predict.sh.j2 to /data/hpcdata/users/jambyr/icenet/notebook-pipeline/ensemble/example_south_ensemble_forecast/example_south_ensemble_forecast-1/icenet_predict.sh\n",
      "[23-11-22 23:44:08    :DEBUG   ] - PRE CHECK\n",
      "[23-11-22 23:44:08    :DEBUG   ] - Calling with limit = 10\n",
      "[23-11-22 23:44:08    :DEBUG   ] - Calling with match = example_south_ensemble_forecast\n",
      "[23-11-22 23:44:08    :INFO    ] - Templating /data/hpcdata/users/jambyr/icenet/notebook-pipeline/ensemble/example_south_ensemble_forecast/example_south_ensemble_forecast-2/icenet_predict.sh.j2 to /data/hpcdata/users/jambyr/icenet/notebook-pipeline/ensemble/example_south_ensemble_forecast/example_south_ensemble_forecast-2/icenet_predict.sh\n",
      "[23-11-22 23:44:08    :DEBUG   ] - PRE CHECK\n",
      "[23-11-22 23:44:08    :DEBUG   ] - Calling with limit = 10\n",
      "[23-11-22 23:44:08    :DEBUG   ] - Calling with match = example_south_ensemble_forecast\n",
      "[23-11-22 23:44:08    :DEBUG   ] - Command successful\n",
      "[23-11-22 23:44:08    :DEBUG   ] - SLURM JOBS result: []\n",
      "[23-11-22 23:44:08    :DEBUG   ] - Jobs in action 0 with limit 10\n",
      "[23-11-22 23:44:08    :DEBUG   ] - POST CHECK\n",
      "[23-11-22 23:44:08    :DEBUG   ] - Sleeping for 1 seconds before submission\n",
      "[23-11-22 23:44:08    :DEBUG   ] - Executing command squeue -o \"%j,%T\" -h -p short, cwd /data/hpcdata/users/jambyr/icenet/notebook-pipeline/ensemble/example_south_ensemble_forecast/example_south_ensemble_forecast-1\n",
      "[23-11-22 23:44:08    :DEBUG   ] - Command successful\n",
      "[23-11-22 23:44:08    :DEBUG   ] - SLURM JOBS result: []\n",
      "[23-11-22 23:44:08    :DEBUG   ] - Jobs in action 0 with limit 10\n",
      "[23-11-22 23:44:08    :DEBUG   ] - POST CHECK\n",
      "[23-11-22 23:44:08    :DEBUG   ] - Sleeping for 0 seconds before submission\n",
      "[23-11-22 23:44:08    :DEBUG   ] - Executing command squeue -o \"%j,%T\" -h -p short, cwd /data/hpcdata/users/jambyr/icenet/notebook-pipeline/ensemble/example_south_ensemble_forecast/example_south_ensemble_forecast-2\n",
      "[23-11-22 23:44:08    :DEBUG   ] - Executing command sbatch --no-requeue icenet_predict.sh, cwd /data/hpcdata/users/jambyr/icenet/notebook-pipeline/ensemble/example_south_ensemble_forecast/example_south_ensemble_forecast-1\n",
      "[23-11-22 23:44:08    :DEBUG   ] - Command successful\n",
      "[23-11-22 23:44:08    :INFO    ] - Submitted job with ID 5478185\n",
      "[23-11-22 23:44:08    :DEBUG   ] - Executing command sacct -XnP -j 5478185 -o jobname,state,start,end, cwd unset\n",
      "[23-11-22 23:44:08    :DEBUG   ] - Command successful\n",
      "[23-11-22 23:44:08    :DEBUG   ] - SLURM JOBS result: [{'name': 'example_south_ensemble_forecast-1', 'state': 'PENDING'}]\n",
      "[23-11-22 23:44:08    :DEBUG   ] - Jobs in action 1 with limit 10\n",
      "[23-11-22 23:44:08    :DEBUG   ] - POST CHECK\n",
      "[23-11-22 23:44:08    :DEBUG   ] - Sleeping for 0 seconds before submission\n",
      "[23-11-22 23:44:08    :DEBUG   ] - Executing command sbatch --no-requeue icenet_predict.sh, cwd /data/hpcdata/users/jambyr/icenet/notebook-pipeline/ensemble/example_south_ensemble_forecast/example_south_ensemble_forecast-2\n",
      "[23-11-22 23:44:08    :DEBUG   ] - Command successful\n",
      "[23-11-22 23:44:08    :INFO    ] - Submitted job with ID 5478186\n",
      "[23-11-22 23:44:08    :DEBUG   ] - Executing command sacct -XnP -j 5478186 -o jobname,state,start,end, cwd unset\n",
      "[23-11-22 23:44:08    :DEBUG   ] - Command successful\n",
      "[23-11-22 23:44:08    :WARNING ] - Could not retrieve job from list\n",
      "[23-11-22 23:44:08    :DEBUG   ] - Command successful\n",
      "[23-11-22 23:44:08    :WARNING ] - Could not retrieve job from list\n",
      "[23-11-22 23:44:09    :DEBUG   ] - Executing command sbatch --no-requeue icenet_predict.sh, cwd /data/hpcdata/users/jambyr/icenet/notebook-pipeline/ensemble/example_south_ensemble_forecast/example_south_ensemble_forecast-0\n",
      "[23-11-22 23:44:09    :DEBUG   ] - Command successful\n",
      "[23-11-22 23:44:09    :INFO    ] - Submitted job with ID 5478187\n",
      "[23-11-22 23:44:09    :DEBUG   ] - Executing command sacct -XnP -j 5478187 -o jobname,state,start,end, cwd unset\n",
      "[23-11-22 23:44:09    :DEBUG   ] - Command successful\n",
      "[23-11-22 23:44:09    :WARNING ] - Could not retrieve job from list\n",
      "[23-11-22 23:44:38    :DEBUG   ] - Executing command sacct -XnP -j 5478185 -o jobname,state,start,end, cwd unset\n",
      "[23-11-22 23:44:38    :DEBUG   ] - Executing command sacct -XnP -j 5478186 -o jobname,state,start,end, cwd unset\n",
      "[23-11-22 23:44:39    :DEBUG   ] - Command successful\n",
      "[23-11-22 23:44:39    :DEBUG   ] - SLURM find result name: example_south_ensemble_forecast-1\n",
      "[23-11-22 23:44:39    :DEBUG   ] - Executing command sacct -XnP -j 5478185 -o jobname,state,start,end, cwd unset\n",
      "[23-11-22 23:44:39    :DEBUG   ] - Command successful\n",
      "[23-11-22 23:44:39    :DEBUG   ] - SLURM find result name: example_south_ensemble_forecast-2\n",
      "[23-11-22 23:44:39    :DEBUG   ] - Executing command sacct -XnP -j 5478186 -o jobname,state,start,end, cwd unset\n",
      "[23-11-22 23:44:39    :DEBUG   ] - Command successful\n",
      "[23-11-22 23:44:39    :DEBUG   ] - SLURM find result name: example_south_ensemble_forecast-1\n",
      "[23-11-22 23:44:39    :DEBUG   ] - example_south_ensemble_forecast-1 monitor got state COMPLETED for job 5478185\n",
      "[23-11-22 23:44:39    :INFO    ] - example_south_ensemble_forecast-1 monitor got state COMPLETED for job 5478185\n",
      "[23-11-22 23:44:39    :INFO    ] - End run example_south_ensemble_forecast-1 at 2022-11-23 23:44:39.129619\n",
      "[23-11-22 23:44:39    :DEBUG   ] - Command successful\n",
      "[23-11-22 23:44:39    :DEBUG   ] - SLURM find result name: example_south_ensemble_forecast-2\n",
      "[23-11-22 23:44:39    :DEBUG   ] - example_south_ensemble_forecast-2 monitor got state COMPLETED for job 5478186\n",
      "[23-11-22 23:44:39    :INFO    ] - example_south_ensemble_forecast-2 monitor got state COMPLETED for job 5478186\n",
      "[23-11-22 23:44:39    :INFO    ] - End run example_south_ensemble_forecast-2 at 2022-11-23 23:44:39.141006\n",
      "[23-11-22 23:44:39    :DEBUG   ] - Executing command sacct -XnP -j 5478187 -o jobname,state,start,end, cwd unset\n",
      "[23-11-22 23:44:39    :DEBUG   ] - Command successful\n",
      "[23-11-22 23:44:39    :DEBUG   ] - SLURM find result name: example_south_ensemble_forecast-0\n",
      "[23-11-22 23:44:39    :DEBUG   ] - Executing command sacct -XnP -j 5478187 -o jobname,state,start,end, cwd unset\n",
      "[23-11-22 23:44:40    :DEBUG   ] - Command successful\n",
      "[23-11-22 23:44:40    :DEBUG   ] - SLURM find result name: example_south_ensemble_forecast-0\n",
      "[23-11-22 23:44:40    :DEBUG   ] - example_south_ensemble_forecast-0 monitor got state COMPLETED for job 5478187\n",
      "[23-11-22 23:44:40    :INFO    ] - example_south_ensemble_forecast-0 monitor got state COMPLETED for job 5478187\n",
      "[23-11-22 23:44:40    :INFO    ] - End run example_south_ensemble_forecast-0 at 2022-11-23 23:44:40.090486\n",
      "[23-11-22 23:44:40    :DEBUG   ] - Batch example_south_ensemble_forecast result #0 from run 0: job 5478187\n",
      "[23-11-22 23:44:40    :DEBUG   ] - Batch example_south_ensemble_forecast result #1 from run 1: job 5478185\n",
      "[23-11-22 23:44:40    :DEBUG   ] - Batch example_south_ensemble_forecast result #2 from run 2: job 5478186\n",
      "[23-11-22 23:44:40    :DEBUG   ] - TASK CWD: /data/hpcdata/users/jambyr/icenet/notebook-pipeline/ensemble/example_south_ensemble_forecast\n",
      "[23-11-22 23:44:40    :DEBUG   ] - TASK CTX: {'arg_batch': '1',\n",
      " 'arg_dataset': 'notebook_data',\n",
      " 'arg_filter_factor': '1.44',\n",
      " 'arg_network': 'notebook_ensemble',\n",
      " 'arg_prep': 'bashpc.sh',\n",
      " 'arg_testset': True,\n",
      " 'basedir': './ensemble/example_south_ensemble_forecast',\n",
      " 'cluster': 'short',\n",
      " 'dir': '/data/hpcdata/users/jambyr/icenet/notebook-pipeline/ensemble/example_south_ensemble_forecast/example_south_ensemble_forecast-2',\n",
      " 'email': 'someone@example.com',\n",
      " 'id': 'example_south_ensemble_forecast-2',\n",
      " 'idx': 2,\n",
      " 'job_file': 'icenet_predict.sh',\n",
      " 'length': '00:30:00',\n",
      " 'maxjobs': 10,\n",
      " 'maxruns': 100,\n",
      " 'mem': '224gb',\n",
      " 'name': 'example_south_ensemble_forecast',\n",
      " 'nodes': 1,\n",
      " 'ntasks': 8,\n",
      " 'repeat': False,\n",
      " 'seed': 45,\n",
      " 'symlinks': ['../../../data',\n",
      "              '../../../loader.notebook_data.json',\n",
      "              '../../../dataset_config.notebook_data.json',\n",
      "              '../../../network_datasets',\n",
      "              '../../../processed',\n",
      "              '../../../results'],\n",
      " 'templatedir': '../template',\n",
      " 'templates': ['icenet_predict.sh.j2']}\n",
      "[23-11-22 23:44:40    :DEBUG   ] - TASK FUNC: Task(name='execute', args={'cmd': 'icenet_output -m -r ../.. -o ../../results/predict example_south_ensemble_forecast notebook_data predict_dates.csv'}, value=None)\n",
      "[23-11-22 23:44:40    :DEBUG   ] - Calling with cmd = icenet_output -m -r ../.. -o ../../results/predict example_south_ensemble_forecast notebook_data predict_dates.csv\n",
      "[23-11-22 23:44:40    :INFO    ] - Running command: icenet_output -m -r ../.. -o ../../results/predict example_south_ensemble_forecast notebook_data predict_dates.csv\n",
      "[23-11-22 23:44:40    :DEBUG   ] - Executing command icenet_output -m -r ../.. -o ../../results/predict example_south_ensemble_forecast notebook_data predict_dates.csv, cwd unset\n",
      "[23-11-22 23:44:49    :WARNING ] - Command returned err: None\n",
      "[23-11-22 23:44:49    :INFO    ] - Batch example_south_ensemble_forecast completed: 2022-11-23 23:44:49.232015\n",
      "Removing temporary configuration ./tmp.GhYiKd00Zi.predict\n"
     ]
    }
   ],
   "source": [
    "!./run_predict_ensemble.sh \\\n",
    "    -b 1 -f 0.6 -p bashpc.sh \\\n",
    "    notebook_ensemble notebook_data example_south_ensemble_forecast testdates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "As with the previous example, the individual numpy outputs, samples and sample weights are deposited into `/results/predict` for each ensemble member. However, the ensemble also runs `icenet_output` to generate __a CF-compliant NetCDF containing the forecasts requested__ which can then be post-processed or [deposited to an external location](#Uploading-to-Azure) (which is the platform for the [wider IceNet forecasting infrastructure](https://github.com/alan-turing-institute/IceNet-Project). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-23 23:44:59.809415: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-23 23:45:00.593340: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/\n",
      "2022-11-23 23:45:00.593489: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/\n",
      "2022-11-23 23:45:00.593504: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "[23-11-22 23:45:01 :INFO    ] - Loading configuration ./dataset_config.notebook_data.json\n",
      "[23-11-22 23:45:01 :INFO    ] - Training dataset path: ./network_datasets/notebook_data/south/train\n",
      "[23-11-22 23:45:01 :INFO    ] - Validation dataset path: ./network_datasets/notebook_data/south/val\n",
      "[23-11-22 23:45:01 :INFO    ] - Test dataset path: ./network_datasets/notebook_data/south/test\n",
      "[23-11-22 23:45:01 :INFO    ] - Downloading single daily SIC netCDF file for regridding ERA5 data to EASE grid...\n",
      "--2022-11-23 23:45:01--  ftp://osisaf.met.no/reprocessed/ice/conc/v2p0/1979/01/ice_conc_sh_ease2-250_cdr-v2p0_197901021200.nc\n",
      "           => ‘./_sicfile/.listing’\n",
      "Resolving osisaf.met.no (osisaf.met.no)... 157.249.75.10\n",
      "Connecting to osisaf.met.no (osisaf.met.no)|157.249.75.10|:21... connected.\n",
      "Logging in as anonymous ... Logged in!\n",
      "==> SYST ... done.    ==> PWD ... done.\n",
      "==> TYPE I ... done.  ==> CWD (1) /reprocessed/ice/conc/v2p0/1979/01 ... done.\n",
      "==> PASV ... done.    ==> LIST ... done.\n",
      "\n",
      "    [ <=>                                   ] 3,239       --.-K/s   in 0.02s   \n",
      "\n",
      "2022-11-23 23:45:02 (135 KB/s) - ‘./_sicfile/.listing’ saved [3239]\n",
      "\n",
      "--2022-11-23 23:45:02--  ftp://osisaf.met.no/reprocessed/ice/conc/v2p0/1979/01/ice_conc_sh_ease2-250_cdr-v2p0_197901021200.nc\n",
      "           => ‘./_sicfile/ice_conc_sh_ease2-250_cdr-v2p0_197901021200.nc’\n",
      "==> CWD not required.\n",
      "==> PASV ... done.    ==> RETR ice_conc_sh_ease2-250_cdr-v2p0_197901021200.nc ... done.\n",
      "Length: 9856141 (9.4M)\n",
      "\n",
      "100%[======================================>] 9,856,141   9.20MB/s   in 1.0s   \n",
      "\n",
      "2022-11-23 23:45:03 (9.20 MB/s) - ‘./_sicfile/ice_conc_sh_ease2-250_cdr-v2p0_197901021200.nc’ saved [9856141]\n",
      "\n",
      "FINISHED --2022-11-23 23:45:03--\n",
      "Total wall clock time: 2.0s\n",
      "Downloaded: 2 files, 9.4M in 1.0s (9.00 MB/s)\n",
      "[23-11-22 23:45:03 :INFO    ] - Child returned: 0\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/icenet/process/predict.py:58: FutureWarning: Ignoring a datum in netCDF load for consistency with existing behaviour. In a future version of Iris, this datum will be applied. To apply the datum when loading, use the iris.FUTURE.datum_support flag.\n",
      "  cube = iris.load_cube(path, 'sea_ice_area_fraction')\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/icenet/process/predict.py:58: FutureWarning: Ignoring a datum in netCDF load for consistency with existing behaviour. In a future version of Iris, this datum will be applied. To apply the datum when loading, use the iris.FUTURE.datum_support flag.\n",
      "  cube = iris.load_cube(path, 'sea_ice_area_fraction')\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/icenet/process/predict.py:58: FutureWarning: Ignoring a datum in netCDF load for consistency with existing behaviour. In a future version of Iris, this datum will be applied. To apply the datum when loading, use the iris.FUTURE.datum_support flag.\n",
      "  cube = iris.load_cube(path, 'sea_ice_area_fraction')\n",
      "/data/hpcdata/users/jambyr/miniconda3/envs/icenet-notebooks/lib/python3.8/site-packages/icenet/process/predict.py:58: FutureWarning: Ignoring a datum in netCDF load for consistency with existing behaviour. In a future version of Iris, this datum will be applied. To apply the datum when loading, use the iris.FUTURE.datum_support flag.\n",
      "  cube = iris.load_cube(path, 'sea_ice_area_fraction')\n",
      "[23-11-22 23:45:04 :INFO    ] - Post-processing 2020-04-01\n",
      "[23-11-22 23:45:04 :INFO    ] - Post-processing 2020-04-02\n",
      "[23-11-22 23:45:04 :INFO    ] - Dataset arr shape: (2, 432, 432, 7, 2)\n",
      "[23-11-22 23:45:04 :INFO    ] - Saving to results/predict/example_south_forecast.nc\n"
     ]
    }
   ],
   "source": [
    "!icenet_output -o results/predict example_south_forecast notebook_data testdates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "Note that the ensemble run automatically handles this generation of output for the ensemble. For a single run this is relatively meaningless as there is only a single model making predictions, giving no uncertainty quantification, __so this is provided as an example only__. _Please review the `-h` help option for the script to gain further insight the options available._\n",
    "\n",
    "### Uploading to Azure\n",
    "\n",
    "The following command uploads a specific date from the `icenet_output` produced dataset to an Azure storage blob storage account.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!icenet_upload_azure results/predict/example_south_ensemble_forecast.nc 2020-04-30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "## Other Pipeline Considerations\n",
    "\n",
    "### A bit more information on ensemble runs\n",
    "\n",
    "#### Cleaning up runs\n",
    "\n",
    "Ensemble runs take place under `/ensemble/` in the pipeline folder and ARE NOT deleted after they've happened, to allow for debugging. Commonly, the ensemble configurations will contain a delete task to remove the extraneous run folders. __In the meantime this should be done manually__ after running `run_train_ensemble` or `run_predict_ensemble`.\n",
    "\n",
    "The only exception to this is the use of `run_daily.sh` (see below) which does clean up prior to rerunning. \n",
    "\n",
    "### Daily execution\n",
    "\n",
    "Daily execution is facilitated in the pipeline by using [`run_daily.sh`](https://github.com/antarctica/IceNet-Pipeline/blob/main/run_daily.sh). This wraps all the necessary steps to perform the following sequence for producing forecasts from yesterday for the next 93 days, for both northern and southern hemispheres. \n",
    "\n",
    "* Removes any old ensemble runs\n",
    "* Downloads [HRES forecast data from the ECMWF MARS API](https://www.ecmwf.int/en/forecasts/datasets/catalogue-ecmwf-real-time-products)\n",
    "* Processes the HRES and necessary training metadata to produce a data loader\n",
    "* Creates a dataset configuration for it\n",
    "* Runs a [prediction ensemble](#Predict) to produce a NetCDF\n",
    "* Uploads to the necessary endpoint\n",
    "\n",
    "#### Automation\n",
    "\n",
    "With the above shell script it's trivial to automate using cron. Of course this is simply for demonstration, with more complex workflow managers offering far great flexibility especially when considering analysis of the produced forecasts.\n",
    "\n",
    "```bash\n",
    "# We assume your environment is configured appropriately to run conda from cron files, for example by adding...\n",
    "#\n",
    "# SHELL=/bin/bash\n",
    "# BASH_ENV=~/.bashrc_env\n",
    "#\n",
    "# With conda initialisation in bashrc_env at the top of your crontab\n",
    "25 9 * * * conda activate icenet; cd $HOME/hpc/icenet/pipeline && bash run_daily.sh >$HOME/daily.log 2>&1; conda deactivate\n",
    "```\n",
    "\n",
    "TODO: more information on the usage of this command.\n",
    "\n",
    "## Summary\n",
    "\n",
    "Within this notebook we've attempted to give a full crash course to running the CLI tools both __manually__ and using the __pipeline helper scripts__. This is the first of four (currently) notebooks contained within the pipeline repository, covering further information: \n",
    "\n",
    "* [Data structure and analysis](02.data_analysis.ipynb): understand the structure of the data stores and products created by these workflows and what tools currently exist in IceNet to looks over them.\n",
    "* [Library usage](03.library_usage.ipynb): understand how to programmatically perform an end to end run.\n",
    "* [Library extension](04.library_extension.ipynb): understand why and how to extend the IceNet library.\n",
    "\n",
    "## Version\n",
    "- IceNet Codebase: v0.2.0.dev7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-icenet-notebooks]",
   "language": "python",
   "name": "conda-env-miniconda3-icenet-notebooks-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
