{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick hack to put us in the icenet-pipeline folder, assuming it was created as per 01.cli_demonstration.ipynb\n",
    "import os\n",
    "if os.path.exists(\"03.library_usage.ipynb\"):\n",
    "    os.chdir(\"../notebook-pipeline\")\n",
    "print(\"Running in {}\".format(os.getcwd()))\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IceNet Library Usage\n",
    "\n",
    "## Context\n",
    "\n",
    "### Purpose\n",
    "The IceNet library provides the ability to download, process, train and predict from end to end via a set of command-line interfaces.\n",
    "\n",
    "Using this notebook we can understand how to programmatically undertake various activities using the IceNet library, allowing for significant customisation of the end to end deep learning pipeline for research and operational use.\n",
    "\n",
    "### Modelling approach\n",
    "This modelling approach allows users to immediately utilise the library for producing sea ice concentraion forecasts.\n",
    "\n",
    "### Highlights\n",
    "The key features of an end to end run are: \n",
    "* Setup: _this was concerned with setting up the conda environment, which remains the same as in 01.cli_demonstration_\n",
    "* [Download](#Download) \n",
    "* [Process](#Process)\n",
    "* [Train](#Train)\n",
    "* [Predict](#Predict)\n",
    "\n",
    "_This follows the same structure as the CLI demonstration notebook so that it's easy to follow step-by-step..._\n",
    "\n",
    "### Contributions\n",
    "#### Notebook\n",
    "James Byrne (author)\n",
    "\n",
    "__Please raise issues [in this repository](https://github.com/antarctica/IceNet-Pipeline) to suggest updates to this notebook!__ \n",
    "\n",
    "Contact me at _jambyr \\<at\\> bas.ac.uk_ for anything else...\n",
    "\n",
    "#### Modelling codebase\n",
    "James Byrne (code author), Tom Andersson (science author)\n",
    "\n",
    "#### Modelling publications\n",
    "Andersson, T.R., Hosking, J.S., PÃ©rez-Ortiz, M. et al. Seasonal Arctic sea ice forecasting with probabilistic deep learning. Nat Commun 12, 5124 (2021). https://doi.org/10.1038/s41467-021-25257-4\n",
    "\n",
    "#### Involved organisations\n",
    "The Alan Turing Institute and British Antarctic Survey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Once installed the API can be utilised as easily as the CLI commands from a shell, via any Python interpreter. As usual ensure that you're operating within the conda environment you installed the library into.\n",
    "\n",
    "### A tip on CLI - API usage\n",
    "\n",
    "All of the `icenet_*` CLI commands behind the scenes implement API activities. By inspecting the [`setup.py` ](https://github.com/JimCircadian/icenet2/blob/main/setup.py#L32) entry points you can locate the module and thus the code used by these. \n",
    "\n",
    "In most cases the CLI imposes various assumptions about what to do without exposing, necessarily, all available options to change the behaviour of the library. This is primarily as the CLI entry points are still under development to open up the options, so these CLI operations are for introductory use and API usage is recommended for advanced use cases and pipeline integrations.\n",
    "\n",
    "### What we'll cover\n",
    "\n",
    "For the sake of illustration this notebook will display and execute the equivalent API code, equivalent to [the first notebook of this collection](01.cli_demonstration.ipynb) as well as some updates that incorporate the visualisations from [the second notebook describing the data](02.data_and_forecasts.ipynb). However, for the sake of extending our dataset, we'll work towards extending our original downloads from covering *2019-12-28 through 2020-4-30* to cover 2020 in totality, as well as creating a more complex selection of dates for our dataset, training and predicting with new networks.\n",
    "\n",
    "We'll start with some frequently useful imports..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "import glob, json, os, random, sys\n",
    "import datetime as dt\n",
    "import numpy as np, pandas as pd, xarray as xr, matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "\n",
    "# We also set the logging level so that we get some feedback from the API\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Download\n",
    "\n",
    "The following is preparation of the downloaders, whose instantiation describes the interactions with the upstream APIs/data interfaces used to source various types of data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icenet.data.sic.mask import Masks\n",
    "from icenet.data.interfaces.cds import ERA5Downloader\n",
    "from icenet.data.sic.osisaf import SICDownloader\n",
    "\n",
    "masks = Masks(north=False, south=True)\n",
    "era5 = ERA5Downloader(\n",
    "    var_names=[\"tas\", \"zg\", \"uas\", \"vas\"],\n",
    "    pressure_levels=[None, [250, 500], None, None],\n",
    "    dates=[pd.to_datetime(date).date() for date in\n",
    "           pd.date_range(\"2020-1-1\", \"2020-4-30\", freq=\"D\")],\n",
    "    delete_tempfiles=False,\n",
    "    max_threads=64,\n",
    "    north=False,\n",
    "    south=True,\n",
    "    # NOTE: there appears to be a bug with the toolbox API at present (icenet#54)\n",
    "    use_toolbox=False\n",
    ")\n",
    "sic = SICDownloader(\n",
    "    dates=[pd.to_datetime(date).date() for date in\n",
    "           pd.date_range(\"2020-1-1\", \"2020-4-30\", freq=\"D\")],\n",
    "    delete_tempfiles=False,\n",
    "    north=False,\n",
    "    south=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we download all required data with our extended date range. All downloaders inherit a `download` method from the `Downloader` class in [`icenet2.data.producers`](https://github.com/JimCircadian/icenet2/blob/main/icenet2/data/producers.py), which also contains two other data producing classes `Generator` (which Masks inherits from) and `Processor` (used in the next section), each providing abstract implementations that multiple classes derive from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The original downloading takes a while, hence I left it over a weekend to preserve \n",
    "# the logging. When needing to pick up the processing, you can rerun these items\n",
    "# to continue to regridding/rotating of fields\n",
    "\n",
    "masks.generate(save_polarhole_masks=False)\n",
    "era5.download()\n",
    "sic.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ERA5Downloader` inherits from `ClimateDownloader`, from which several implementations derive their functionality. Two particularly useful methods shown below allow the downloaded data to be converted to the same grid and orientation as the OSISAF SIC data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "era5.regrid()\n",
    "era5.rotate_wind_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is hopefully obvious now that the CLI operations wrap several activities within the API up for convenience and initial ease of use, but that for experimentation, research and advancing the pipeline the API offers greater flexibility to manipulate processing chains as required for these purposes.\n",
    "\n",
    "## Process\n",
    "\n",
    "Similarly to the downloaders, each data producer (be it a `Downloader` or `Generator`) has a respective `Processor` that converts the `/data/` products into a normalised, preprocessed dataset under `/processed/` as per the `icenet_process_*` commands.\n",
    "\n",
    "Firstly, to make life a bit easier, we set up some variables that are normally handled from the CLI arguments. In this case we're splitting the validation and test sets out of the 2020 data in a fairly naive manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_dates = dict(\n",
    "    train=[pd.to_datetime(el) for el in pd.date_range(\"2020-1-1\", \"2020-3-31\")],\n",
    "    val=[pd.to_datetime(el) for el in pd.date_range(\"2020-4-3\", \"2020-4-23\")],\n",
    "    test=[pd.to_datetime(el) for el in pd.date_range(\"2020-4-1\", \"2020-4-2\")],\n",
    ")\n",
    "processed_name = \"notebook_api_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create the data producer and configure them for the dataset we want to create. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icenet.data.processors.era5 import IceNetERA5PreProcessor\n",
    "from icenet.data.processors.meta import IceNetMetaPreProcessor\n",
    "from icenet.data.processors.osi import IceNetOSIPreProcessor\n",
    "\n",
    "pp = IceNetERA5PreProcessor(\n",
    "    [\"uas\", \"vas\"],\n",
    "    [\"tas\", \"zg500\", \"zg250\"],\n",
    "    processed_name,\n",
    "    processing_dates[\"train\"],\n",
    "    processing_dates[\"val\"],\n",
    "    processing_dates[\"test\"],\n",
    "    linear_trends=tuple(),\n",
    "    north=False,\n",
    "    south=True\n",
    ")\n",
    "osi = IceNetOSIPreProcessor(\n",
    "    [\"siconca\"],\n",
    "    [],\n",
    "    processed_name,\n",
    "    processing_dates[\"train\"],\n",
    "    processing_dates[\"val\"],\n",
    "    processing_dates[\"test\"],\n",
    "    linear_trends=None,\n",
    "    north=False,\n",
    "    south=True\n",
    ")\n",
    "meta = IceNetMetaPreProcessor(\n",
    "    processed_name,\n",
    "    north=False,\n",
    "    south=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we initialise the data processors using `init_source_data` which scans the data source directories to understand what data is available for processing based on the parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.init_source_data(\n",
    "    lag_days=1,\n",
    ")\n",
    "pp.process()\n",
    "osi.init_source_data(\n",
    "    lag_days=1,\n",
    "    lead_days=7,\n",
    ")\n",
    "osi.process()\n",
    "meta.process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point the preprocessed data is ready to convert or create a configuration for the network dataset.\n",
    "\n",
    "### Dataset creation\n",
    "\n",
    "As with the `icenet_dataset_create` command we can create a dataset configuration for training the network. As before this can include cached data for the network in the format of a TFRecordDataset compatible set of tfrecords. To achieve this we create the `IceNetDataLoader`, which can both generate `IceNetDataSet` configurations (which easily provide the necessary functionality for training and prediction) as well as individual data samples for direct usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icenet.data.loaders import IceNetDataLoaderFactory\n",
    "\n",
    "dl = IceNetDataLoaderFactory().create_data_loader(\n",
    "    \"loader.notebook_api_data.json\",\n",
    "    \"api_dataset\",\n",
    "    1,\n",
    "    n_forecast_days=93,\n",
    "    north=False,\n",
    "    south=True,\n",
    "    output_batch_size=4,\n",
    "    generate_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we can either use `generate` or `write_dataset_config_only` to produce a ready-to-go `IceNetDataSet` configuration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl.generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "For single runs we programmatically can call the same method used by the CLI. `train_model` defines the training process from start to finish. The [`model-ensembler`](https://github.com/JimCircadian/model-ensembler) works outside the API, controlling multiple CLI submissions. Customising an ensemble can be achieved through looking at the configuration in [the pipeline repository](https://github.com/antarctica/IceNet-Pipeline). That said, if workflow system integration (e.g. Airflow) is desired, integrating via this method is the way to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icenet.model.train import train_model\n",
    "from icenet.data.dataset import IceNetDataSet\n",
    "import tensorflow as tf\n",
    "\n",
    "dataset = IceNetDataSet(\"dataset_config.api_dataset.json\",\n",
    "                        batch_size=4)\n",
    "strategy = tf.distribute.get_strategy()\n",
    "\n",
    "trained_path, history = \\\n",
    "    train_model(\"api_test_run\",\n",
    "                dataset,\n",
    "                batch_size=4,\n",
    "                epochs=10,\n",
    "                n_filters_factor=0.6,\n",
    "                seed=42,\n",
    "                strategy=strategy,\n",
    "                # == 2 for notebook usage\n",
    "                training_verbosity=2,\n",
    "                # Various other parameters can be set here as with the CLI\n",
    "                # learning_rate=args.lr,\n",
    "                # lr_10e_decay_fac=args.lr_10e_decay_fac,\n",
    "                # lr_decay_start=args.lr_decay_start,\n",
    "                # lr_decay_end=args.lr_decay_end,\n",
    "                # pre_load_network=args.preload is not None,\n",
    "                # pre_load_path=args.preload,\n",
    "                # use_multiprocessing=args.multiprocessing,\n",
    "                # use_wandb=not args.no_wandb,\n",
    "                # wandb_offline=args.wandb_offline,\n",
    "                # workers=args.workers, \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breaking `train_model` apart, one can look at customising the training process itself programmatically. Here, we've reduced `train_model` to its component parts with some notes about missing items (e.g. callbacks and wandb integration), to give some insight into how the training workflow is architected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icenet.data.dataset import IceNetDataSet\n",
    "from icenet.model.models import unet_batchnorm\n",
    "import icenet.model.losses as losses\n",
    "import icenet.model.metrics as metrics\n",
    "\n",
    "# train_model sets up wandb and attempts seeding here (see icenet#8 for issues around multi-GPU determinism)\n",
    "os.environ['PYTHONHASHSEED'] = str(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "tf.keras.utils.set_random_seed(42)\n",
    "\n",
    "ds = IceNetDataSet(dataset_config, batch_size=4)\n",
    "\n",
    "input_shape = (*ds.shape, ds.num_channels)\n",
    "train_ds, val_ds, test_ds = ds.get_split_datasets()\n",
    "\n",
    "# train_model handles pickup runs/trained networks\n",
    "run_name = \"custom_run\"\n",
    "network_folder = os.path.join(\".\", \"results\", \"networks\", run_name)\n",
    "\n",
    "if not os.path.exists(network_folder):\n",
    "    logging.info(\"Creating network folder: {}\".format(network_folder))\n",
    "    os.makedirs(network_folder)\n",
    "\n",
    "network_path = os.path.join(network_folder,\n",
    "                            \"{}.network_{}.{}.h5\".format(run_name,\n",
    "                                                         ds.identifier,\n",
    "                                                         seed))\n",
    "\n",
    "callbacks_list = list()\n",
    "# train_model sets up various callbacks: early stopping, lr scheduler, \n",
    "# checkpointing, wandb and tensorboard\n",
    "\n",
    "with strategy.scope():\n",
    "    loss = losses.WeightedMSE()\n",
    "    metrics_list = [\n",
    "        metrics.WeightedMAE(),\n",
    "        metrics.WeightedRMSE(),\n",
    "        losses.WeightedMSE()\n",
    "    ]\n",
    "\n",
    "    network = unet_batchnorm(\n",
    "        input_shape=input_shape,\n",
    "        loss=loss,\n",
    "        metrics=metrics_list,\n",
    "        learning_rate=1e-4,\n",
    "        filter_size=3,\n",
    "        n_filters_factor=0.6,\n",
    "        n_forecast_days=ds.n_forecast_days,\n",
    "    )\n",
    "\n",
    "# train_model loads weights\n",
    "network.summary()\n",
    "\n",
    "model_history = network.fit(\n",
    "    train_ds,\n",
    "    epochs=5,\n",
    "    verbose=2,\n",
    "    callbacks=callbacks_list,\n",
    "    validation_data=val_ds,\n",
    "    max_queue_size=10,\n",
    ")\n",
    "\n",
    "logging.info(\"Saving network to: {}\".format(network_path))\n",
    "network.save_weights(network_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen the training workflow is very standard for deep learning networks, with `train_model` and CLI wrapping up the training process with a lot of customisation of extraneous functionality. \n",
    "\n",
    "## Predict\n",
    "\n",
    "In much the same manner as with `train_model`, the `predict_forecast` method acts as a convenient entry point workflow system integration, CLI entry as well as an overridable method upon which to base custom implementations. Using the method directly relies on loading from a prepared (but perhaps not cached) dataset.\n",
    "\n",
    "Some parameters are fed to `predict_forecast` that ideally shouldn't need to be specified (like `seed` and `n_filters_factor`) and might seem contextually odd. They're used to locate the appropriate saved network. *This will be cleaned up in a future version*.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icenet.model.predict import predict_forecast\n",
    "\n",
    "# Same as our training set, we'll use the test dates defined when we created this\n",
    "# dataset\n",
    "dataset_config = \"dataset_config.api_dataset.json\"\n",
    "\n",
    "# Follows the naming convention used by the CLI version\n",
    "output_dir = os.path.join(\".\", \"results\", \"predict\",\n",
    "                          \"custom_run_forecast\",\n",
    "                          \"{}.{}\".format(\"custom_run\", \"42\"))\n",
    "\n",
    "forecasts, gen_outputs, sample_weights = \\\n",
    "    predict_forecast(dataset_config,\n",
    "                     \"custom_run\",\n",
    "                     n_filters_factor=0.6,\n",
    "                     seed=42,\n",
    "                     # Range previously defined as processing_dates[\"test\"]\n",
    "                     start_dates=[pd.to_datetime(el).date()\n",
    "                                  for el in pd.date_range(\"2020-4-1\", \"2020-4-2\")],\n",
    "                     testset=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The persistence and respective use of these results is then up to the user, with the threefold outputs correlating to that which is normally saved to disk as individual files containing the numpy arrays by the CLI command.\n",
    "\n",
    "The [internals of the `predict_forecast` method](https://github.com/JimCircadian/icenet2/blob/main/icenet2/model/predict.py#L17) are still undergoing some development, but it should be noted that this method can be easily overridden or called as part of a larger workflow. In particular, within this method it's worth noting the importance of the `testset` parameter. \n",
    "\n",
    "Should `testset` be true, then cached data generated in `network_datasets` is never used, and instead the preprocessed data in `processed` is used directly. This actually makes the implementation of `predict_forecast` extremely simple compared with the alternative, due to some outstanding work to derive dates from the cached batched files. \n",
    "\n",
    "As before this is revised implementation in order to illustrate the \"non-testset\" use case, so several modifications are in situ for notebook execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icenet.data.dataset import IceNetDataSet\n",
    "from icenet.model.models import unet_batchnorm\n",
    "import tensorflow as tf\n",
    "\n",
    "# Usually passed to predict forecast\n",
    "start_dates = [pd.to_datetime(el).date()\n",
    "               for el in pd.date_range(\"2020-4-1\", \"2020-4-2\")]\n",
    "testset = False\n",
    "# End predict_forecast args\n",
    "\n",
    "ds = IceNetDataSet(dataset_config)\n",
    "dl = ds.get_data_loader()\n",
    "\n",
    "if not testset:\n",
    "    logging.info(\"Generating forecast inputs from processed/ files\")\n",
    "\n",
    "    forecast_inputs, gen_outputs, sample_weights = \\\n",
    "        list(zip(*[dl.generate_sample(date) for date in start_dates]))\n",
    "else:\n",
    "    # Use the network_dataset cached data, which is a much messier implementation\n",
    "    # but worthwhile using if running massive datasets incl.datasets\n",
    "    # ...\n",
    "    pass\n",
    "\n",
    "network_folder = os.path.join(\".\", \"results\", \"networks\", \"custom_run\")\n",
    "\n",
    "dataset_name = ds.identifier\n",
    "network_path = os.path.join(network_folder,\n",
    "                            \"{}.network_{}.{}.h5\".format(\"custom_run\",\n",
    "                                                         \"api_dataset\",\n",
    "                                                         42))\n",
    "\n",
    "logging.info(\"Loading model from {}...\".format(network_path))\n",
    "\n",
    "network = unet_batchnorm(\n",
    "    (*ds.shape, dl.num_channels),\n",
    "    [],\n",
    "    [],\n",
    "    n_filters_factor=0.6,\n",
    "    n_forecast_days=ds.n_forecast_days\n",
    ")\n",
    "network.load_weights(network_path)\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for i, net_input in enumerate(forecast_inputs):\n",
    "    logging.info(\"Running prediction {} - {}\".format(i, start_dates[i]))\n",
    "    pred = network(tf.convert_to_tensor([net_input]), training=False)\n",
    "    predictions.append(pred)\n",
    "print(\"Predictions: {} shape {}\".format(len(predictions), \n",
    "                                        predictions[0].shape))\n",
    "print(\"Generated outputs: {} shape {}\".format(len(gen_outputs), \n",
    "                                              gen_outputs[0].shape))\n",
    "print(\"Sample weights: {} shape {}\".format(len(sample_weights), \n",
    "                                           sample_weights[0].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has attempted to illustrate the workflow implementations of the CLI as well as highlight the flexibility of direct integration using it. Ultimately, library usage is the only way to achieve truly novel and flexible usage, the CLI is for convenience of running existing pipelines without having to manually implement complex scripts. \n",
    "\n",
    "The key to leveraging the benefits of both of these interfaces being provided is to consider using the following workflow:\n",
    "\n",
    "* Get your environment(s) set up, be they research, development or production\n",
    "* Use the existing CLI implementations to seed the data stores and get baseline networks operational\n",
    "* Start to customise the existing operations via custom calls to the API, for example by downloading new variables or adding extra analysis to training/prediction runs\n",
    "* If researching, consider [extending the functionality of the API to include revised or completely new implementations, such as additional data sources](04.library_extension.ipynb)\n",
    "\n",
    "This last point brings us to the topic of the last of the introductory notebooks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version\n",
    "- Codebase: drafted for v0.2.0, as yet untested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-icenet]",
   "language": "python",
   "name": "conda-env-miniconda3-icenet-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
